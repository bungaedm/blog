[{"content":"1. Scale-free Network Scale-free Network는 Random Network와 달리 Power Law 분포를 따른다. 여기서 scale은 노드를 랜덤하게 선택했을 때 평균적으로 얻어지는 안정적인 degree를 뜻한다. 그러므로 random network에서는 평균 근처로 scale을 구할 수 있지만, scale-free network는 power law 분포를 따르기 때문에 분산값이 무한대가 되므로 scale이 없게 된다. 그래서 scale-free network라고 불린다.\n2. Assortativity 자유도가 높은 것은 높은 것끼리, 낮은 것은 낮은 것끼리\n참고사이트  Scale-free Network Assortativity  ","description":"좋은 사이트 정리","id":0,"section":"posts","tags":null,"title":"기본개념 정리","uri":"https://jiwooblog.netlify.app/posts/statistics/network/tip/"},{"content":"실수 Real Number 내용 구성  Field Order  ","description":"","id":1,"section":"posts","tags":null,"title":"Real Number","uri":"https://jiwooblog.netlify.app/posts/mathmaticalanalysis/2_real_number/"},{"content":"1. 추천 사이트  태블로 기초 기능 (네이버 블로그)  ","description":"","id":2,"section":"posts","tags":null,"title":"Tableau 꿀팁","uri":"https://jiwooblog.netlify.app/posts/tableau/tableau_tip/"},{"content":"다중 측정값 활용 Reference [1] 유튜브 PLANIT DATAV\n","description":"","id":3,"section":"posts","tags":null,"title":"다중 측정값 활용","uri":"https://jiwooblog.netlify.app/posts/tableau/multiple_measure/"},{"content":"Chapter 01. Some problems posed on vector spaces 본 포스팅은 Finite Dimensional Linear Algebra (Gokenbach)를 참고하였다.\n1. Linear Equations 2. Best Approximation 2-1. Overdetermined linear systems Orthogonality is a generalization of perpendicularity.\nThe range of any linear operator is a subspace of the co-domain.\n2-2. Best approximation by a polynomial 3. Diagonalization 4. Summary 앞으로 나올 개념들 정리\n vector space field subspace spanning set basis linear operator kernel, range norm inner product orthogonal vectors projection coupled vs. decoupled systems eigenvalues and eigenvectors  Conclusion 기본 개념 정의 확실하게 알아두자. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":4,"section":"posts","tags":null,"title":"Chapter01","uri":"https://jiwooblog.netlify.app/posts/linearalgebra/chapter01/"},{"content":"Intro 생키 차트(Sankey Chart)는 흐름(Flow)을 보여주기에 최적화된 차트 형태이다. 예를 들어 지역이나 국가 간의 에너지를 표현하는데에 적합하다.\n  기본 전처리 코드(R)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  library(tidyverse) library(xlsx) setwd(\u0026#39;C:/Users/bunga/Desktop/Tableau/data\u0026#39;) data \u0026lt;- read.table(\u0026#39;seoul_park_raw.txt\u0026#39;, header=TRUE, sep=\u0026#39;\\t\u0026#39;) data \u0026lt;- as.tibble(data) data \u0026lt;- data %\u0026gt;% mutate(ym = (ym*100)%%100) data \u0026lt;- data %\u0026gt;% select(-total) data \u0026lt;- data[data[\u0026#39;region\u0026#39;] != \u0026#39;합계\u0026#39;,] data \u0026lt;- data %\u0026gt;% mutate(ord = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, ord)), health = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, health)), bicycle = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, bicycle)), event = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, event)), special = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, special)), etc = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, etc))) %\u0026gt;% mutate_all(~ifelse(is.na(.), 0, .)) write.table(data, file = \u0026#34;seoul_park.txt\u0026#34;, sep = \u0026#34;\\t\u0026#34;, row.names = FALSE) write.xlsx(data,file=\u0026#39;seoul_park.xlsx\u0026#39;, sheetName = \u0026#39;Sheet1\u0026#39;)      Reference [1] WeViz 유튜브\n[2] https://qliksense.tistory.com/32\n[3] 서울열린데이터광장\n","description":"","id":5,"section":"posts","tags":null,"title":"생키 차트","uri":"https://jiwooblog.netlify.app/posts/tableau/sankey_chart/"},{"content":"Intro 본 포스팅은 WeViz 유튜브를 적극 참고하였습니다.\n결과물  1. 팔레트 커스텀 추가 내 태블로 리포지토리 Preferences 파일에 색상 커스텀 추가\n본 결과물에는 해당 참고사이트에서 추천한 색상 조합을 적용하였다.\n2. 색상 참고 사이트 2-1. UI gradients (측정값 그라데이션) UI gradients\n2-2. Adobe Color (차원 색조합) Adobe color\nReference [1] WeViz 유튜브\n","description":"","id":6,"section":"posts","tags":null,"title":"한국 픽셀맵","uri":"https://jiwooblog.netlify.app/posts/tableau/korea_pixel_map/"},{"content":"Intro 본 포스팅은 WeViz 유튜브를 적극 참고하였습니다.\n결과물  1. 기본 수정  [맵] - [맵 계층] 스타일 어둡게 변경 기본도, 토지 피복도 체크박스 해제 해안선 체크  2. mapbox 이용하기  mapbox 홈페이지 Studio -\u0026gt; New Style -\u0026gt; Basic(다른 스타일 선택해도 무방)  2-1. 영역 색깔 바꾸기 특정 영역 클릭 후 색깔 변경\n2-2. 폰트 바꾸기 STEP1. 텍스트는 기본적으로 자물쇠 잠금 해제(override) 클릭 후 변경 가능\nSTEP2. 특정 레이블을 선택 -\u0026gt; Components -\u0026gt; Typography\nSTEP3. 원하는 글꼴 검색 후 적용(Noto sans 추천)\n2-3. 영어 레이블을 한글 레이블로 바꾸기 STEP1. 특정 레이블 클릭 -\u0026gt; Layers -\u0026gt; T-country label 클릭\nSTEP2. override를 클릭하여 name_en을 name_ko로 바꿔준다.\n2-4. 배포하기 오른쪽 상단 Publish 클릭 -\u0026gt; Publish as new\n2-5. Tableau로 가져오기 STEP1. mapbox preview only 링크 복사\nSTEP2. Tableau 맵 관리 -\u0026gt; 추가\nSTEP3. 내보내기 꼭 하기! (다음에도 활용하기 위해서!)\nReference [1] WeViz 유튜브\n","description":"","id":7,"section":"posts","tags":null,"title":"맵 커스텀","uri":"https://jiwooblog.netlify.app/posts/tableau/map_custom/"},{"content":"Intro 본 포스팅은 WeViz 유튜브를 적극 참고하였습니다.\n시각화에 관심이 있었는데, 해당 연합동아리 소속의 친구에게 추천을 받아서 참고하게 되었습니다.\n결과물  배운점   구글 스프레드시트 GEOCODE 활용법\n 경도(longitude), 위도(latitude) 자동 추출 기능    MAKELINE, MAKEPOINT 활용법\nMAKELINE(MAKEPOINT(30.602101,114.316826), MAKEPOINT([Latitude],[Longitude]))\n  맵 관리\n 커스텀 맵 디자인 추가    Reference [1] WeViz 유튜브\n[2] 데이터 출처\n","description":"","id":8,"section":"posts","tags":null,"title":"코로나","uri":"https://jiwooblog.netlify.app/posts/tableau/corona_map/"},{"content":"esquisse esquisse는 드래그 앤 드롭(drag \u0026amp; drop)으로 ggplot을 간단하게 그릴 수 있는 획기적인 패키지이다.\n복잡한 커스터마이징은 디테일한 수정이 추가적으로 필요하겠지만, 간단한 특징들을 반복적인 코드수정과 확인과정을 거치기 않고서도 즉각적으로 그래프 모양을 확인할 수 있다는 큰 장점이 있다.\n거의 Tableu 같은 느낌도 든다. 간단한 ggplot 그릴 때 또는 ggplot 입문자가 먼저 거쳐가도 좋을 것 같다.\n1 2 3  library(ggplot2) library(dplyr) library(esquisse)   STEP1. Addins을 클릭하고, ggplot2 builder를 이어서 클릭한다.\n\nSTEP2. validate imported data를 클릭한다.\n\nSTEP3. 드래그 앤 드롭으로 X축, Y축 등을 설정한다.\n\nSTEP4-1. Labels \u0026amp; Title에서는 제목과 축 이름 등을 설정한다.\n\nSTEP4-2. Plot Options에서는 색깔을 포함한 전반적인 테마를 설정한다.\n\nSTEP4-4. Data에서는 표현하고픈 또는 표현하고 싶지 않은 데이터를 필터링한다.\n\nSTEP4-3. Export \u0026amp; Data에서는 Insert code in script를 클릭하면 아래와 같은 코드를 바로 script로 옮겨준다.\n\nSTEP5. 친절하게 라이브러리까지 제시된 코드를 실행한다.\nSTEP6. 추가적으로 더 손 보고 싶은 부분을 개선한다.\n  코드 예시  1 2 3 4 5 6 7 8 9 10 11 12  library(dplyr) library(ggplot2) mpg %\u0026gt;% filter(displ \u0026gt;= 1.6 \u0026amp; displ \u0026lt;= 5.95) %\u0026gt;% filter(year \u0026gt;= 1999 \u0026amp; year \u0026lt;= 2005.2) %\u0026gt;% filter(hwy \u0026gt;= 12L \u0026amp; hwy \u0026lt;= 33L) %\u0026gt;% ggplot() + aes(x = manufacturer, y = cyl) + geom_boxplot(fill = \u0026#34;#18c975\u0026#34;) + scale_y_continuous(trans = \u0026#34;log10\u0026#34;) + theme_gray()     참고 [1] https://www.youtube.com/watch?v=FWLxE-ARuO8\n","description":"","id":9,"section":"posts","tags":null,"title":"esquisse","uri":"https://jiwooblog.netlify.app/posts/r/%EC%8B%9C%EA%B0%81%ED%99%94/esquisse/"},{"content":"naniar 패키지 훑어보기 NA 관련해서 직관적으로 깔끔한 그래프로 훑어볼 수 있게 도와주는 패키지이다.\n본 포스팅은 해당 사이트를 적극참고하여 작성하였다.\n1 2  library(tidyverse) library(naniar)   vis_miss 1  vis_miss(airquality)   gg_miss_var 1  gg_miss_var(airquality)   1  gg_miss_var(airquality, show_pct = TRUE)   1  gg_miss_var(airquality, facet = Month)   gg_miss_case 1  gg_miss_case(airquality)   gg_miss_upset 1  gg_miss_upset(riskfactors)   1  n_var_miss(riskfactors)   ## [1] 24\r1  gg_miss_upset(riskfactors, nsets = n_var_miss(riskfactors))   1  gg_miss_upset(riskfactors, nsets = 4) #nset: 변수 개수   1  gg_miss_upset(riskfactors, nsets = 10, nintersects = 5) #nintersects: 변수조합 수   geom_miss_point ggplot과 응용\n1 2  ggplot(airquality, aes(x = Ozone, y = Solar.R)) + geom_point()   ## Warning: Removed 42 rows containing missing values (geom_point).\r1 2  ggplot(airquality, aes(x = Ozone, y = Solar.R)) + geom_miss_point()   gg_miss_fctfas 1  gg_miss_fct(oceanbuoys, year)   miss_var_summary 1 2 3  riskfactors %\u0026gt;% group_by(marital) %\u0026gt;% miss_var_summary()   ## # A tibble: 231 x 4\r## # Groups: marital [7]\r## marital variable n_miss pct_miss\r## \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Married smoke_stop 120 91.6 ## 2 Married pregnant 117 89.3 ## 3 Married smoke_last 84 64.1 ## 4 Married smoke_days 73 55.7 ## 5 Married drink_average 68 51.9 ## 6 Married health_poor 67 51.1 ## 7 Married drink_days 67 51.1 ## 8 Married weight_lbs 6 4.58\r## 9 Married bmi 6 4.58\r## 10 Married diet_fruit 4 3.05\r## # ... with 221 more rows\rmiss_var_span, gg_miss_span 1  miss_var_span(pedestrian, hourly_counts, span_every = 3000)   ## # A tibble: 13 x 5\r## span_counter n_miss n_complete prop_miss prop_complete\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 0 3000 0 1 ## 2 2 0 3000 0 1 ## 3 3 1 2999 0.000333 1.00 ## 4 4 121 2879 0.0403 0.960\r## 5 5 503 2497 0.168 0.832\r## 6 6 555 2445 0.185 0.815\r## 7 7 190 2810 0.0633 0.937\r## 8 8 0 3000 0 1 ## 9 9 1 2999 0.000333 1.00 ## 10 10 0 3000 0 1 ## 11 11 0 3000 0 1 ## 12 12 745 2255 0.248 0.752\r## 13 13 432 2568 0.144 0.856\r1  gg_miss_span(pedestrian, hourly_counts, span_every = 3000)   1  gg_miss_span(pedestrian, hourly_counts, span_every = 3000, facet = sensor_name)   그외 다양한 1  gg_miss_case_cumsum(airquality)   1  gg_miss_var_cumsum(airquality)   1  gg_miss_which(airquality)   ","description":"","id":10,"section":"posts","tags":null,"title":"naniar","uri":"https://jiwooblog.netlify.app/posts/r/%EC%8B%9C%EA%B0%81%ED%99%94/naniar/"},{"content":"Machine Learning  주어진 데이터를 통해서 입력변수와 출력변수 간의 관계를 만드는 함수 $f$를 만드는 것 주어진 데이터 속에서 데이터의 특징을 찾아내는 함수 $f$를 만드는 것  1. 기본 개념구분  지도 학습: 회귀(Regression), 분류(Classification) 비지도 학습: PCA, 군집분석 강화 학습: 수많은 시뮬레이션을 통해 현재의 선택이 먼 미래에 보상이 최대로 하는 action을 학습  2. 다양한 머신러닝 기법  선형회귀분석: 선형관계를 가정하여, 독립변수의 중요도와 영향력 파악 DT(Decision Tree): 독립변수의 조건에 따라 종속변수를 분리 KNN(K-Nearest Neighbor): 새로 들어온 데이터의 주변 K개의 데이터의 class로 분류 NN(Neural Network): 입력층/은닉층/출력층 으로 구성된 모형. 각 층을 연결하는 노드의 가중치를 업데이트하며 학습 SVM(Support Vector Machine): class 간 거리가 최대가 되도록 decision boundary 만드는 방법 K-means Clustering: Label 없이 데이터의 군집 k개 생성 Ensemble Learning: 여러 개의 모델을 결합하여 사용하는 모델로, 구체적으로는 다양한 알고리즘 종류가 있다.\n7-1. Bagging: 모델을 다양하게 만들기 위해 데이터를 재구성\n7-2. Random Forest: 모델을 다양하게 만들기 위해 데이터뿐만 아니라 변수도 재구성\n7-3. Boosting: 맞추기 어려운 데이터에 대해 좀 더 가중치를 두어 seqeuntial하게 학습하는 개념 (ex. AdaBoost, Gradient Boosting(Xgboost, LightGBM, CatBoost)\n7-4. Stacking: 모델의 output값을 새로운 독립변수로 활용 Deep Learning: 딥러닝은 사실 머신러닝의 부분집합이다. 하지만 워낙 깊고 다양하기에 따로 다루도록 하겠다.  3. 모형의 적합성 평가 및 실험설계 데이터를 Training-Validation-Test, 총 세 가지 세트로 나눈다.\nK-Fold Cross Validation 데이터를 k개 부분으로 나누 뒤, 하나를 검증집합 나머지를 학습집합으로 한다. 이 과정을 k번 반복해서 k개의 성능지표를 구하고 그것들의 평균을 구한다.\nLOOCV(Leave One Out Cross Validation) 데이터를 k개의 부분으로 나누기에 부족할 때, 데이터 한 개씩을 빼가면서 K-fold CV를 하는 방식과 똑같이 한다.\n4. 과적합(Overfitting) 머신러닝에서 가장 주의해야 할 것 중 하나가 바로 과적합이다. 이와 관련해서는 Bias-Variance Tradeoff에 대한 이해가 필요하다. 그리고 아주 간단하게 이해하기 위해서는 아래 두 사진을 참고하면 될 것이다.\n참고 [1] https://medium.com/@cs.sabaribalaji/overfitting-6c1cd9af589\n[2] https://www.researchgate.net/figure/The-overfitting-of-model-a-training-error-and-true-error-b-depiction-of-Eq-33_fig5_333505702\n","description":"머신러닝 개괄","id":11,"section":"posts","tags":null,"title":"ML Intro","uri":"https://jiwooblog.netlify.app/posts/machinelearning/intro/"},{"content":"\r\r중심극한정리(Central Limit Theorem)\r\r모분포가 어떤 분포이든 상관없이, 평균이 $\\mu$이고, 분산이 $\\sigma^2$인 분포로부터 무작위로 n개의 샘플을 얻는 다고 가정하자. (이때, $\\sigma^2 \u0026lt; \\infty$라는 조건을 만족한다고 가정하자.)\rn이 충분히 크면, 표본평균의 분포는 평균이 $\\mu$이고, 분산이 $\\sigma^2/\\sqrt{n}$인 정규분포로 근사한다.\r이를 수식으로 표현하면, $Y_n = \\frac{\\sqrt{n}(\\bar{X_n} - \\mu)}{\\sigma} \\stackrel{\\cdot}\\sim N(0,1) $\n\r*주의사항: 표본이 아니라 표본평균의 분포에 대한 이야기이다.\n\rMGF(적률생성함수)를 활용한 증명\r우선, 정규분포의 mgf가 아래와 같다는 사실을 인지하고, mgf가 분포결정성을 갖는다는 것을 받아들이고 시작하자.\n정규분포의 MGF\r\\[X \\sim N(\\mu, \\sigma^2) \\\\\r\\rightarrow M(t) = E[e^{tX}] = exp(\\mu t + \\frac{\\sigma^2}{2}t^2) \\]\n정의\r\\[\\text{Let } Z_i = \\frac{X_i-\\mu}{\\sigma}, m(t) = mgf_{Z_1}(t) \\\\\r\\text{and notice that } E(Z_i) = 0, Var(Z_i) = 1\\]\n\\[\\text{Let } W = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} = \\frac{\\sum_{i=1}^{n} x_i - n\\mu}{\\sqrt{n}\\sigma} \\\\\r\\text{WTS: } M_W(t) = exp(\\frac{t^2}{2}) \\]\n증명\r\\[\\begin{align}\rmgf_{\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}}(t) \u0026amp;= mgf_{\\sqrt{n}\\sqrt{\\bar{Z}}}(t) \\ (\\because \\bar{Z} = \\frac{\\sum X_i - n\\mu}{n\\sigma} = \\frac{\\bar{X}-\\mu}{\\sigma}) \\\\\r\u0026amp;= E\\Big[exp(t\\sqrt{n}\\bar{Z})\\Big] \\\\\r\u0026amp;= E\\bigg[exp\\Big(\\frac{t}{\\sqrt{n}}(Z_1 + \\cdots = Z_n)\\Big)\\bigg] \\\\\r\u0026amp;= \\bigg(E[exp(\\frac{t}{\\sqrt{n}}Z_1)]\\bigg)^n \\ (\\because iid) \\\\\r\u0026amp;= \\Big[m(\\frac{t}{\\sqrt{n}})\\Big]^n\r\\end{align}\\]\n\\[1) \\ log(1+A) = A - \\frac{1}{2}A^2 + \\frac{1}{3}A^3 - \\frac{1}{4}A^4 + \\cdots \\\\\r2) \\ m(0) = 1, m\u0026#39;(0) = E(Z_1) = 0, m\u0026#39;\u0026#39;(0) = E(Z_1^2) = Var(Z_1) = 1\\]\n\\[\\begin{align}\r\\therefore log \\ mgf_{\\sqrt{n}\\sqrt{\\bar{Z}}} \u0026amp;= n \\cdot log \\ m(\\frac{t}{\\sqrt{n}}) \\\\ \u0026amp;= n \\cdot log (m(0)+ \\frac{m\u0026#39;(0)}{1!}\\frac{t}{\\sqrt{n}} + \\frac{m\u0026#39;\u0026#39;(0)}{2!}\\frac{t^2}{n} + o(\\frac{1}{n})) \\\\\r\u0026amp;= n \\cdot log (1+\\frac{t^2}{2n} + o(\\frac{1}{n})) \\\\\r\u0026amp;= n \\cdot (\\frac{t^2}{2n} + o(\\frac{1}{n}) - \\frac{1}{2}\\Big(\\frac{t^2}{2n} + o(\\frac{1}{n})\\Big)^2 + \\cdots) \\\\\r\u0026amp;= n \\cdot \\Big(\\frac{t^2}{2n} + o(\\frac{1}{n})\\Big) \\\\\r\u0026amp;= \\frac{1}{2}t^2 + o(1) \\\\\r\\therefore mgf_{\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}}(t) \u0026amp;\\stackrel{n\\rightarrow\\infty}\\rightarrow exp(\\frac{1}{2}t^2)\r\\end{align}\\]\n\r","description":"","id":12,"section":"posts","tags":null,"title":"중심극한정리","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/central_limit_theorem/"},{"content":"확률분포(Probability Distribution) 출처: https://artificialnetworkforstarters.readthedocs.io/en/latest/_post/chap6.html  -- 연속형(Continuous)  정규 분포 T-분포 감마 분포 베타 분포 카이제곱 분포 F-분포 균일 분포 디리클레 분포 위샤트 분포\n\u0026hellip;  이산형(Discrete)  이항 분포 다항 분포 베르누이 분포 포아송 분포 음이항 분포 기하 분포 초기하 분포\n\u0026hellip;  정규 분포(Normal Distribution)   정규분포  $$ \\text{X~} N(\\mu, \\sigma^2) \\rightarrow f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{(x-\\mu)}{2\\sigma^2}^2) $$\n$$ E(X) = \\mu, Var(X) = \\sigma^2$$   다변수 정규분포(Multivariate Normal Distribution) 관련 포스팅 참고\nT-분포(Student\u0026rsquo;s t-Distribution)   T-분포  $$ \\text{X~} t(n) \\rightarrow f(x) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\Gamma(\\frac{n}{2})\\cdot\\sqrt{\\pi n}}(\\frac{n}{x^2+n})^\\frac{n+1}{2} \\ \\text{ for } -\\infty\u0026lt;x\u0026lt;\\infty$$\n$$ E(X) = 0, Var(X) = \\frac{n}{n-2} $$\nT분포는 기본적으로 통계검정을 하기 위해서 작위적으로 고안된 분포이다.\nT분포의 탄생 과정은 아래와 같다.\n$$ T = \\frac{Z}{\\sqrt{\\frac{V}{\\nu}}} \\text{~ } t(df)$$\nwhere $Z\\text{~ }N(0,1), V\\text{~ } \\chi^2(\\nu)$\n  이항 분포(Binomial Distribution)   이항분포  $$ \\text{X~} Binom(n, p) \\rightarrow f(x) = \\binom{n}{x}p^x(1-p)^{n-x} $$\n$$ E(X) = np, Var(X) = np(1-p) $$   음이항 분포(Negative Binomial Distribution) n번째 시행에서 r번째 성공을 확률을 구하고자 할 때 활용한다.\n음이항 분포에서는 시행 횟수 n이 확률변수이고 성공 횟수 r이 고정되어 있는 반면, 이항분포에서는 시행 횟수 n이 고정되어 있고 성공 횟수 r이 확률변수이다. 그래서 음이항분포라고 이름지어진 것이다.\n  음이항 분포(X : r번째 성공을 얻을 때까지 시행횟수)  $$\\text{X~} Negative \\ Binomial(r, p) $$\n$$\\rightarrow f(x) = \\binom{x-1}{r-1}p^r(1-p)^{x-r}, \\ x=r, r+1, \u0026hellip; $$\n$$ E(X) = \\frac{r}{p}, \\ Var(X) = r \\cdot\\frac{1-p}{p^2}$$  \n여기서 x = r+y로 하여, r번째 성공을 얻을 때까지 실패횟수를 계산하면 아래와 같다.\n해당 확률분포 형태에 따라 음이항분포라고 이름지어진 것임을 다시 한 번 확인할 수 있다.\n  음이항 분포(Y : r번째 성공을 얻을 때까지 실패횟수)  $$\\text{Y~} Negative \\ Binomial(r, p) $$\n$$\\rightarrow f(y) = \\binom{r+y-1}{r-1}p^r(1-p)^y, \\ y=0,1,2,\u0026hellip;$$\n$$\\rightarrow f(y) = (-1)^y\\binom{-r}{y}p^r(1-p)^y, \\ y=0,1,2,\u0026hellip; $$\n$$ E(Y) = r\\cdot\\frac{1-p}{p}, \\ Var(Y) = r \\cdot\\frac{1-p}{p^2}$$  \n감마 분포(Gamma Distribution)   감마분포 (알파: shape, 베타: scale)  $$ \\text{X~} Gamma(\\alpha, \\beta) \\rightarrow f(x) = \\frac{1}{\\beta^\\alpha\\cdot\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}$$\n$\\text{for } x\u0026gt;0, \\ \\alpha\u0026gt;0, \\ \\beta\u0026gt;0 $\n$$ E(X)=\\alpha\\beta, \\ Var(X)=\\alpha\\beta^2 $$  --  Gamma Distribution: k=alpha, theta=beta로 생각하기  -- 참고로, 포아송분포, 지수분포, 카이제곱분포와의 연관성을 생각하면 베타를 rate parameter로 보는 것이 좋다.\n베타를 scale로 보는 방식은 가려두도록 하겠다.\n  감마분포 (알파: shape, 베타: rate)  $$ \\text{X~} Gamma(\\alpha, \\beta) \\rightarrow f(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}$$\n$\\text{for } x\u0026gt;0, \\ \\alpha\u0026gt;0, \\ \\beta\u0026gt;0 $\n$$ E(X)=\\frac{\\alpha}{\\beta}, \\ Var(X)=\\frac{\\alpha}{\\beta^2} $$  \n  역감마 분포 (inverse-Gamma)  $$ \\text{X~} inv-Gamma(\\alpha, \\beta) \\rightarrow f(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left(\\frac{1}{x}\\right)^{\\alpha+1}exp(-\\frac{\\beta}{x}) $$\n$\\text{for } x\u0026gt;0, \\ \\alpha\u0026gt;0, \\ \\beta\u0026gt;0 $\n$$ E(X)=\\frac{\\beta}{\\alpha-1}, \\ Var(X)=\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)} \\ \\text{for } \\alpha\u0026gt;1$$     스케일된 역감마분포 (scaled inverse-Gamma)  $$\\text{X~} \\chi^{-2}(\\nu, \\tau^2) = \\Gamma^{-1}(\\nu/2, \\nu\\tau^2/2) $$\n$$\\rightarrow f(x) = \\frac{(\\nu\\tau^2/2)^{\\nu/2}}{\\Gamma(\\nu/2)}\\left(\\frac{1}{x}\\right)^{\\nu/2+1}exp(-\\frac{\\nu\\tau^2}{2x}) $$   베타 분포(Beta Distribution)   베타 분포  $$ \\text{X~} Beta(\\alpha, \\beta) \\rightarrow f(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} $$\n$\\text{for } x\\in[0,1], \\ \\alpha\u0026gt;0, \\ \\beta\u0026gt;0 $\n$$ E(X) = \\frac{\\alpha}{\\alpha+\\beta}, \\ Var(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}, \\ mode(X) = \\frac{\\alpha-1}{\\alpha+\\beta-2} (\\text{단}, \\alpha\u0026gt;1, \\ \\beta\u0026gt;1) $$   디리클레 분포(Dirichlet Distribution) 디리클레 분포는 베타분포의 다변량 버전이다.\n  디리클레 분포  $$\\theta \\text{ ~ } Dirichlet(\\alpha) \\rightarrow p(\\theta) = \\frac{1}{B(\\alpha)}\\prod_{j=1}^{k}\\theta_j^{\\alpha_j-1} $$\n$$\\text{for } B(\\alpha) = \\frac{\\prod\\Gamma(\\alpha_j)}{\\Gamma(\\sum \\alpha_j)}, \\sum_{j=1}^{k}\\theta_j=1 $$  \n포아송 분포(Poisson Distribution) 정해진 시간 안에 어떤 사건이 일어날 횟수에 대한 기댓값을 $\\lambda$ 라고 했을 때, 그 사건이 n회 일어날 확률은 다음과 같다.\n  포아송 분포  $$ \\text{X~} Pois(\\lambda) \\rightarrow f(x) = \\frac{\\lambda^x e^\\lambda}{x!}$$\nfor $x$: 0이상의 정수, $\\lambda\u0026gt;0$\n$$ E(X) = Var(X) = \\lambda $$  \n\n지수 분포(Exponential Distribution) 사건이 서로 독립적일 때, 일정 시간동안 발생하는 사건의 횟수가 푸아송 분포를 따른다면, 다음 사건이 일어날 때까지 대기 시간 또는 사건이 한 번 일어날 때까지 걸리는 시간은 지수분포를 따른다. 지수분포는 감마분포의 특수한 형태이다.\n  지수 분포  $$\\text{X~} exp(\\lambda) \\rightarrow f(x) = \\lambda e^{\\lambda x} $$\n$exp(\\lambda) = \\Gamma(1,\\lambda) $ where $\\beta$위치의 모수가 rate parameter를 뜻할 때!\n$$ E(X) = \\frac{1}{\\lambda}, \\ Var(X)=\\frac{1}{\\lambda^2} $$  \n\n카이제곱 분포(Chi-squared Distribution) $\\nu$개의 서로 독립적인 표준정규분포 확률변수를 각각 제곱한 다음 합해서 얻어지는 분포이다.\n이때 $\\nu$를 자유도라고 하며, 카이제곱 분포의 매개변수가 된다.\n  카이제곱 분포  $$\\text{X~} \\chi^2(\\nu) \\rightarrow f(x) = \\frac{(\\frac{1}{2})^\\frac{\\nu}{2}}{\\Gamma(\\frac{\\nu}{2})}x^{\\frac{\\nu}{2}-1}e^{-x/2} $$\n$\\chi^2(\\nu) = \\Gamma(\\frac{\\nu}{2}, \\frac{1}{2}) $ where $\\beta$위치의 모수가 rate parameter를 뜻할 때!\n$$ E(X) = \\nu, \\ Var(X) = 2\\nu$$  \n\n  역카이제곱 분포 (inverse Chi-squared)  $$\\text{X~} \\chi^{-2}(\\nu) = \\Gamma^{-1}(\\nu/2, 1/2) $$\n$$\\rightarrow f(x) = \\frac{(1/2)^{\\nu/2}}{\\Gamma(\\nu/2)}\\left(\\frac{1}{x}\\right)^{\\nu/2+1}exp(-\\frac{1}{2x}) $$     스케일된 역카이제곱분포 (scaled inverse chi-squard)  $$\\text{X~} \\chi^{-2}(\\nu, \\tau^2) = \\Gamma^{-1}(\\nu/2, \\nu\\tau^2/2) $$\n$$\\rightarrow f(x) = \\frac{(\\nu\\tau^2/2)^{\\nu/2}}{\\Gamma(\\nu/2)}\\left(\\frac{1}{x}\\right)^{\\nu/2+1}exp(-\\frac{\\nu\\tau^2}{2x}) $$   라플라스 분포(Laplace Distribution) 지수분포를 두 개 붙여놓은 것 같다고 하여 double-exponential distribution이라고도 불린다.\n  라플라스 분포 (Laplcace Distribution)  $$\\text{X~} Laplace(\\mu, b) \\rightarrow f(x) = \\frac{1}{2b}exp(-\\frac{|x-\\mu|}{b}) $$\n$$ E(X) = \\mu, Var(X) = 2b^2 $$  \n\n사진 출처 [1] https://artificialnetworkforstarters.readthedocs.io/en/latest/_post/chap6.html\n[2] 위키백과\n","description":"","id":13,"section":"posts","tags":null,"title":"확률분포","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/probability_distribution/"},{"content":"Index  통계학이란?  표본분포 모수 검정 \u0026amp; 비모수 검정   기술통계  데이터 유형 분포의 이해 CLT   가설검정  양방향 \u0026amp; 단방향   정규성 검정  Kolmogorov-Smirnov test(표본 50개 이상) Shapiro-Wilk test(표본 50개 이하) 모수적 접근 \u0026amp; 비모수적 접근   단일집단 평균  모수적 접근(one sample t-test) 비모수적 접근(Kolmogorov-Smirnov test, Runs 검정)   독립 두 집단 평균  모수적 접근(independent t-test) 비모수적 접근(Mann-Whitney U-test, Wilcoxon rank-sum test, Kolmogorov-Smirnov test, 중앙값 검정)   대응 두 집단 평균  모수적 접근(paired t-test) 비모수적 접근(sign test, Wilcoxon signed rank test)   독립 세 집단 이상 평균  모수적 접근(ANOVA) 비모수적 접근(Kruskal-Wallis test) 사후분석(Bonferroni)   종속 세 집단 이상 평균  비모수적 접근(Cochran Q test, Friedman test)   카이제곱검정(범주형 데이터)  독립성 검정 적합도 검정 동질성 검정   상관분석  모수적 접근 비모수적 접근(Spearman 상관계수, Kendall 순위 상관계수, Kendall 편 순위 상관계수, Kendall 일치도 계수)    ","description":"","id":14,"section":"posts","tags":null,"title":"개요","uri":"https://jiwooblog.netlify.app/posts/spss/prologue/"},{"content":"미분(Differentiation) 1. Chain Rule $$\\frac{dy}{dx} = \\frac{dy}{du} \\times \\frac{du}{dx}$$\n2. Product Rule $$\\frac{dy}{dx} = \\frac{du}{dx}v + u\\frac{dv}{dx}$$\n이를 다르게 쓰면,\n$$y = f(x)g(x)$$\n$$\\rightarrow y'=f'(x)g(x)+f(x)g'(x)$$\n3. Quotient Rule $$y = \\frac{f(x)}{g(x)}$$\n$$\\rightarrow y' = \\frac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}$$\n4. Implicit Differentiation $$\\frac{d}{dx}[f(y)] = \\frac{d}{dy}[f(y)] \\times \\frac{dy}{dx} $$\n","description":"","id":15,"section":"posts","tags":null,"title":"미분","uri":"https://jiwooblog.netlify.app/posts/statistics/calculus/1_differentiation/"},{"content":"1. 추천 사이트  pandas_exercises  2. 추천 책  Python for Data Analysis (Wes McKinney)  3. 검색 팁 구글링 시, 뒤에 \u0026lsquo;towards data science\u0026rsquo; 또는 \u0026lsquo;medium\u0026rsquo; 붙이기\n하루 열람 제한이 있는데, 더 읽고 싶은 경우는 Chrome 시크릿 모드를 활용하면 제한이 풀린다.\n","description":"","id":16,"section":"posts","tags":null,"title":"Python 꿀팁","uri":"https://jiwooblog.netlify.app/posts/python/python_tip/"},{"content":"2021.1.30  또 다시 보니 11.18달러 과금으로 계속 증가하고 있어서 이것저것 최대한 삭제하고 계정도 해지했다. 90일 동안 또 다시 과금이 되는지 지속적으로 살펴봐야겠다. 그리고 해지한 메일 계정으로는 다시 계정 개설이 안된다길래 hanmail.net으로 수정한 후 삭제했다. 참고사이트는 두 곳을 참고했다. 참고사이트1, 참고사이트2 제발 다음에는 과금 안 되길\u0026hellip;AWS는 너무 어려운 것 같다. 나중에 AWS에 조금 더 관심이 생기면 참고사이트2의 다양한 포스트들을 참고해도 좋을 것 같다.  2021.1.15  다시 보니 10.68달러 과금으로 증가해있어서 얼른 RDS를 삭제했다. 교훈: 돌다리도\u0026hellip; 다음주부터는 주식 관련해서도 update 해야겠다.  2021.1.14  FastCampus 따라하다가 AWS 과금 4.13달러 발생했다\u0026hellip; Free Tier라고 모든게 다 공짜는 아니었다\u0026hellip;  2021.1.11  Data Engineering에 관심이 생겼다.  2021.1.8  First Course in Bayesian Statistical Methods(이하 FCB) 1장을 정리해서 올렸다. 오래된 Github Repository를 정리했는데\u0026hellip;이들을 삭제하면 각 repository와 연관된 commit 기록들도 사라지기 때문에 잔디 심어둔 것들이 사라져버리는 사태가 발생했다\u0026hellip;아깝다\u0026hellip; 교훈: repository는 신중하게 만들고, 가능하면 지우지 말자\u0026hellip;  2021.1.7  LaTex 수식 삽입기능 추가 ( 참고사이트 )  2021.1.4  baseURL 수정 댓글 기능 확인  블로그 제작일: 2021년 1월 2일 토요일  ","description":"","id":17,"section":"updates","tags":null,"title":"January 2021","uri":"https://jiwooblog.netlify.app/updates/diary/2021_01/"},{"content":"To Do List 1. Want to Upload Posts   프로젝트들 정리\n Rhino: 수소차 충전소 입지 선정 [ESC 2020_SUMMER] 빅콘테스트 NS Shop+ 심리성향 예측 AI 경진대회 [ESC 2020_FALL] NLP 논문 따라해보기 [ESC 2020_SPRING] 행복지수 예측\n- 배운 점: 해석 시 Domain Knowledge의 중요성, FA의 효과    R 다양한 기능들 정리\n dpylr: group_by, summarise, mutate, select, filter 시각화\n2-1. ggplot2\n2-2. plotly\n2-3. highcharter\n2-4. g2r\n2-5. rayshader\n2-6. ggmap\n2-7. r4issactoast\n2-8. naniar(gg_miss_upset)\n2-9. gganimate\n2-10. esquisse: 드래그\u0026amp;드롭으로 ggplot 그리기 shiny NA imputation\n4-1. mice visNetwork(visNodes, visEdges) apriori(연관성분석, 장바구니분석) xaringan(R로 PPT 만들기)\n7-1. 참고사이트1\n7-2. 참고사이트2 Rstudio Ligature 적용하기 참고사이트 latex2exp: expression 대신 latex문법 활용하기    SPSS\n 기술통계(데이터 유형, 분포의 이해) 표본분포의 개념 정규성 검정(모수적 접근 \u0026amp; 비모수적 접근) 가설검정 단일집단 평균 독립 두 집단 평균 대응 두 집단 평균 독립 세 집단 이상 평균 카이제곱검정(범주형 데이터)    Statistics\n0. SPSS 내용 대부분\n 분포 간 관계도 정리 mgf(moment generating function)\n2-1. 테일러급수 normal sampling theory 증명    Updates  주식  gitignore나 config.toml ignoreFiles에 추가해두기 매달 초 기록하기 매 매매를 기준으로 계산해보기 코스피, 코스닥, 나스닥, S\u0026amp;P500, 다우존스, 환율 등의 지표와 지수들을 정리하기 슈카월드 정리하기(미정)    Blog  Blogdown으로 블로그 만들기  blogdown 설치 및 사이트 개설\n1-1. 테마 추천(가장 유명한 Academic, Rstudio에서 공식적으로 밀고 있는 Apero, 깔끔한 distill ) github 업로드 방식 (Terminal 사용법)\n2-1. 세부 오류 수정(자격증명 관리자-Windows 자격증명)참고사이트\n2-2. git origin 수정하기 참고사이트\n2-3. .Rprofile 수정하기 (blogdown.knit.on_save = TRUE) 테마 선택하기 / Markdown 문법 기본 소개 세부수정하기 1) baseURL 세부수정하기 2) (ex. config.toml의 baseURL, 이미지 삽입) Comment 기능 추가하기(disqus, gitalk) LaTex 수식 리스트 (md와 RMD 구분하기) Netlify로 배포하기 블로그탭 바꾸기(static-\u0026gt;favicon) favicon 변환사이트 코드 결과물 글씨색깔 바꾸기 (assets / sass / themes / _lightcode.scss 중에서 content-code-in-pre-color 바꿔주기) (단, zdoc theme에 한정된 것일 수도!)\n10-1. 위의 방법이 안된다면, config.toml 중 \u0026lsquo;markup.highlight\u0026rsquo;에 \u0026lsquo;style=\u0026ldquo;fruity\u0026rdquo;\u0026lsquo;와 같은 style 추가하기 style 참고사이트    --- 2. Want to Know  Rmd로 plot 그릴 때, data를 어디에 넣어놔야 하는지  ","description":"","id":18,"section":"updates","tags":null,"title":"To Do List","uri":"https://jiwooblog.netlify.app/updates/todo_list/"},{"content":"2021년 7월    주문날짜 제품명 브랜드 금액 쇼핑몰 비고     07.09 오픈카라셔츠 검정/옐로우 비슬로우 퍼플 64,000원 무신사    07.15 후드 플리츠 스웨트셔츠 자라 15,000원 자라 공홈     2021년 6월    주문날짜 제품명 브랜드 금액 쇼핑몰 비고     06.13 스톤 팔찌 이자벨마랑 56,005원 미스터포터    06.15 플뢰르 드 패츌리(오드 퍼퓸) 40ml 자라 32,000원 자라    06.19 자자나스 반팔 L 자바나스 19,600원 자바나스 우주형 선물   06.28 생로랑 팔찌 YSL $117.6 ln-cc     2021년 5월    주문날짜 제품명 브랜드 금액 쇼핑몰 비고     05.03 지오다노 피케티 베이지, 오트밀 지오다노 21,900원 롯데온    05.04 1507 양말 1507 9,094원 무신사    05.10 Tarr 44 Olive Toltoise 애쉬컴팩트 62,220원 무신사    05.22 수아레 린넨 바지(카키\u0026amp;블랙) 수아레 35,640원 W컨셉    05.24 반투명 글라스 화병 \u0026amp; 화이트 나시 H\u0026amp;M 4,220원 H*M    05.24 Boom 3D(Audio Equalizer for Windows) Boom 3D 4,220원 Boom 3D    05.25 유니클로 U와이드피트스탠드칼라스트라이프셔츠(긴팔) 다크그레이 \u0026amp; 핑크 유니클로 28,800원 유니클로 공홈     2021년 4월    주문날짜 제품명 브랜드 금액 쇼핑몰 비고     4.08 캉골 트로픽 헌팅캡 504 BEIGE (S사이즈) 캉골 42,000원 네이버스토어 밴테어로 변경 수령   4.10 메종 마르지엘라 넘버링 목걸이 메종 마르지엘라 150,000원 오진성(직거래)    4.11 U샤이니레이온폴로튜닉(반팔), U와이드피트스탠드칼라스트라이프셔츠(긴팔) 유니클로U 32,800원 유니클로 공홈    4.28 U릴랙스피트테이퍼드팬츠, U릴랙스피트테이퍼드재킷 유니클로U 42,800원 유니클로 공홈     2021년 3월    주문날짜 제품명 브랜드 금액 쇼핑몰 링크     3.12 캉골 트로픽 헌팅캡 504 BEIGE (S사이즈) 캉골 39,647원 SSG닷컴 링크   3.19 인사일런스 구르카 팬츠 Indigo (S 사이즈) 인사일런스 63,000원 공홈 링크    2021년 2월    주문날짜 제품명 브랜드 금액 쇼핑몰     2.6 뉴발란스 327 오렌지 뉴발란스 114,000원 크림   2.23 캉골 트로픽 헌팅캡 504 BEIGE (M사이즈, 반품) 캉골 37,850원 인터파크   2.24 19ss easy pants brick 유니폼브릿지 15,000원 에딧에디션   2.24 19fw fisherman cardigan orange 유니폼 브릿지 25,000원 에딧에디션    ","description":"","id":19,"section":"updates","tags":null,"title":"쇼핑 리스트","uri":"https://jiwooblog.netlify.app/updates/shopping_list/"},{"content":"Part 2-1. 데이터 엔지니어 기초 다지기 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Unix 환경 및 커맨드 cd\nmkdir\nls\ncp\n./run.sh\nchmod +x run.sh: Permission denied 되었을 때, 권한 부여하기\nrun.sh 코드(참고용) #!/bin/bash\rpython examply.py 1 \u0026gt; example.txt\rpython examply.py 2 \u0026gt; example.txt\r*주의사항: python 대신에 python3를 쓰면 Windows에서는 오류가 날 수도 있다.\n2. AWS 기초 및 CLI 세팅 AWS CLI (Command Line Interface) 다운로드\n IAM에서 사용자 추가하기 Windows Powershell에다가 aws configure 입력하기 액세스 키 ID와 비밀 액세스 키 입력하기  이는 앞으로 shell에서 aws command를 쳐도 가능하게끔 세팅해놓는 것이다.\n","description":"데이터 엔지니어 기초 다지기","id":20,"section":"posts","tags":null,"title":"Unix \u0026 AWS","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part2_1/"},{"content":"1. Drawing MVN plots with ggplot2 1 2  mu = matrix(c(0,10), ncol=1) invSig = solve(matrix(c(4,10,10,100), ncol=2, byrow=TRUE))   1-1. Vectorize + Outer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  dmvlnorm = function(theta, mu, invSig){ (-nrow(mu)/2) * log(2*pi) + 0.5*log(det(invSig)) - 0.5*(t(theta-mu) %*% invSig %*% (theta-mu)) } calc.dens = Vectorize(function(a,b){ theta = c(a,b) exp(dmvlnorm(theta, mu, invSig)) }) A = seq(-5, 5, length=100) B = seq(-15, 40, length=100) dense = outer(A, B, FUN=calc.dens) rownames(dense) = A colnames(dense) = B dens = reshape2::melt(dense) colnames(dens) = c(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;dens\u0026#39;) ggplot(data=dens, aes(x=a, y=b)) + geom_raster(aes(fill=dens, alpha=dens), interpolate=TRUE) + geom_contour(aes(z=dens), color=\u0026#39;black\u0026#39;, size=0.2) + scale_fill_gradient(low=\u0026#39;cornflowerblue\u0026#39;, high=\u0026#39;steelblue\u0026#39;, guide=FALSE) + scale_alpha(range=c(0,1), guide=FALSE) + labs(title=\u0026#39;MVN density\u0026#39;, x=\u0026#39;alpha\u0026#39;, y=\u0026#39;beta\u0026#39;)   2. Gibbs sampling for MVN draws 1 2 3  Y = dget(\u0026#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/Y.reading\u0026#39;) ggplot(data.frame(Y)) + geom_point(aes(x=pretest, y=posttest))   Prior specification 1 2 3 4  Mu0 \u0026lt;- c(50,50) Lambda0 = matrix(c(625,312.5,312.5,625), ncol=2) nu0 = 4 S0 = (nu0-nrow(Lambda0)-1) * Lambda0   Gibbs Sampler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  inv = solve n = nrow(Y) ybar = colMeans(Y) Sigma = cov(Y) #initials S = 5000 MU = matrix(NA, nrow=S, ncol=2) SIGMA = matrix(NA, nrow=S, ncol=4) for(s in 1:S){ # update Mu Lambdan = inv(inv(Lambda0) + n*inv(Sigma)) Mun = Lambdan %*% (inv(Lambda0) %*% Mu0 + n*inv(Sigma) %*% ybar) Mu = MASS::mvrnorm(n=1, Mun, Lambdan) # update Sigma Sn = S0 + (t(Y)-c(Mu)) %*% t((t(Y)-c(Mu))) Sigma = inv(rWishart(1, nu0+n, inv(Sn))[,,1]) MU[s,] = Mu SIGMA[s,] = c(Sigma) } disp = tail(1:S, S/2) p1 \u0026lt;- data.frame(mu1=MU[disp,1], mu2=MU[disp,2]) %\u0026gt;% ggplot(aes(x=mu1, y=mu2)) + geom_point(size=0.5, color=\u0026#39;steelblue\u0026#39;) + geom_abline(slope=1, intercept=0) + coord_fixed(ratio=1) + ggtitle(\u0026#39;Posterior darws of MU\u0026#39;) meandiff = MU[disp,2] - MU[disp,1] p2 \u0026lt;- data.frame(meandiff=meandiff) %\u0026gt;% ggplot(aes(x=meandiff)) + geom_histogram(color=\u0026#39;white\u0026#39;, fill=\u0026#39;steelblue\u0026#39;, bins=30) + geom_vline(xintercept=0) + ggtitle(\u0026#39;Posterior draws of Mu2 - Mu1\u0026#39;) grid.arrange(p1, p2, ncol=2)   3. Gibbs Sampling for NA imputation 1 2  Y = dget(\u0026#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/Y.pima.miss\u0026#39;) head(Y)   ## glu bp skin bmi\r## 1 86 68 28 30.2\r## 2 195 70 33 NA\r## 3 77 82 NA 35.8\r## 4 NA 76 43 47.9\r## 5 107 60 NA NA\r## 6 97 76 27 NA\r1  psych::pairs.panels(Y, method=\u0026#39;pearson\u0026#39;, density=T, breaks=20, hist.col=\u0026#39;steelblue\u0026#39;)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  # priors n = nrow(Y) p = ncol(Y) Mu0 = c(120,64,26,26) sd0 = Mu0/2 L0 = matrix(0.1, p, p) diag(L0) = 1 L0 = L0*outer(sd0,sd0) nu0 = p+2 S0 = (nu0-p-1)*L0 Sigma = S0 Y.full = Y O = 1*(!is.na(Y)) for(j in 1:p){ Y.full[is.na(Y.full)[,j], j] = mean(Y.full[,j], na.rm=TRUE) #mean imputation } inv = solve S = 100 for(s in 1:S){ # update Mu ybar = colMeans(Y.full) Ln = inv(inv(L0) + n*inv(Sigma)) Mun = Ln %*% (inv(L0) %*% Mu0 + n*inv(Sigma) %*% ybar) Mu = MASS::mvrnorm(n=1, Mun, Ln) # update Sigma Sn = S0 + (t(Y.full) - c(Mu)) %*% t((t(Y.full) - c(Mu))) Sigma = inv(rWishart(1, nu0+n, inv(Sn))[,,1]) # update missing data for(i in 1:n){ #iterate over rows b = (O[i,] == 0) #missing at each row a = (O[i,] == 1) #observed at each row if(sum(b)==0) next iSa = inv(Sigma[a,a]) beta.j = Sigma[b,a] %*% iSa Sigma.j = Sigma[b,b] - Sigma[b,a] %*% iSa %*% Sigma[a,b] Mu.j = Mu[b] + beta.j %*% (t(Y.full[i,a]) - Mu[a]) Y.full[i,b] = MASS::mvrnorm(1, Mu.j, Sigma.j) } if(s%% 10 == 1) cat(s, \u0026#39;\\t\u0026#39;) }   ## 1 11 21 31 41 51 61 71 81 91 1  colSums(is.na(Y))   ## glu bp skin bmi ## 15 23 25 22\r1  colSums(is.na(Y.full))   ## glu bp skin bmi ## 0 0 0 0\r","description":"다변량 정규분포","id":21,"section":"posts","tags":null,"title":"MVN","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/mvn/"},{"content":"Vector Space  덧셈에 닫혀있다. 스칼라배에 닫혀있다.  Subspace 벡터공간 $V_n$ 의 부분 집합 $W_n$이 벡터공간이면, $W_n$을 $V_n$의 부분공간이라고 한다. 즉, $W_n$이 부분공간이려면, 덧셈과 스칼라배에 닫혀있으면 된다.\nGauss-Jordan Elimination  확장행렬을 기약행사다리꼴(RREF, reduced row echelon form)로 바꾸는 알고리즘 가우스조던 소거법 = 가감법 + 대입법  선형사상의 특징  가산성 $f(x+y) = f(x) + f(y)$ 동차성 $f(ax) = af(x)$  LU Decomposition $E_{k}...E_{2}E_{1}A = U$\n=\u0026gt; $A = E_{1}^{-1}E_{2}^{-1}...E_{k}^{-1}U = LU$  여기서 $E_{k}...E_{2}E_{1}$는 하삼각행렬이며, $E_{1}^{-1}E_{2}^{-1}...E_{k}^{-1}$도 하삼각행렬이다.\n  근거(1) 가우스 소거법을 시행할 때 사용되는 모든 기본행렬은 항상 하삼각행렬이다. (단, 행교환 제외) 근거(2) 하삼각행렬인 기본행렬의 역행렬은 여전히 하삼각행렬이다. 근거(3) 하삼각행렬 $\\times$ 하삼각행렬 = 하삼각행렬 PLU Decomposition 행교환이 필요한 경우, 행교환을 미리 해주고 LU 분해하는 법 (P: permutation)\n용어 정리  consistent: 해가 적어도 한 개는 있는 경우 homogeneous: 동차 (ex. $A\\vec{x} = \\vec{0}$: 동차 연립선형방정식)  ","description":"Intro","id":22,"section":"posts","tags":null,"title":"Lecture 01","uri":"https://jiwooblog.netlify.app/posts/statistics/linearalgebra/l01/"},{"content":"Chapter 01. Introduction and Examples 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\n이번 장을 통해서는 Likelihood and Prior를 살펴보고 Full probability model의 의미를 보는 데에 주목해보쟈.\n베이지안 추론의 목적 우리는 데이터 획득을 통해, 모집단 특성에 대한 불확실성을 줄여나가고자 한다. 이때, 불확실성 정도의 변화 수준을 계량화하는 것이 베이지안 추론통계의 목적이라고 할 수 있다.\n핵심 개념  prior distribution $p(\\theta)$  사전확률 모수에 대해 기존에 갖고 있던 믿음의 정도   sampling model $p(y|\\theta)$  일종의 가능도 함수(likelihood) 사전확률이 참이라는 가정 하에, 특정 데이터가 관찰된 확률   posterior distribution $p(\\theta|y)$  데이터가 관찰되었을 때, 이를 바탕으로 수정된 모수에 대한 믿음의 정도    Bayes' Rule $$p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{\\int_{\\Theta}p(y|\\tilde{\\theta})p(\\tilde{\\theta})d\\tilde{\\theta}}$$\n이는 사후분포가 사전분포와 가능도 함수에 의해 어떻게 업데이트 되는지를 수식적으로 나타난 것이다.\n베이즈 통계의 전부라고 해도 무방하다.\n활용예시  희소사건 확률 추정(Estimation)  감염 확률(infectious probability) 확률론자(frequentist)는 sample이 적을 때 확률추정을 합리적으로 하는 데에 있어서 취약할 수 있다. 예를 들어, 20명만을 대상으로 감염 여부를 확인하고 감염 확률을 추론한다면, 감염확률을 0%라고 제안하는 것은 통계적으로는 그럴 듯하게 계산될 수 있다. 하지만 이는 현실과는 다소 거리가 있을 수 있다. 이에 반해, 베이지안은 감염 확률을 분포로서 제시할 뿐더러 기존의 믿음을 사전확률로서 제안하기 때문에 이러한 부분에 있어서 덜 취약할 수 있다.   예측 모델 구축(Prediction)  당뇨병(diabetes progression) 50% 확률로 변수의 coefficient가 0라고 사전확률을 제안한다면, 변수선택의 효과를 얻을 수 있다. 이와 관련된 자세한 내용은 FCB chapter 09서 Bayesian Linear Regression과 관련하여 설명될 예정이다.    ETC  \u0026lsquo;Adjusted\u0026rsquo; Wald interval\n흔히 알려진 신뢰구간을 베이지안적으로 바꾼 형태이다.  `\\hat{\\theta} \\pm 1.96\\sqrt{\\hat{\\theta}(1-\\hat{\\theta})//n}` , where `\\hat{\\theta} = \\frac{n}{n+4}\\bar{y} + \\frac{4}{n+4}\\frac{1}{2}`\n Lasso\n변수 선택의 한 방법이다. 아래 제시된 SSR를 최소화하는 것을 목표로 한다.\n베이지안의 맥락에서 처음 연구된 방법론은 아니지만, 특정 사전확률을 적용한다면 베이지안의 관점과 일치한다.\n여기서 말하는 그 특정 사전확률분포란, $\\beta_j$가 0에서 첨점을 갖는 라플라스 분포(또는 double-exponential distribution)를 따른다는 것을 의미한다.\n그리고 이때 lasso estimate은 $\\beta$의 사후 최빈값(posterior mode)과 같다.\n$$SSR(\\beta:\\lambda) = \\sum_{i=1}^{n}(y_i-\\boldsymbol{x_i}^T\\boldsymbol{\\beta})^2 + \\lambda\\sum_{j=1}^{n}|\\beta_j|$$  Conclusion \"All models are wrong, but some are useful\" - Box and Draper, 1987 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"Introduction and Examples","id":23,"section":"posts","tags":null,"title":"What is Bayesian","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb01/"},{"content":"심리학 수강 과목  심리학개론 충동과자기관리 심리학의 실험연구방법 발달심리학 행복의 과학 언어심리학 이론 및 실습 인지신경과학의 기초 심리통계 학습과 기억의 심리학 UT세미나(실패와 좌절의 심리학) 성격심리학 재능과 기술의 심리학 인지공학심리학 산업심리학  ","description":"","id":24,"section":"posts","tags":null,"title":"수강과목","uri":"https://jiwooblog.netlify.app/posts/yonsei/psychology/lecture/"},{"content":"통계학 수강 과목  통계학입문 미분적분학 통계방법론 선형대수 컴퓨터자료처리 심리통계 데이터사이언스를 위한 확률과정 회귀분석 수리통계학(1) 베이즈통계 수리통계학 (2) 시계열분석 금융리스크관리 실무와 통계학 UT세미나(생물통계학) 범주형자료분석 데이터사이언스입문 데이터마이닝 이론통계학(1) 데이터사이언스(2): 네트워크 자료분석  ","description":"","id":25,"section":"posts","tags":null,"title":"수강과목","uri":"https://jiwooblog.netlify.app/posts/yonsei/statistics/lecture/"},{"content":"NH투자증권 Y\u0026amp;Z세대 투자자 프로파일링 본 대회는 NH투자증권에서 주최한 대회로, 2020년 급격하게 늘어난 20~30대 투자자들에 대한 분석을 하고 이를 시각화하는 것이 주목적이었다.\n코드가 상당히 복잡한 관계로 Dacon에 코드공유한 링크와 이미지 일부를 올리는 것으로 포스팅을 대체하고자 한다.\nDacon 코드 공유 Factor Analysis Word Cloud_국내 Word Cloud_해외 Cluster Polygon Cluster Characteristics Idea Table Idea Sample Domain Knowledge 1. 해외주식 소수점 거래  현재 신한금융투자, 한국투자증권에서 실현 중 의결권, 배당권을 빼고 소수점 거래 가능! (ex. 한투 미니스톡)  2. 휴면 고객에 대한 이벤트도 주기적으로 한다. 3. ETP (= ETF + ETN) 금융투자협회: 한눈에 알아보는 레버리지 ETP Guide\n  괴리율\n 순자산가치(NAV) \u0026amp; 지표가치(IV)은 장중 실시간으로 산출한다.  전일 종가로 확정된 순자산 가치 + 당일의 가격 움직임   괴리율이 플러스(+): 추적대상 지수보다 고평가되어 있다(=비싸게 거래되고 있다). 괴리율이 마이너스(-): 추적대상 지수보다 저평가되어 있다(=싸게 거래되고 있다.) 괴리율이 너무 커지면, 한국거래소에서 단일가 매매 또는 매매거래정지를 시킬 수 있다. 유동성공급자(Liquidity Provider, LP)  괴리율이 과도하게 높을 경우: ETP를 매도하거나 대상자산을 매입해야겠다! 괴리율이 과도하게 낮을 경우: ETP를 매수하거나 대상자산을 매도해야겠다!      복리 효과\n 레버리지 ETP의 운용방식: \u0026lsquo;일별\u0026rsquo; 수익률의 ±2배 상승장일 경우, 2X의 상승률 \u0026gt; 인버스2X 하락률 하락장일 경우, 2X의 하락률 \u0026lt; 인버스2X 상승률 횡보장일 경우, 2X와 인버스2X 모두 손해볼 가능성이 높다. 즉, 장기투자에 적합하지 않다. 상식: 레버리지 상품을 만들기 위해서는 파생상품을 집어넣는다.    롤오버(roll-over)\n 파생상품의 거래월물을 교체하는 것(파생상품의 만기에 발생) 즉, 롤오버 과정에서 거래월물의 가겨차이에 따라 ETP의 자산가치 변동이 이루어질 수 있다. 선물 \u0026lsquo;매수\u0026rsquo;포지션을 보유하고 있는 레버리지(2X) ETP  거래월물의 가격차이가 (+)상태이면 이득, (-)상태이면 손해   선물 \u0026lsquo;매도\u0026rsquo;포지션을 보유하고 있는 레버리지(2X) ETP  거래월물의 가격차이가 (+)상태이면 손해, (-)상태이면 이득      지수의 방법론\n 괴리율, 복리효과, 롤오버만큼의 중요성은 아니지만, 간과해서는 안되는 POINT ex) 2020년 4월 유례 없는 마이너스 유가 폭락으로 인한 원유선물의 거래월물 편입대상과 롤오버 방식 변화    배운 점 1. 크롤링 능력  크롤링은 Python을 통해서 진행하였다. BeautifulSoup, selenium의 webdriver 등의 라이브러리를 활용하였다. (1) 해외사이트 크롤링의 어려움  느리고, 자잘한 오류가 생긴다. 예를 들어, 특정 단어를 검색을 해도 나오지 않는다.   (2) 크롤링 권한 접근  접근 권한의 제한으로 단순 크롤링을 할 수 없는 경우\nNetwork-XHR의 Request Headers 활용하기 에러 핸들링 관점에서 데이터 엔지니어링의 중요성 체감    2. 파생변수 제작의 어려움  run_away_cd(휴면 여부) 라는 변수를 새롭게 만들었다. 총 세가지 기준에 의해 각 고객의 휴면 여부를 판단하였는데, 그 기준은 다음과 같다.  (1) 거래주기에 비해 거래휴식기가 지나치게 긴 경우 (2) 예상 거래횟수에 비해 실제 거래횟수가 눈에 띄게 적은 경우 (3) 최근 2달 이내에 계좌개설한 사람들은 제외   해당 변수를 만들고 유의성을 검토하는 데에 적지 않은 시간을 투자하였던 경험👍 해당 변수를 만들기 위해, 중간과정에서 orr_prd, orr_cyl, orr_idx_1, orr_idx2를 만들었다.  3. 요인분석 Factor 조합 노하우  어려운 변수보다는 직관적으로 간단한 변수들의 조합으로 구성하는 것이 좋다.. 왜냐하면 요인분석 자체가 해석을 목적으로 하기 때문이다.  ","description":"","id":26,"section":"posts","tags":null,"title":"YZ 투자자 프로파일링","uri":"https://jiwooblog.netlify.app/posts/dacon/nh_yz/"},{"content":"Minimum Spanning Tree Need to connect all nodes to the tree in the smallest possible way\n1, 만드는 법 Kruskal\u0026rsquo;s algorithm에 따라서 만든다. (자세한 예시는 참고사이트[1] 참고하기)\n2. 규칙 및 특징 규칙1. Check each node is connected and no breaks in the tree.\n규칙2. Don\u0026rsquo;t add an edge if it doesn\u0026rsquo;t improve connectivity.\n특징1. There may be more than one MST.\n참고사이트 [1] Youtube\n","description":"Minimum Spanning Tree","id":27,"section":"posts","tags":null,"title":"MST","uri":"https://jiwooblog.netlify.app/posts/statistics/network/minumum_spanning_tree/"},{"content":"LUP \u0026amp; MSP completeness에 대해서 알아보자\n내용 구성  LUP (Least Upper Bound Property) MSP (Monotone Sequence Property)  ","description":"","id":28,"section":"posts","tags":null,"title":"LUP \u0026 MSP","uri":"https://jiwooblog.netlify.app/posts/mathmaticalanalysis/3_lup_msp/"},{"content":"Chapter 02. Fields and Vector Spaces 본 포스팅은 Finite Dimensional Linear Algebra (Gokenbach)를 참고하였다.\nConclusion 해석학을 배울 필요가 있을지도? 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":29,"section":"posts","tags":null,"title":"Chapter02","uri":"https://jiwooblog.netlify.app/posts/linearalgebra/chapter02/"},{"content":"\r\rVariable Transformation\r변수변환\r\r\r1. 적분\r변수변환은 적분할 때 유용하게 쓰인다. 예를 들어, 아래와 같은 경우를 생각해보자.\n\\[\\int_1^5 \\frac{2x}{x^2+1}dx\\]\r이때는 $x^2+1 = t$로 치환해서 적분해주면, $2xdx = dt$로 바뀌므로\n\\[\\int_2^{26}\\frac{1}{t}dt = \\Big[log\\ t\\Big]_{2}^{26} = log13\\]\n이렇게 쉽게 구할 수 있다.\n2. 확률변수\r하지만 꼭 적분할 때만 변수변환이 유용한 것은 아니다. 베이지안이 prior를 부여할 때 확률변수를 변수변환해가며 생각하는 경우가 있는데, 이에 대해서 아래에서 이야기해보도록 하겠다.\n2-1. 일변수\r우선은 일변수일 때 예시는 Jeffrey’s Prior를 이야기하는 포스팅에서 살펴보면 될 것이다.\n\r예를 들어, $p(\\theta) \\propto 1$라고 uninformative prior를 주자. 그리고 $ \\phi = exp(\\theta)$라고 가정해보자.\n\r\\[\\begin{align}\rp(\\phi) \u0026amp;= p(\\theta) \\bigg|\\frac{d\\theta}{d\\phi}\\bigg| \\\\\r\u0026amp;\\propto \\frac{1}{\\phi} \\neq 1\r\\end{align}\\]\n\r2-2. 다변수\r이를 다변수로 확장하여 생각해보자. 그러기 위해서는 일단 이변수로 생각해보는 걸 연습하자.\n\\[p(X, Y) = p(\\alpha, \\beta) \\times |J| \\text{ where } J = \\begin{vmatrix} \\frac{\\partial\\alpha}{\\partial X} \u0026amp; \\frac{\\partial\\alpha}{\\partial Y} \\\\\r\\frac{\\partial\\beta}{\\partial X} \u0026amp; \\frac{\\partial\\beta}{\\partial Y}\r\\end{vmatrix} \\]\rBayesian의 Hierarchical Model 중에서 Beta-Binomial Model의 $\\alpha, \\beta$에 대해 hyperprior를 줄 때, 어떤 모습의 확률을 부여해주어야할지 고민하다가 아래와 같은 hyperprior를 주기로 했다고 한다. 자세한 이야기는 관련 포스팅을 살펴보자. 여기서는 변수변환 그 자체에만 주목해보자.\n\\[p\\Big(log\\frac{\\alpha}{\\beta}, (\\alpha+\\beta)^{-\\frac{1}{2}}\\Big) \\propto 1\\]\r왜 이렇게 hyperprior를 주었는지에 대해서는 차치하고서라도, 일단 이것을 우리는 간단하게 해보자. 그러기 위해서는 이변수 차원에서의 변수변환이 필요하다. $log\\frac{\\alpha}{\\beta} = X, (\\alpha+\\beta)^{-\\frac{1}{2}} = Y$라고 생각하고 변수 변환을 해보자. STEP1. Jacobian(J) 구하기\n\\[\\begin{align}\rX \u0026amp;= log\\frac{\\alpha}{\\beta} \\\\\r\\rightarrow e^X \u0026amp; = \\frac{\\alpha}{\\beta} \\\\\r\\end{align} \\\\\\]\n\\[\\rightarrow \\begin{cases}\r\\alpha = \\beta\\cdot e^X \\\\\r\\beta = \\alpha\\cdot e^{-X}\r\\end{cases} \\\\\\]\n\\[\\therefore \\begin{cases}\r\\frac{\\partial\\alpha}{\\partial X} = \\beta\\cdot e^X = \\beta \\cdot \\frac{\\alpha}{\\beta} = \\alpha\\\\\r\\frac{\\partial\\beta}{\\partial X} = -\\alpha\\cdot e^{-X} = -\\alpha \\cdot \\frac{\\beta}{\\alpha} = -\\beta\r\\end{cases}\r\\]\r\\[\\begin{align}\rY \u0026amp;= (\\alpha + \\beta)^{-\\frac{1}{2}} \\\\\rY^{-2} \u0026amp;= \\alpha + \\beta \\\\\r\\therefore \\frac{\\partial\\alpha}{\\partial Y} \u0026amp;= \\frac{\\partial\\beta}{\\partial Y} = -2\\cdot Y^{-3} = -2(\\alpha+\\beta)^{\\frac{3}{2}}\r\\end{align}\\]\n\n\\[\\begin{align}\r\\therefore J \u0026amp;= \\begin{vmatrix} \\frac{\\partial\\alpha}{\\partial X} \u0026amp; \\frac{\\partial\\alpha}{\\partial Y} \\\\\r\\frac{\\partial\\beta}{\\partial X} \u0026amp; \\frac{\\partial\\beta}{\\partial Y}\r\\end{vmatrix} \\\\\r\u0026amp;= \\begin{vmatrix}\r\\alpha \u0026amp; -2(\\alpha+\\beta)^{\\frac{3}{2}} \\\\\r-\\beta \u0026amp; -2(\\alpha+\\beta)^{\\frac{3}{2}}\r\\end{vmatrix} \\\\\r\u0026amp;= -2\\alpha(\\alpha+\\beta)^{\\frac{3}{2}} -2\\beta(\\alpha+\\beta)^{\\frac{3}{2}} \\\\\r\u0026amp;= -2(\\alpha+\\beta)^{\\frac{5}{2}}\r\\end{align}\\]\nSTEP2. 대입하기\n\\[p\\Big(log\\frac{\\alpha}{\\beta}, (\\alpha+\\beta)^{-\\frac{1}{2}}\\Big) \\propto 1 \\\\\rp(\\alpha, \\beta) |J| \\propto 1 \\\\\rp(\\alpha, \\beta) \\cdot 2(\\alpha+\\beta)^{\\frac{5}{2}} \\propto 1 \\\\\rp(\\alpha, \\beta) \\propto (\\alpha+\\beta)^{-\\frac{5}{2}}\\]\n\r\r\r","description":"","id":30,"section":"posts","tags":null,"title":"변수 변환","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/variable_transformation/"},{"content":"lubridate 패키지 훑어보기 lubridate는 날짜 데이터를 처리하기 위한 패키지입니다.\n1  library(lubridate)    목차  parse datetimes  1. parse datetimes   다양한 형태 #1  1 2 3 4 5 6 7 8 9 10 11  # 다양한 형태가 있다. # ymd_hms(), ymd_hm(), ymd_h() # ydm_hms(), ydm_hm(), ydm_h() # mdy_hms(), mdy_hm(), mdy_h() # dmy_hms(), dmy_hm(), dmy_h() # ymd(), ydm() # mdy(), myd() # dmy(), dym() # yq() Q for quarter ymd_hms(\u0026#34;2017-11-28T14:02:00\u0026#34;)   ## [1] \u0026quot;2017-11-28 14:02:00 UTC\u0026quot;\r1  ydm_hms(\u0026#34;2017-22-12 10:00:00\u0026#34;)   ## [1] \u0026quot;2017-12-22 10:00:00 UTC\u0026quot;\r1  mdy_hms(\u0026#34;11/28/2017 1:02:03\u0026#34;)   ## [1] \u0026quot;2017-11-28 01:02:03 UTC\u0026quot;\r1  dmy_hms(\u0026#34;1 Jan 2017 23:59:59\u0026#34;)   ## [1] \u0026quot;2017-01-01 23:59:59 UTC\u0026quot;\r1  ymd(20170131)   ## [1] \u0026quot;2017-01-31\u0026quot;\r1  mdy(\u0026#34;July 4th, 2000\u0026#34;)   ## [1] \u0026quot;2000-07-04\u0026quot;\r1  dmy(\u0026#34;4th of July \u0026#39;99\u0026#34;)   ## [1] \u0026quot;1999-07-04\u0026quot;\r1  yq(\u0026#34;2001: Q3\u0026#34;)   ## [1] \u0026quot;2001-07-01\u0026quot;\r     다양한 형태 #2  1  today()   ## [1] \u0026quot;2021-03-23\u0026quot;\r1  now()   ## [1] \u0026quot;2021-03-23 11:24:45 KST\u0026quot;\r1  date_decimal(2021.5) #2021년 5월이 아니라, 2021년의 절반   ## [1] \u0026quot;2021-07-02 12:00:00 UTC\u0026quot;\r1  fast_strptime(\u0026#39;9/1/01\u0026#39;, \u0026#39;%y/%m/%d\u0026#39;)   ## [1] \u0026quot;2009-01-01 UTC\u0026quot;\r1  parse_date_time(\u0026#34;9/1/01\u0026#34;, \u0026#34;ymd\u0026#34;) # fast_strptime과 달리, 조금 더 디테일하게 써줘야한다.   ## Warning: All formats failed to parse. No formats found.\r## [1] NA\r1  parse_date_time(\u0026#34;2009/1/01\u0026#34;, \u0026#34;ymd\u0026#34;)   ## [1] \u0026quot;2009-01-01 UTC\u0026quot;\r   ","description":"","id":31,"section":"posts","tags":null,"title":"lubridate","uri":"https://jiwooblog.netlify.app/posts/r/lubridate/"},{"content":"\r\rEntropy\r정보량 = 불확실성\r\\[\\begin{align}\rH(p) \u0026amp;= \\sum_{i=1}p_i log\\frac{1}{p_i} \\\\\r\u0026amp;= -\\sum_{i=1}p_i log(p_i)\r\\end{align}\\]\n여기서 $\\frac{1}{p_i}$는 발생확률의 역수로, 다르게 보면 가능한 결과의 수라고 볼 수 있다.\n그렇기 때문에 $log\\frac{1}{p_i}$는 필요한 질문의 수라고 생각할 수 있다.\n합쳐서 생각해보면, 정보량이라고 하는 것은 필요한 질문의 수 x 확률의 총합이라고 생각할 수 있다.\n\rCross Entropy\rp에 대해, 전략 Q를 사용했을 때의 불확실성\r즉, 특정 전략을 쓸 때, 예상되는 질문개수에 대한 기댓값\n그냥 Entropy와의 차이점은 log 안의 $p_i$가 $q_i$로 바뀌었다는 것이다.\r이것의 의미를 잘 파악해야 한다.\n\\[\\begin{align}\rH(p,q) \u0026amp;= \\sum_{i=1}p_i log\\frac{1}{q_i} \\\\\r\u0026amp;= -\\sum_{i=1}p_i log(q_i)\r\\end{align}\\]\nCross Entropy는 Log Loss 또는 Negative Log Likelihood라고 불리기도 한다.\r즉, Cross Entropy를 최소화하는 것은 log likelihood를 최대화하는 것과 같다.\n\rKL-Divergence\r쿨백-라이블러 발산(Kullback-Leibler Divergence)는 두 확률분포의 차이에서 계산된 엔트로피 차이를 뜻한다.\r참고로, H(p)는 상수값이기 때문에 Cross Entropy를 최소화하는 것은 KLD를 최소하는 것과 같은 task이다.\r즉, KLD를 최소화하는 것은 log likelihood를 최대화하는 것과 같아진다.\n\\[\\begin{align}\rKL(p||q) \u0026amp;= H(p,q) - H(p) \\\\\r\u0026amp;= \\sum_{i=1}p_i log\\frac{p_i}{q_i} \\\\\r\u0026amp;= -\\sum_{i=1}p_i log\\frac{q_i}{p_i}\r\\end{align}\\]\nKL-Divergence는 항상 0 이상이다. 직관적으로는 $H(p,q)$의 lower bound가 $H(p)$(상수값)이기 때문이라고 생각할 수 있다. 그리고 이를 증명하고자 한다면, convex function인 -log를 f(x)로 생각하고 Jensen’s inequality로 증명할 수 있다.\n\rJensen-Shannon Divergence\rKL-Divergence는 대칭이 아니다. 즉, p와 q의 위치를 바꿔쓸 수 없다. 그렇기 때문에 거리 개념으로 혼동하면 안된다.\n직관적으로 이해할 때, KL-Divergence는 두 확률분포 간의 거리라고 설명하곤 하지만, 그것이 옳지 않다는 것이다.\n그래도 거리 개념으로 활용하고 싶다면, Jensen-Shannon Divergence를 활용하면 된다.\n\\[JSD(p||q) = \\frac{1}{2}KL(p||M) + \\frac{1}{2}KL(q||M) \\\\\r\\text{where } M = \\frac{1}{2}(p+q)\\]\nReference\r[1] https://hyunw.kim/blog/2017/10/14/Entropy.html\n\r\r","description":"Entropy, Cross-Entropy, KL-Divergence","id":32,"section":"posts","tags":null,"title":"Entropy, KL-Divergence","uri":"https://jiwooblog.netlify.app/posts/machinelearning/entorpy_kld/"},{"content":"tidyr 패키지 훑어보기 tidyr은 tidy data를 형성하기 위해 고안된 패키지입니다. tidy data에서 1) 열은 변수를 의미하고, 2) 행은 하나의 케이스를 의미하며, 3) 하나의 셀은 하나의 값을 의미합니다.\n1  library(tidyverse)    목차  nest  nest 예시를 통해, 단순히 group_by를 하는 것과 group_by 이후 nest를 한 후에 어떻게 데이터가 정리되는지 확인해보자.\n  nest 예시  1 2  iris %\u0026gt;% group_by(Species)   ## # A tibble: 150 x 5\r## # Groups: Species [3]\r## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows\r1 2 3  iris %\u0026gt;% group_by(Species) %\u0026gt;% nest()   ## # A tibble: 3 x 2\r## # Groups: Species [3]\r## Species data ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026gt; ## 1 setosa \u0026lt;tibble [50 x 4]\u0026gt;\r## 2 versicolor \u0026lt;tibble [50 x 4]\u0026gt;\r## 3 virginica \u0026lt;tibble [50 x 4]\u0026gt;\r  \n참고 [1] https://gomguard.tistory.com/229\n","description":"","id":33,"section":"posts","tags":null,"title":"tidyr","uri":"https://jiwooblog.netlify.app/posts/r/tidyr/"},{"content":"2021.02.26  hugo theme 중 zzo와 zdoc은 다른 것이다. zzo 참고사이트 zdoc 참고사이트  ","description":"","id":34,"section":"updates","tags":null,"title":"February 2021","uri":"https://jiwooblog.netlify.app/updates/diary/2021_02/"},{"content":"대회 리스트  빅콘테스트 데이콘(Dacon) 서울특별시 빅데이터 캠퍼스 공모전 한국정보화진흥원 데이터 크리에이터 캠프 상권분석 빅데이터 경진대회 KB 국민은행 Future Finance Ai Challenge 빅데이터 활용정책 아이디어 공모전 Big Data Competition 미래에셋대학생디지털 금융페스티벌  ","description":"","id":35,"section":"updates","tags":null,"title":"대회 리스트","uri":"https://jiwooblog.netlify.app/updates/contest_list/"},{"content":"dplyr 패키지 훑어보기 1 2  library(tidyverse) library(MASS)   목차  rowwise\n1-1. pmax slice relocate lag, lead between, near coalsece recode first, last, nth rownames_to_column, column_to_rownames bind_rows, bind_cols mutate_all, mutate_if inner_join, left_join, right_join, full_join semi_join, anti_join  데이터   data  1 2 3 4  tmp \u0026lt;- tibble(x=round(rnorm(n=5, mean=5, sd=1)), y=round(rnorm(n=5, mean=5, sd=3)), z=round(rnorm(n=5, mean=5, sd=5))) tmp   ## # A tibble: 5 x 3\r## x y z\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 7 -1\r## 2 5 4 -2\r## 3 5 2 6\r## 4 6 2 12\r## 5 5 4 2\r1 2  data(survey) glimpse(survey)   ## Rows: 237\r## Columns: 12\r## $ Sex \u0026lt;fct\u0026gt; Female, Male, Male, Male, Male, Female, Male, Female, Male, ...\r## $ Wr.Hnd \u0026lt;dbl\u0026gt; 18.5, 19.5, 18.0, 18.8, 20.0, 18.0, 17.7, 17.0, 20.0, 18.5, ...\r## $ NW.Hnd \u0026lt;dbl\u0026gt; 18.0, 20.5, 13.3, 18.9, 20.0, 17.7, 17.7, 17.3, 19.5, 18.5, ...\r## $ W.Hnd \u0026lt;fct\u0026gt; Right, Left, Right, Right, Right, Right, Right, Right, Right...\r## $ Fold \u0026lt;fct\u0026gt; R on L, R on L, L on R, R on L, Neither, L on R, L on R, R o...\r## $ Pulse \u0026lt;int\u0026gt; 92, 104, 87, NA, 35, 64, 83, 74, 72, 90, 80, 68, NA, 66, 60,...\r## $ Clap \u0026lt;fct\u0026gt; Left, Left, Neither, Neither, Right, Right, Right, Right, Ri...\r## $ Exer \u0026lt;fct\u0026gt; Some, None, None, None, Some, Some, Freq, Freq, Some, Some, ...\r## $ Smoke \u0026lt;fct\u0026gt; Never, Regul, Occas, Never, Never, Never, Never, Never, Neve...\r## $ Height \u0026lt;dbl\u0026gt; 173.00, 177.80, NA, 160.00, 165.00, 172.72, 182.88, 157.00, ...\r## $ M.I \u0026lt;fct\u0026gt; Metric, Imperial, NA, Metric, Metric, Imperial, Imperial, Me...\r## $ Age \u0026lt;dbl\u0026gt; 18.250, 17.583, 16.917, 20.333, 23.667, 21.000, 18.833, 35.8...\r   1. rowwise() 행별로 최대값 구하기\n  rowwise 예시  1 2 3 4  #올바른 버전 tmp %\u0026gt;% rowwise() %\u0026gt;% mutate(max = max(x,y,z))   ## # A tibble: 5 x 4\r## # Rowwise: ## x y z max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 7 -1 7\r## 2 5 4 -2 5\r## 3 5 2 6 6\r## 4 6 2 12 12\r## 5 5 4 2 5\r1 2 3  #잘못된 버전 tmp %\u0026gt;% mutate(max = max(x,y,z))   ## # A tibble: 5 x 4\r## x y z max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 7 -1 12\r## 2 5 4 -2 12\r## 3 5 2 6 12\r## 4 6 2 12 12\r## 5 5 4 2 12\r  \n1-1. pmax 그런데 사실은 여기서 rowwise를 사용하지 않고, pmax를 사용하면 보다 간단하게 구할 수 있기도 하다.\n  pmax 예시  1 2 3  #간단한 버전 tmp %\u0026gt;% mutate(max = pmax(x,y,z))   ## # A tibble: 5 x 4\r## x y z max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 7 -1 7\r## 2 5 4 -2 5\r## 3 5 2 6 6\r## 4 6 2 12 12\r## 5 5 4 2 5\r  \n2. slice() 행 선택\n  slice 예시  1 2 3  # choose 10 rows with smallest 10 pulse values. survey %\u0026gt;% slice_min(Pulse, n = 10)   ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I\r## 1 Male 20.0 20.0 Right Neither 35 Right Some Never 165.00 Metric\r## 2 Female 16.5 17.0 Right L on R 40 Left Freq Never 167.64 Imperial\r## 3 Female 18.0 17.5 Right R on L 48 Neither Freq Never 165.00 Metric\r## 4 Male 21.0 21.0 Right L on R 48 Neither Freq Never 174.00 Metric\r## 5 Female 18.0 17.9 Right R on L 50 Left None Never 165.00 Metric\r## 6 Female 15.5 15.5 Right Neither 50 Right Some Regul NA \u0026lt;NA\u0026gt;\r## 7 Male 18.0 19.0 Right L on R 54 Neither Some Regul NA \u0026lt;NA\u0026gt;\r## 8 Male 22.0 21.5 Left R on L 55 Left Freq Never 200.00 Metric\r## 9 Male 20.5 19.5 Right L on R 56 Right Freq Never 179.00 Metric\r## 10 Male 19.8 20.0 Left L on R 59 Right Freq Never 180.00 Metric\r## Age\r## 1 23.667\r## 2 17.417\r## 3 18.667\r## 4 21.333\r## 5 30.750\r## 6 18.500\r## 7 17.750\r## 8 18.500\r## 9 17.417\r## 10 17.417\r1 2 3  # choose 10 columns with greatest 10 pulses values. survey %\u0026gt;% slice_max(Pulse, n = 10)   ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I\r## 1 Male 19.5 20.5 Left R on L 104 Left None Regul 177.8 Imperial\r## 2 Female 19.0 18.5 Left L on R 104 Left Freq Never 170.0 Metric\r## 3 Female 18.5 18.0 Left L on R 100 Neither Some Never 171.0 Metric\r## 4 Male 21.0 20.4 Right L on R 100 Right Freq Heavy 184.0 Metric\r## 5 Female 17.5 17.5 Right R on L 98 Left Freq Never NA \u0026lt;NA\u0026gt;\r## 6 Male 17.5 17.0 Left L on R 97 Neither None Never 165.0 Metric\r## 7 Male 22.5 23.0 Right R on L 96 Right None Never 170.0 Metric\r## 8 Male 21.4 21.0 Right L on R 96 Neither Some Never 180.0 Metric\r## 9 Female 17.5 17.8 Right R on L 96 Right Some Never NA \u0026lt;NA\u0026gt;\r## 10 Female 18.5 18.0 Right R on L 92 Left Some Never 173.0 Metric\r## 11 Female 18.0 17.7 Left R on L 92 Left Some Never NA \u0026lt;NA\u0026gt;\r## 12 Female 18.5 18.0 Right R on L 92 Right Freq Never 172.0 Metric\r## 13 Female 18.0 18.0 Right L on R 92 Neither Freq Never 165.0 Metric\r## 14 Female 16.3 16.2 Right L on R 92 Right Some Regul 152.4 Imperial\r## 15 Male 20.0 19.5 Right R on L 92 Right Some Never 179.1 Imperial\r## Age\r## 1 17.583\r## 2 17.250\r## 3 18.917\r## 4 20.083\r## 5 17.667\r## 6 19.500\r## 7 19.417\r## 8 19.000\r## 9 18.667\r## 10 18.250\r## 11 17.583\r## 12 17.500\r## 13 20.000\r## 14 23.500\r## 15 18.917\r1 2 3  # randomly select 10 rows. survey %\u0026gt;% slice_sample(n = 10)   ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I\r## 1 Female 18.3 18.5 Right R on L 75 Left Freq Never 170.00 Metric\r## 2 Male 20.0 19.5 Right R on L 92 Right Some Never 179.10 Imperial\r## 3 Female 17.5 18.0 Right R on L 68 Neither Freq Never 157.48 Imperial\r## 4 Female 18.5 18.2 Right R on L 72 Neither Freq Never 167.64 Imperial\r## 5 Female 20.5 20.5 Right R on L NA Left Freq Regul NA \u0026lt;NA\u0026gt;\r## 6 Male 18.0 16.0 Right R on L NA Right Some Never 180.34 Imperial\r## 7 Female 17.7 17.0 Right R on L 76 Right Some Never 167.00 Metric\r## 8 Male 16.0 15.5 Right Neither 71 Right Freq Never 154.94 Imperial\r## 9 Male 18.5 18.5 Right L on R NA Neither Freq Never 171.00 Metric\r## 10 Male 20.5 19.5 Left L on R 80 Right Some Occas 182.88 Imperial\r## Age\r## 1 18.750\r## 2 18.917\r## 3 17.750\r## 4 17.333\r## 5 19.250\r## 6 20.750\r## 7 17.250\r## 8 17.167\r## 9 18.333\r## 10 18.667\r1 2 3  # randomly select 10% of the data observations. survey %\u0026gt;% slice_sample(prop = .05)   ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I\r## 1 Female 17.0 17.0 Right L on R 79 Right Some Never 163.00 Metric\r## 2 Female 17.7 17.0 Right R on L 76 Right Some Never 167.00 Metric\r## 3 Female 16.7 15.1 Right Neither NA Right None Never 157.48 Imperial\r## 4 Male 19.5 20.5 Left R on L 104 Left None Regul 177.80 Imperial\r## 5 Male 19.5 20.2 Right R on L 60 Neither Freq Never 185.42 Imperial\r## 6 Female 13.0 13.0 \u0026lt;NA\u0026gt; L on R 70 Left Freq Never 180.34 Imperial\r## 7 Female 18.0 17.8 Right L on R 68 Right Some Never 168.90 Imperial\r## 8 Male 20.5 20.0 Right L on R 68 Right Freq Never 190.00 Metric\r## 9 Male 20.5 21.0 Right R on L 60 Right Freq Never 185.00 Metric\r## 10 Female 19.5 19.2 Right R on L 70 Right Some Never 170.00 Metric\r## 11 Female 16.2 16.4 Right R on L NA Right Freq Occas 172.00 Metric\r## Age\r## 1 24.667\r## 2 17.250\r## 3 18.167\r## 4 17.583\r## 5 32.667\r## 6 17.417\r## 7 17.083\r## 8 17.500\r## 9 17.917\r## 10 18.167\r## 11 17.000\r  \n3. relocate() relocate: changes the order of the columns.\n  relocate 예시  1 2 3  # move columns with factor variables to the front survey %\u0026gt;% relocate(where(is.factor)) %\u0026gt;% colnames()   ## [1] \u0026quot;Sex\u0026quot; \u0026quot;W.Hnd\u0026quot; \u0026quot;Fold\u0026quot; \u0026quot;Clap\u0026quot; \u0026quot;Exer\u0026quot; \u0026quot;Smoke\u0026quot; \u0026quot;M.I\u0026quot; \u0026quot;Wr.Hnd\u0026quot;\r## [9] \u0026quot;NW.Hnd\u0026quot; \u0026quot;Pulse\u0026quot; \u0026quot;Height\u0026quot; \u0026quot;Age\u0026quot;\r1 2 3  # move Pulse before Height survey %\u0026gt;% relocate(Pulse, .before = Height) %\u0026gt;% colnames()   ## [1] \u0026quot;Sex\u0026quot; \u0026quot;Wr.Hnd\u0026quot; \u0026quot;NW.Hnd\u0026quot; \u0026quot;W.Hnd\u0026quot; \u0026quot;Fold\u0026quot; \u0026quot;Clap\u0026quot; \u0026quot;Exer\u0026quot; \u0026quot;Smoke\u0026quot; ## [9] \u0026quot;Pulse\u0026quot; \u0026quot;Height\u0026quot; \u0026quot;M.I\u0026quot; \u0026quot;Age\u0026quot;\r1 2 3  # move Pulse to the end survey %\u0026gt;% relocate(Pulse, .after = last_col()) %\u0026gt;% colnames()   ## [1] \u0026quot;Sex\u0026quot; \u0026quot;Wr.Hnd\u0026quot; \u0026quot;NW.Hnd\u0026quot; \u0026quot;W.Hnd\u0026quot; \u0026quot;Fold\u0026quot; \u0026quot;Clap\u0026quot; \u0026quot;Exer\u0026quot; \u0026quot;Smoke\u0026quot; ## [9] \u0026quot;Height\u0026quot; \u0026quot;M.I\u0026quot; \u0026quot;Age\u0026quot; \u0026quot;Pulse\u0026quot;\r  \n4. lag(), lead()   lag, lead 예시  1  lag(1:5)   ## [1] NA 1 2 3 4\r1  lag(1:5, n = 2)   ## [1] NA NA 1 2 3\r1  lag(1:5, default = 0)   ## [1] 0 1 2 3 4\r1  lead(1:5)   ## [1] 2 3 4 5 NA\r1  lead(1:5, default = 6)   ## [1] 2 3 4 5 6\r   5. between(), near()   between, near 예시  1 2  # between: \u0026gt;=, \u0026lt;= 조건을 한번에 사용하기 between(1:12, 7, 9)   ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE\r1 2  # near: ==의 안전한 버전(특히 소수점 계산시) sqrt(2) ^ 2 == 2   ## [1] FALSE\r1  near(sqrt(2) ^ 2, 2)   ## [1] TRUE\r   6. coalesce() 각 위치별로 NA가 아닌 값을 첫번째 값을 반환\n  coalesce 예시  1 2 3 4 5  library(tidyverse) a \u0026lt;- NA b \u0026lt;- 3 c \u0026lt;- 5 coalesce(a,b,c)   ## [1] 3\r1 2 3  # 응용: coalesce를 NA imputation으로 활용하기 x \u0026lt;- sample(c(1:5, NA, NA, NA)) coalesce(x, 0)   ## [1] 0 2 0 3 1 4 0 5\r1 2 3  y \u0026lt;- c(1, 2, NA, NA, 5) z \u0026lt;- c(NA, NA, 3, 4, 5) coalesce(y, z)   ## [1] 1 2 3 4 5\r1 2 3 4 5  vecs \u0026lt;- list( c(1, 2, NA, NA, 5), c(NA, NA, 3, 4, 5) ) coalesce(!!!vecs)   ## [1] 1 2 3 4 5\r  \n7. recode() case_when의 특수한 형태로서 데이터를 교체할 때 사용할 수 있을 것이다.\n  recode 예시  1 2  tmp_char \u0026lt;- sample(c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;), 10, replace = TRUE) recode(tmp_char, a = \u0026#34;Apple\u0026#34;)   ## [1] \u0026quot;b\u0026quot; \u0026quot;b\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;b\u0026quot; \u0026quot;b\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;b\u0026quot; \u0026quot;Apple\u0026quot;\r## [10] \u0026quot;c\u0026quot;\r1  recode(tmp_char, a = \u0026#34;Apple\u0026#34;, b = \u0026#34;Banana\u0026#34;)   ## [1] \u0026quot;Banana\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Banana\u0026quot;\r## [9] \u0026quot;Apple\u0026quot; \u0026quot;c\u0026quot;\r1  recode(tmp_char, a = \u0026#34;Apple\u0026#34;, b = \u0026#34;Banana\u0026#34;, .default = NA_character_)   ## [1] \u0026quot;Banana\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Banana\u0026quot;\r## [9] \u0026quot;Apple\u0026quot; NA\r1 2 3  # 숫자형은 아래와 같이 ``표시가 들어가야 한다. tmp_num \u0026lt;- sample(c(1,2,3), 10, replace=TRUE) recode(tmp_num, `1`=5)   ## [1] 3 5 3 5 2 3 3 2 5 3\r1 2 3  # !!!을 활용하면, python에서 dictionary 형태로 활용하는 것처럼 쓸 수 있다. level_key \u0026lt;- c(a = \u0026#34;apple\u0026#34;, b = \u0026#34;banana\u0026#34;, c = \u0026#34;carrot\u0026#34;) recode(tmp_char, !!!level_key)   ## [1] \u0026quot;banana\u0026quot; \u0026quot;banana\u0026quot; \u0026quot;apple\u0026quot; \u0026quot;apple\u0026quot; \u0026quot;banana\u0026quot; \u0026quot;banana\u0026quot; \u0026quot;apple\u0026quot; \u0026quot;banana\u0026quot;\r## [9] \u0026quot;apple\u0026quot; \u0026quot;carrot\u0026quot;\r  \n8. first(), last(), nth() 첫번째, 마지막 또는 특정 위치에 있는 요소를 반환하는 함수이다.\n  first, last, nth 예시  1 2 3 4  x \u0026lt;- 1:10 y \u0026lt;- 10:1 first(x)   ## [1] 1\r1  last(y)   ## [1] 1\r1  nth(x, 3)   ## [1] 3\r1  nth(y, 4)   ## [1] 7\r  \n9. rownames_to_column(), column_to_rownames()   rownames_to_column, column_to_rownames 예시  1 2  a \u0026lt;- rownames_to_column(iris, var = \u0026#34;C\u0026#34;) head(a)   ## C Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 1 5.1 3.5 1.4 0.2 setosa\r## 2 2 4.9 3.0 1.4 0.2 setosa\r## 3 3 4.7 3.2 1.3 0.2 setosa\r## 4 4 4.6 3.1 1.5 0.2 setosa\r## 5 5 5.0 3.6 1.4 0.2 setosa\r## 6 6 5.4 3.9 1.7 0.4 setosa\r1 2  b \u0026lt;- column_to_rownames(a, var = \u0026#34;C\u0026#34;) head(b)   ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 5.1 3.5 1.4 0.2 setosa\r## 2 4.9 3.0 1.4 0.2 setosa\r## 3 4.7 3.2 1.3 0.2 setosa\r## 4 4.6 3.1 1.5 0.2 setosa\r## 5 5.0 3.6 1.4 0.2 setosa\r## 6 5.4 3.9 1.7 0.4 setosa\r   10. bind_rows(), bind_cols() 기존의 rbind랑 cbind 대신에 활용하면 될 것 같다.\n  bind_rows, bind_cols 예시  1 2 3 4 5  # bind_rows one_r \u0026lt;- starwars[1:4, ] two_r \u0026lt;- starwars[9:12, ] three_r \u0026lt;- starwars[9:12, 3] bind_rows(one_r, two_r)   ## # A tibble: 8 x 14\r## name height mass hair_color skin_color eye_color birth_year sex gender\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke~ 172 77 blond fair blue 19 male mascu~\r## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold yellow 112 none mascu~\r## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, bl~ red 33 none mascu~\r## 4 Dart~ 202 136 none white yellow 41.9 male mascu~\r## 5 Bigg~ 183 84 black light brown 24 male mascu~\r## 6 Obi-~ 182 77 auburn, w~ fair blue-gray 57 male mascu~\r## 7 Anak~ 188 84 blond fair blue 41.9 male mascu~\r## 8 Wilh~ 180 NA auburn, g~ fair blue 64 male mascu~\r## # ... with 5 more variables: homeworld \u0026lt;chr\u0026gt;, species \u0026lt;chr\u0026gt;, films \u0026lt;list\u0026gt;,\r## # vehicles \u0026lt;list\u0026gt;, starships \u0026lt;list\u0026gt;\r1  bind_rows(one_r, three_r) # 에러 안 뜸. NA로 채움   ## # A tibble: 8 x 14\r## name height mass hair_color skin_color eye_color birth_year sex gender\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke~ 172 77 blond fair blue 19 male mascu~\r## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold yellow 112 none mascu~\r## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, bl~ red 33 none mascu~\r## 4 Dart~ 202 136 none white yellow 41.9 male mascu~\r## 5 \u0026lt;NA\u0026gt; NA 84 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 \u0026lt;NA\u0026gt; NA 77 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 \u0026lt;NA\u0026gt; NA 84 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 8 \u0026lt;NA\u0026gt; NA NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # ... with 5 more variables: homeworld \u0026lt;chr\u0026gt;, species \u0026lt;chr\u0026gt;, films \u0026lt;list\u0026gt;,\r## # vehicles \u0026lt;list\u0026gt;, starships \u0026lt;list\u0026gt;\r1 2 3 4 5  # bind_cols one_c \u0026lt;- starwars[,1:4] two_c \u0026lt;- starwars[,7:9] three_c \u0026lt;- starwars[10:50,7:9] bind_cols(one_c, two_c)   ## # A tibble: 87 x 7\r## name height mass hair_color birth_year sex gender ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 77 blond 19 male masculine\r## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; 112 none masculine\r## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; 33 none masculine\r## 4 Darth Vader 202 136 none 41.9 male masculine\r## 5 Leia Organa 150 49 brown 19 female feminine ## 6 Owen Lars 178 120 brown, grey 52 male masculine\r## 7 Beru Whitesun lars 165 75 brown 47 female feminine ## 8 R5-D4 97 32 \u0026lt;NA\u0026gt; NA none masculine\r## 9 Biggs Darklighter 183 84 black 24 male masculine\r## 10 Obi-Wan Kenobi 182 77 auburn, white 57 male masculine\r## # ... with 77 more rows\r1  # bind_cols(one_c, three_c) # 에러 뜸, bind_rows와 차이점     \n11. mutate_all, mutate_if 모든 변수를 다 특정 함수를 거친 형태로 바꾸거나, 조건을 주어서 특정 함수를 거친 형태로 바꾸는 함수이다.\n  mutate_all, mutate_if 예시  1  iris %\u0026gt;% mutate_all(as.integer) %\u0026gt;% head() # Species까지 integer로 바꿔버림   ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 5 3 1 0 1\r## 2 4 3 1 0 1\r## 3 4 3 1 0 1\r## 4 4 3 1 0 1\r## 5 5 3 1 0 1\r## 6 5 3 1 0 1\r1  iris %\u0026gt;% mutate_if(is.double, as.integer) %\u0026gt;% head()   ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 5 3 1 0 setosa\r## 2 4 3 1 0 setosa\r## 3 4 3 1 0 setosa\r## 4 4 3 1 0 setosa\r## 5 5 3 1 0 setosa\r## 6 5 3 1 0 setosa\r  \n12. inner_join, left_join, right_join, full_join SQL에서 봤던 join 함수가 역시나 그대로 있다. 그런데 친절하게 무엇을 기준으로 join 했는지도 위에 알려줘서 좋은 것 같다.\n  join 예시  1  band_members   ## # A tibble: 3 x 2\r## name band ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mick Stones ## 2 John Beatles\r## 3 Paul Beatles\r1  band_instruments   ## # A tibble: 3 x 2\r## name plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John guitar\r## 2 Paul bass ## 3 Keith guitar\r1  band_members %\u0026gt;% inner_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 2 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John Beatles guitar\r## 2 Paul Beatles bass\r1  band_members %\u0026gt;% left_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 3 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mick Stones \u0026lt;NA\u0026gt; ## 2 John Beatles guitar\r## 3 Paul Beatles bass\r1  band_members %\u0026gt;% right_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 3 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John Beatles guitar\r## 2 Paul Beatles bass ## 3 Keith \u0026lt;NA\u0026gt; guitar\r1  band_members %\u0026gt;% full_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 4 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mick Stones \u0026lt;NA\u0026gt; ## 2 John Beatles guitar\r## 3 Paul Beatles bass ## 4 Keith \u0026lt;NA\u0026gt; guitar\r  \n13. semi_join, anti_join   semi_join, anti_join 예시  1 2  #이미 조인된 결과값 band_members %\u0026gt;% inner_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 2 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John Beatles guitar\r## 2 Paul Beatles bass\r1 2  #band_members 중 join될 값 확인 band_members %\u0026gt;% semi_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 2 x 2\r## name band ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John Beatles\r## 2 Paul Beatles\r1 2  #band_members 중 join되지 않을 값 확인 band_members %\u0026gt;% anti_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 1 x 2\r## name band ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mick Stones\r   참고 [1] slice와 relocate 예시는 slack 슬기로운통계생활을 참고하였습니다.\n","description":"","id":36,"section":"posts","tags":null,"title":"dplyr","uri":"https://jiwooblog.netlify.app/posts/r/dplyr/"},{"content":"purrr 패키지 훑어보기 purrr는 R에서 깔끔하게 반복 작업 처리하는 패키지입니다. Purrr 을 이용하면 반복작업을 Apply family 에 비해 더욱 직관적이고 쉽게 할 수 있습니다. purrr는 고양이 울음소리와 R의 합성어로, 로고는 아래와 같습니다.\n1  library(tidyverse)   목차  map, map2 pmap, invoke_map rerun every, some, none reduce, accumulate  map, map2   map, map2 예시  1 2 3 4 5  num \u0026lt;- c(1,2,4,5,7) num2 \u0026lt;- c(3,5,6,8,9) #list map(num, function(x){x^2})   ## [[1]]\r## [1] 1\r## ## [[2]]\r## [1] 4\r## ## [[3]]\r## [1] 16\r## ## [[4]]\r## [1] 25\r## ## [[5]]\r## [1] 49\r1  map2(num, num2, sum)   ## [[1]]\r## [1] 4\r## ## [[2]]\r## [1] 7\r## ## [[3]]\r## [1] 10\r## ## [[4]]\r## [1] 13\r## ## [[5]]\r## [1] 16\r1 2  #numeric vector map_dbl(num, function(x){x^2})   ## [1] 1 4 16 25 49\r1  map2_dbl(num, num2, sum)   ## [1] 4 7 10 13 16\r     map 실전활용- iris data  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  n_iris \u0026lt;- iris %\u0026gt;% group_by(Species) %\u0026gt;% nest() mod_fun \u0026lt;- function(df){ lm(Sepal.Length ~ ., data = df) } m_iris \u0026lt;- n_iris %\u0026gt;% mutate(model = map(data, mod_fun)) b_fun \u0026lt;- function(mod){ coefficients(mod)[[1]] } m_iris %\u0026gt;% transmute(Species, beta = map_dbl(model, b_fun))   ## # A tibble: 3 x 2\r## # Groups: Species [3]\r## Species beta\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 setosa 2.35 ## 2 versicolor 1.90 ## 3 virginica 0.700\r  \npmap, invoke_map   pmap, invoke_map 예시  1 2 3 4 5 6  x \u0026lt;- list(3, 6, 9) y \u0026lt;- list(10, 21, 30) z \u0026lt;- list(100, 200, 300) # pmap은 3개 이상의 리스트일 때 사용한다. pmap(list(x, y, z), sum)   ## [[1]]\r## [1] 113\r## ## [[2]]\r## [1] 227\r## ## [[3]]\r## [1] 339\r1 2  # invoke_map은 각각의 리스트에 다른 함수를 적용시키고 싶을 때 활용한다. invoke_map(list(runif, rnorm), list(list(n = 10), list(n = 5)))   ## [[1]]\r## [1] 0.07250356 0.88643385 0.53457501 0.45493290 0.95970419 0.57638199\r## [7] 0.62800763 0.63266467 0.64451000 0.77471082\r## ## [[2]]\r## [1] 0.1348207 -0.5144428 -0.8763132 0.8955774 0.3386345\r   rerun rerun은 샘플 데이터를 리스트 형식으로 형성하는 데에 효율적인 방법이다.\n  rerun 예시  1 2 3 4 5  # 예시 set.seed(2021) a \u0026lt;- 10 %\u0026gt;% rerun(rnorm(5)) a   ## [[1]]\r## [1] -0.1224600 0.5524566 0.3486495 0.3596322 0.8980537\r## ## [[2]]\r## [1] -1.92256952 0.26174436 0.91556637 0.01377194 1.72996316\r## ## [[3]]\r## [1] -1.0822049 -0.2728252 0.1819954 1.5085418 1.6044701\r## ## [[4]]\r## [1] -1.841476 1.623310 0.131389 1.481122 1.513318\r## ## [[5]]\r## [1] -0.9424433 -0.1856850 -1.1011246 1.2081153 -1.6249385\r## ## [[6]]\r## [1] 0.10537833 -1.45544335 -0.35401614 -0.09370004 1.10066863\r## ## [[7]]\r## [1] -1.9638251 -1.4479444 1.0194434 -1.4214171 -0.6045321\r## ## [[8]]\r## [1] -1.58347390 -1.28593235 -1.45468488 -0.08707112 0.50473644\r## ## [[9]]\r## [1] 0.11638871 1.76021373 -0.34511646 2.12000016 -0.03437749\r## ## [[10]]\r## [1] -0.7921541 1.4755152 -0.7255572 0.3123790 0.6919641\r1 2 3 4 5 6 7  # 위 함수는 아래의 함수와 같은 결과를 산출함을 알 수 있다. set.seed(2021) b \u0026lt;- list() for(i in 1:10){ b[[i]] \u0026lt;- rnorm(5) print(b[i]) }   ## [[1]]\r## [1] -0.1224600 0.5524566 0.3486495 0.3596322 0.8980537\r## ## [[1]]\r## [1] -1.92256952 0.26174436 0.91556637 0.01377194 1.72996316\r## ## [[1]]\r## [1] -1.0822049 -0.2728252 0.1819954 1.5085418 1.6044701\r## ## [[1]]\r## [1] -1.841476 1.623310 0.131389 1.481122 1.513318\r## ## [[1]]\r## [1] -0.9424433 -0.1856850 -1.1011246 1.2081153 -1.6249385\r## ## [[1]]\r## [1] 0.10537833 -1.45544335 -0.35401614 -0.09370004 1.10066863\r## ## [[1]]\r## [1] -1.9638251 -1.4479444 1.0194434 -1.4214171 -0.6045321\r## ## [[1]]\r## [1] -1.58347390 -1.28593235 -1.45468488 -0.08707112 0.50473644\r## ## [[1]]\r## [1] 0.11638871 1.76021373 -0.34511646 2.12000016 -0.03437749\r## ## [[1]]\r## [1] -0.7921541 1.4755152 -0.7255572 0.3123790 0.6919641\r1 2 3  for(i in 1:10){ print(a[[i]] == b[[i]]) }   ## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r1 2  # 참고로, base에 있는 replicate와 어떻게 다른지 한번 살펴보자! replicate(10, rnorm(5))   ## [,1] [,2] [,3] [,4] [,5] [,6]\r## [1,] -0.50029080 0.1037663 0.01604353 -0.9836134 -0.34823176 -0.2369450\r## [2,] -2.25586935 0.4272891 -0.18536431 0.5650808 -0.04298997 -0.9991415\r## [3,] 0.04374133 -0.1704815 0.39193326 1.6167519 -1.39755396 -1.3925426\r## [4,] -0.36881809 -1.5491403 -0.75671092 -0.2519641 1.49021633 0.9820053\r## [5,] -0.96022240 -1.5055999 0.23141761 -1.0558786 -1.03938712 0.3609409\r## [,7] [,8] [,9] [,10]\r## [1,] -0.3375092 -1.2400271 0.81061837 -0.1220018\r## [2,] -0.6433876 0.5339593 -0.29366457 -0.6467737\r## [3,] -2.1668853 -1.5882648 -0.05345832 -0.8678583\r## [4,] 0.6332890 -0.9909645 0.73518450 -0.5087003\r## [5,] -0.1449141 0.4832608 0.01498499 -2.0775844\r1  typeof(a)   ## [1] \u0026quot;list\u0026quot;\r1  typeof(replicate(10, rnorm(5)))   ## [1] \u0026quot;double\u0026quot;\r  \nevery, some, none 리스트형식을 summarise하는 데에 효율적인 방법이다.\n  every, some, none 예시  1 2  y \u0026lt;- list(0:10, 5.5) y   ## [[1]]\r## [1] 0 1 2 3 4 5 6 7 8 9 10\r## ## [[2]]\r## [1] 5.5\r1  y %\u0026gt;% every(is.numeric)   ## [1] TRUE\r1  y %\u0026gt;% every(is.integer)   ## [1] FALSE\r1  y %\u0026gt;% some(is.integer)   ## [1] TRUE\r1  y %\u0026gt;% none(is.character)   ## [1] TRUE\r  \nreduce, accumulate 함수를 재귀적으로(recursively) 적용시키는 효율적인 방법이다.\n  reduce, accumulate 예시  1 2  #1. reduce: reduce(1:10, sum)   ## [1] 55\r1 2  #2. accumulate accumulate(1:10, sum)   ## [1] 1 3 6 10 15 21 28 36 45 55\r1 2 3  #reduce 응용 버전 paste2 \u0026lt;- function(x, y, sep = \u0026#34;.\u0026#34;) paste(x, y, sep = sep) letters[1:4] %\u0026gt;% reduce(paste2)   ## [1] \u0026quot;a.b.c.d\u0026quot;\r  \n","description":"","id":37,"section":"posts","tags":null,"title":"purrr","uri":"https://jiwooblog.netlify.app/posts/r/purrr/"},{"content":"1. 추천 사이트  R for data science  2. 검색 팁 구글링 시, 뒤에 \u0026lsquo;Rpubs\u0026rsquo; 붙이기\n3. blogdown Auto-Knit 끄기 Ctrl+S 단축키로 수시로 저장하는 습관 때문에, Rmd 파일을 작업할 때 knit가 수시로 되어 작업속도에 영향을 미친다.\n이럴 때는 .Rprofile이라는 이름의 파일을 찾아서\n blogdown.knit.on_save = TRUE\n 라는 코드에서 TRUE를 FALSE로 바꿔주어야 한다.\n4. 자동정렬 단축키 ctrl + I\n5. python의 dictionary처럼 사용하기 1 2 3 4 5  # recode: case_when의 특수한 형태로서 데이터를 교체할 때 사용할 수 있을 것이다. tmp_char \u0026lt;- sample(c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;), 10, replace = TRUE) # !!!을 활용하면, python에서 dictionary 형태로 활용하는 것처럼 쓸 수 있다. level_key \u0026lt;- c(a = \u0026#34;apple\u0026#34;, b = \u0026#34;banana\u0026#34;, c = \u0026#34;carrot\u0026#34;) recode(tmp_char, !!!level_key)   6. Rmarkdown ggplot 그래프에서 한글 출력하기 1 2 3 4 5 6 7  library(extrafont) library(showtext) fond_add(\u0026#39;nanum\u0026#39;, \u0026#39;NanumGothic.ttf\u0026#39;) showtext_auto() font_import() theme_set(theme_bw(base_family=\u0026#39;nanum\u0026#39;))    그런데 code chunk 안에 들어가는 한글에서는 pdf 만들 때 안 나오던데 이거는 어떻게 해야하는지 아직 모르겠다.  ","description":"","id":38,"section":"posts","tags":null,"title":"R 꿀팁","uri":"https://jiwooblog.netlify.app/posts/r/r_tip/"},{"content":"\r\r데이터 설명\rdataset containing demographic data and laboratory data of 857 patients with acute coronary syndrome(ACS).\n# 변수별 NA값 확인\rcolSums(is.na(acs))\r## age sex cardiogenicShock entry ## 0 0 0 0 ## Dx EF height weight ## 0 134 93 91 ## BMI obesity TC LDLC ## 93 0 23 24 ## HDLC TG DM HBP ## 23 15 0 0 ## smoking ## 0\rcolSums(is.na(acs))[colSums(is.na(acs))\u0026gt;0]\r## EF height weight BMI TC LDLC HDLC TG ## 134 93 91 93 23 24 23 15\rna.var \u0026lt;- names(colSums(is.na(acs))[colSums(is.na(acs))\u0026gt;0])\r# 그래프로 보기\raggr(acs, prop=FALSE) \r# 상관관계\racs.na \u0026lt;- is.na(acs[,na.var])\rround(cor(acs.na),2)\r## EF height weight BMI TC LDLC HDLC TG\r## EF 1.00 0.46 0.45 0.46 0.13 0.12 0.13 0.11\r## height 0.46 1.00 0.99 1.00 0.20 0.19 0.20 0.21\r## weight 0.45 0.99 1.00 0.99 0.20 0.19 0.20 0.21\r## BMI 0.46 1.00 0.99 1.00 0.20 0.19 0.20 0.21\r## TC 0.13 0.20 0.20 0.20 1.00 0.98 1.00 0.75\r## LDLC 0.12 0.19 0.19 0.19 0.98 1.00 0.98 0.73\r## HDLC 0.13 0.20 0.20 0.20 1.00 0.98 1.00 0.75\r## TG 0.11 0.21 0.21 0.21 0.75 0.73 0.75 1.00\r\rMissing Data 종류\rMCAR (missing completely at random): 변수의 종류와 값 모두와 무관한 경우\rMAR (missing at random): 누락이 변수와는 관련있지만 그 값과는 관계 없는 경우\rMNAR (missing at not random): 누락의 원인이 있는 경우\r\r# na.omit과 complete.cases는 같은 역할을 한다.\rnrow(na.omit(acs)) == nrow(acs[complete.cases(acs),])\r## [1] TRUE\r\r추가로 알아볼 만한 주제\rNA imputation with Gibbs Sampler\rNA imputation with GAN(Generative Adversarial Network)\r\r\n\r\n참고사이트: https://rstudio-pubs-static.s3.amazonaws.com/192402_012091b9adac42dbbd22c4d07cb00d36.html\n\r\r","description":"","id":39,"section":"posts","tags":null,"title":"NA Imputation","uri":"https://jiwooblog.netlify.app/posts/r/na_imputation/"},{"content":"Part 2-2. 데이터 엔지니어 기초 다지기 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n3. SQLite Studio SQLite Studio 다운로드\n데이터 다운로드  editor 여는 법: Tools \u0026gt; Open SQL Editor (or Alt + E) SQLite과 MySQL을 포함한 다른 프로그램들과 코드가 다른 것들이 사소하게 있을 수 있다.  SQL 기본 문법 (1) SELECT 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  SELECT * FROM Salaries LIMIT 10; SELECT * FROM Salaries ORDER BY salary DESC LIMIT 10; SELECT * FROM Salaries WHERE yearID = \u0026#39;2010\u0026#39; AND lgID = \u0026#39;AL\u0026#39; ORDER BY salary DESC LIMIT 20; --SUM, AVG SELECT SUM(salary) FROM Salaries WHERE playerID = \u0026#39;rodrial01\u0026#39;; --Concat, Count, Group By SELECT nameFirst || \u0026#39; \u0026#39; || nameLast AS name FROM People Limit 10; SELECT nameFirst || \u0026#39; \u0026#39; || nameLast AS name FROM People Where playerID = \u0026#39;rodrial01\u0026#39;; SELECT COUNT(DISTINCT(nameFirst || \u0026#39; \u0026#39; || nameLast)) FROM People; SELECT nameFirst || \u0026#39; \u0026#39; || nameLast AS name, COUNT(*) FROM People GROUP BY name HAVING COUNT(*) \u0026gt; 1; SELECT teamID, SUM(Salary) as total_salary FROM Salaries GROUP BY teamID ORDER BY total_salary DESC;   SQL 기본 문법 (2) JOIN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  --Join SELECT t2.nameFirst ||\u0026#39; \u0026#39;||t2.nameLast AS name, t1.salary FROM Salaries t1 JOIN People t2 ON t2.playerID = t1.playerID ORDER BY salary DESC LIMIT 20; --Quiz. Top paid player for each team in 2010 SELECT t1.teamID, t2.nameFirst||\u0026#39; \u0026#39;||t2.nameLast AS name, t1.salary --using MAX(salary) instead of ORDER BY would be more efficient FROM Salaries t1 JOIN People t2 ON t2.playerID = t1.playerID WHERE t1.yearID = \u0026#39;2010\u0026#39; GROUP BY teamID ORDER BY salary DESC; -- Left Join, Right Join SELECT t1.playerID, COUNT(*) FROM People t1 LEFT JOIN AllstarFull t2 ON t2.playerID = t1.playerID GROUP BY 1 ORDER BY COUNT(*) DESC LIMIT 20;   SQL 기본 (3) 데이터 타입들 및 키 값들, 테이블 생성 Primary Key: 빠른 처리, 중복 처리를 위해서 설정하기도 함!\nForeign Key: 다른 테이블에서 온 칼럼 처리\nUnique: 중복 처리 방지\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  -- 데이터베이스 테이블 생성 CREATE TABLE mytable (id INT, name VARCHAR(255), debut DATE); CREATE TABLE mytable2 (id INTEGER PRIMARY KEY AUTOINCREMENT, name VARCHAR(255), debut DATE); INSERT INTO mytable2 (name, debut) VALUES (\u0026#39;jiwoo\u0026#39;, \u0026#39; 2000-09-01\u0026#39;); SELECT * FROM mytable2; --INSERT INSERT INTO mytable2 (name, debut) VALUES (\u0026#39;jiwoo\u0026#39;, \u0026#39;2000-09-05\u0026#39;); SELECT * FROM mytable2; --Update UPDATE mytable2 SET debut = \u0026#39;2010-09-01\u0026#39; WHERE id = 1; --Replace REPLACE INTO mytable2 (id, name, debut) VALUES (5, \u0026#39;jiwoo2\u0026#39;, \u0026#39;2015-09-01\u0026#39;); -- Update는 기존의 값이 없다면 아무런 행동도 하지 않지만, Replace는 기존의 값이 없다면 새로 만들어버린다는 차이점이 있다.  --Insert Or Ignore INSERT OR IGNORE INTO mytable2 (id, name, debut) VALUES (1, \u0026#39;jiwoo3\u0026#39;, \u0026#39;2010-09-11\u0026#39;); --이미 id가 1인 행이 있을 경우, 그냥 insert into만 하면 \u0026#39;unique constraint failed\u0026#39;가 뜨지만 or ignore을 추가해주면 괜찮다.  -- Delete, ALter, Drop -- 아주아주 신중하게 써야하는 커맨드들이다! SELECT * FROM mytable2; Delete FROM mytable2 WHERE id=1; ALTER TABLE mytalbe2 RENAME TO players; ALTER TABLE players ADD COLUMN DOB date; SELECT * FROM players; DROP TABLE mytable;  \nSQL 기본 (4) Functions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  -- Functions(1) 기본처리 및 연산 SELECT * FROM players; SELECT SUBSTR(name, 1, 3) FROM players; SELECT UPPER(name) FROM players; SELECT AVG(LENGTH(name)) FROM players; --MAX, AVG, COUNT, SUM  -- Functions(2) 날짜데이터, Case When SELECT CURRENT_TIMESTAMP; --UTC기준 SELECT DATE(\u0026#39;NOW\u0026#39;); SELECT DATETIME(CURRENT_TIMESTAMP, \u0026#39;+1 DAY\u0026#39;); SELECT id, name, CASE WHEN name = \u0026#39;jiwoo\u0026#39; THEN \u0026#39;OK\u0026#39; WHEN name = \u0026#39;jiwoo2\u0026#39; THEN \u0026#39;OK2\u0026#39; ELSE \u0026#39;No OK\u0026#39; END AS ok_name, --CASE WHEN부터 여기까지가 variable 하나!  debut FROM players;   ","description":"데이터 엔지니어 기초 다지기","id":40,"section":"posts","tags":null,"title":"SQL 기초 (SQLite)","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part2_2/"},{"content":"\r\rChapter 02. Belief, Probability and Exchangeability\r본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\r이번 장의 목표는 independence와 exchangeability를 이해하는 것이다. 이를 바탕으로 de Finetti’s theorem이 Bayesian에 갖는 의의를 이해한다면, 베이즈 통계를 공부할 준비가 된 것이다.\nBelief functions and Probabilities\r$Be()$는 belief function이라고 하자. 예를 들어, $Be(F) \u0026gt; Be(G)$는 G보다 F를 더 믿는다고 해석하면 된다. F, G, H를 아래와 같은 각각의 상황이라고 가정해보자.\n\rF : 좌파 후보자를 투표하는 경우 G : 소득이 하위 10%에 속하는 경우 H : 대도시에 거주하는 경우\n\rAxioms of beliefs\r$Be($not $H|H) \\le Be(F|H) \\le Be(H|H)$\r$Be(F $ or $G|H) \\ge max(Be(F|H), Be(G|H))$\r$Be(F $ and $G|H)$ can be drvied from $Be(G|H)$ and $Be(F|G $ and $H)$\r\r\rAxioms of probability\r$0 = Pr($not $H|H) \\le Pr(F|H) \\le Pr(H|H) \\le = 1$\r$Pr(F \\cup G|H) = Pr(F|H) + Pr(G|H)$ if $F \\cap G = \\emptyset$\r$Pr(F \\cap G|H) = Pr(G|H)Pr(F|g \\cap H)$\r\rbelief와 probability에 대한 각각의 공리들이 매칭되므로, 우리는 믿음의 정도를 계산할 때 확률함수를 계산하는 것처럼 다뤄도 무방하다고 결론낼 수 있다.\n\r\rConditional Independence\r사건 F와 G는 아래와 같은 상황에서 조건부 독립(conditional independence)이라고 한다.\r\\[Pr(F \\cap G|H) = Pr(F|H)Pr(G|H)\\]\r이를 풀어서 해석해보자면, H를 알고 있는 상황에서, 추가적으로 G에 대해서 알게 되는 것은 F에 대한 믿음을 변화시키는 데에 영향이 없다는 것이다. 위를 통해서 아래를 알 수 있는 것이다.\r\\[Pr(F|H \\cap G) = Pr(F|H) \\]\n\rExchangeability\r$Y_1, ..., Y_n$이 있을 때, 이 순서를 어떻게 섞더라도 결합확률은 바꾸지 않을 때 exchangeable하다고 한다. 이는 직관적으로 풀어쓴 것이며, 다시 한 번 수학적 정의로 자세히 써보자면 아래와 같다.\rLet $p(y_1, ... y_n)$ be the joint density of $Y_1, ..., Y_n$. If $p(y_1, ..., y_n) = p(y_{\\pi_1}, ..., y_{\\pi_n})$ for all permutations $\\pi$ of {1, …, n}, then $Y_1, ..., Y_n$ are exchangeable.\n\\[\\begin{equation}\r\\left.\\begin{aligned}\rY_1, ..., Y_n|\\theta \\text{ i.i.d} \\\\ \\theta \\sim p(\\theta)\r\\end{aligned}\\right\\} \\Rightarrow Y_1, ... Y_n \\text{ are exchangeable}\r\\end{equation}\\]\n\rde Finetti’s Theorem\r만약 $Y_1, ..., Y_n$이 exchangeability를 만족한다면, 아래와 같이 말할 수 있다.\n\\[p(y_1, ..., y_n) = \\int{\\Bigg\\{\\prod_{1}^{n}p(y_i|\\theta)\\Bigg\\} \\:p(\\theta)d\\theta} \\\\\r\\text{for some parameter} \\: \\theta\\]\n이는 확률변수 $Y_1, ..., Y_n$에 대해서 exchangeability를 만족한다면, $p(y_1, ..., y_n)$에 대해 $\\theta$라는 parameter를 활용하여 위와 같은 수식으로 나타낼 수 있다는 것이다. 그렇다면 이 정리가 베이지안에게 어떤 의미를 갖는 것일까? 이는 사전확률분포(prior model)와 가능도함수(sampling model)가 belief model $p(y_1, ..., y_n)$에 의존함을 의미한다. 풀어서 이야기하자면, parameter $\\theta$가 확률론자가 주장하는 것처럼 미지의 고정된 값이 아니라, 어떤 분포를 갖는 확률변수로 볼 수 있다는 것이다. (그리고 그것을 우리는 사전확률분포 prior distribution이라고 부른다.)\n\r주의사항\rBayes’ rule은 데이터를 접한 이후, 우리의 믿음이 어떻게 업데이트되는지에 대한 수식이다.\r여기서 헷갈리면 안되는 것이 있다. Bayes’ rule은 우리의 믿음이 어때야 하는지(should be)에 대해서 이야기하고 있는 것이 아니라 어떻게 변해야 하는지(should change)에 대해서 이야기하는 것이다.\n\rConclusion\r믿음(Belief)도 확률(Probability)로써 이야기할 수 있다.\rparamter $\\theta$는 분포를 갖는 확률변수이다.\r\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다.\r\n\r\r","description":"Belief, Probability and Exchangeability","id":41,"section":"posts","tags":null,"title":"Exchangeability","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb02/"},{"content":"Clusetring 주어진 데이터의 특성을 고려해서 데이터 집단을 정의하고, 데이터 집단을 대표할 수 있는 대표점을 찾는 과정이다.\n1. K-Means Clustering Step 1. 클래스 개수 결정 \u0026amp; 중심점 무작위 선택\nStep 2. 가장 가까운 클래스에 데이터를 배정\nStep 3. 클래스 중심점 재계산\nStep 4. 수렴할 때까지 Step 2~3 반복\n  장단점  단점 1. 클래스 개수를 미리 결정하여야 한다.\n단점 2. 이상치들로 인해 평균값을 기준으로 하는 것이 옳지 않을 수 있다. 이때는 중앙값을 사용하는 K-Medians를 활용해볼 수 있다.   2. Mean-Shift Clustering 높은 밀도를 보이는 지역으로 sliding window를 옮겨가는 hill climbing 알고리즘이다.\n  장단점  장점 1. 클래스 개수를 미리 결정할 필요가 없다.\n단점 1. kernel = 반지름 r 사이즈를 선택해야 한다.   3. DBSCAN Density-Based Spatial Clustering of Applications with Noise\n  장단점  장점 1. 클래스 개수를 미리 결정할 필요가 없다.\n단점 1. 별로 잘 작동하지 않는다.   4. EM Clusteinrg with GMM Expectation-Maximization using Gausian Mixture Models\nStep 1. K-means와 같이 클러스터 개수를 정하고,각 클러스터의 가우시안 분포 모수를 정한다.\nStep 2. 각 데이터가 특정 클러스터에 속할 확률을 계산한다.\nStep 3. Step 2에서 계산한 확률(likelihood)에 근거하여 이를 최대화하는 가우시안 분포의 새로운 모수를 계산한다.\n  장단점  장점 1. K-Means보다 유연하다.\n장점 2. 각 데이터는 여러 클러스터를 가질 수 있지만, 확률을 계산하여 판단할 수 있다.   5. Agglomerative Hierarchical Clustering Step 1. 각 데이터를 각각의 클러스터로 본다. Step 2. 평균 간의 거리를 통해서 두 클러스터를 하나로 합친다. Step 3. 나무의 뿌리가 만들어질 때까지 Step2를 계속한다. 또는 원하는 개수의 클러스터 개수가 되면 멈춘다.\n  장단점  장점 1. 클래스 수를 미리 결정하지 않으며, 오히려 원하는 클래스 개수에 따라 정할 수 있다.\n장점 2. 데이터의 계층적 구조를 잘 반영한다.\n장점 3. 불균형데이터에 대해 좋다(?, [5])\n단점 1. K-Means나 GMM에 비해 계산량이 크다.   6. Deep Clustering for Unsupervised Learning of Visual Features unsupervised learning 모델(k-means)에서 나오는 pseudo label(cluster index)를 pre-training모델에 fine-tuning을 시킨다.\n6-1. Main  Conv Top layer를 이용해 클러스터링 알고리즘(K-Means)을 사용 Pseudo Label를 생성해 Fine-Tuning  6-2. Detail  Sobel filter를 통해 edge 검출 Feature map 차원 축소 (PCA) Pseudo Label를 생성할 때 균등 샘플링  참고자료 [1] https://zinniastop.blogspot.com/2019/10/5.html\n[2] https://astralworld58.tistory.com/58\n[3] https://www.youtube.com/watch?v=cCwzxVwfrgM\n[4] http://dsba.korea.ac.kr/seminar/?mod=document\u0026amp;uid=28\n[5] https://towardsdatascience.com/clustering-analyses-with-highly-imbalanced-datasets-27e486cd82a4\n","description":"Clustering","id":42,"section":"posts","tags":null,"title":"Clustering","uri":"https://jiwooblog.netlify.app/posts/machinelearning/clustering/"},{"content":"Exponential Family 한국어로는 지수족 또는 지수류라고도 하지만, 영어로 보는 편이 직관적으로 받아들이는 데에 편할 것이다.\n$f(x;\\theta) = \\begin{cases}\rexp\\big[p(\\theta)K(x) + s(x) + q(\\theta)\\big] \u0026amp; x \\in S \\\\\r0 \u0026amp; o.w\r\\end{cases} $\n S does not depend on $\\theta$ $p(\\theta)$ is a nontrivial continuous function of $\\theta \\in \\Omega$\n3-1. If X is continuous, $K'(x) \\neq 0$ and $s(x)$ is continuous function.\n3-2. If X is discrete, $K(x)$ is nontrivial function.  또는\n$ f(y|\\phi) = \\begin{cases}\rh(y)c(\\phi)exp\\big[\\phi K(y)\\big] \u0026amp; y \\in S \\\\\r0 \u0026amp; o.w\r\\end{cases} $\n S does not depend on $\\phi$ $\\phi$ is a nontrivial continuous function of $\\theta \\in \\Omega$\n3-1. If X is continuous, $K'(y) \\neq 0$ and $h(y)$ is continuous function.\n3-2. If X is discrete, $K(y)$ is nontrivial function.  첫번째는 Hogg 책을 기준으로 서술한 것이며, 두번째는 FCB 기준으로 서술한 것이다.\n즉 위는 Frequentist 입장, 아래는 Bayesian 입장이라고 보면 된다. 수식에 있어서 큰 차이는 없지만, parameter가 given인지 아닌지가 차이라고 볼 수 있다.\nSufficient Statistic 우선은 Hogg책 서술을 기준으로 이야기해보자.\n$X_1, ..., X_n \\text{ ~ iid } f(x;\\theta)$이고 $f(x;\\theta)$가 exponential family라고 한다면,\n$Y_1 = \\sum_{i=1}^{n}K(X_i)$는 $\\theta$에 대한 완전충분통계량(complete sufficient statistic)이다.\n그렇다면 Sufficient 하다는 것의 의미는 무엇일까? Defintion: $p(X_1, ..., X_n|Y_1=y_1)$가 $\\theta$에 의존하지 않는다.\n즉 $\\frac{p(X_1, ..., X_n, Y_1=y_1;\\theta)}{p(Y_1=y_1; \\theta)}$를 계산할 때, $\\theta$에 의해 값이 좌지우지 되지 않는다는 것이다.\n이를 풀어서 말하자면, 각각 $X_1,...,X_n$ 데이터를 직접 알지는 못하더라도 이들에 대한 정보가 $Y_1$에 들어가있다는 것이다. 그렇기 때문에 $Y_1$ 값을 알게 되면 $X_1,...,X_n$의 joint probability를 계산할 수 있다. 그래서 **충분(sufficient)**하다는 것이다.\n","description":"지수족","id":43,"section":"posts","tags":null,"title":"Exponential Family","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/exponential_family/"},{"content":"2021.03.06  문득 빈도론자와 베이지안의 논쟁이 쿤과 포퍼의 과학철학 논쟁과 비슷하다는 생각을 했다. 빈도론자는 가설검정이라는 것을 하며 귀무가설과 대립가설을 놓고 검정통계량을 통해 기각하거나 채택을 한다. 반면, 베이지안은 철저하게 베이즈 이론만을 기반으로 하여 분포를 제시하여 가능성을 제시한다. 포퍼가 반증가능성을 이야기했다면, 쿤은 이를 비판하며 패러다임의 전환에 대해 이야기했다.  2021.03.28  대학원 진학을 하기 위해 어느 정도 고민을 마쳤지만, 생각보다 마음이 덜 잡힌 것 같다. 열품타를 사용하고 있는데 생각보다 좋은 것 같다. 더 잘 활용해보도록 해야겠다.  ","description":"","id":44,"section":"updates","tags":null,"title":"March 2021","uri":"https://jiwooblog.netlify.app/updates/diary/2021_03/"},{"content":"비모수통계학 비모수통계학(nonparametric statistics)는 모수적 검정의 가정이 충족되지 못하거나, 데이터 형식이 순서형일 경우처럼 일반적이지 않은 경우에 사용하는 통계적 방법론에 대한 연구를 한다.\nt검정에서 독립표본 t-검정과 대응표본 t-검정이 있습니다. 이에 대응하여 비모수통계학에서는 Wilcoxon rank-sum test(Mann-Whitney U-test)와 Wilcoxon signed-rank test가 있습니다.\n1. Wilcoxon rank-sum test Mann-Whitney U-test라고도 한다.\n독립표본 t-검정의 비모수 버전이다.\n2. Wilcoxon signed-rank test 대응표본 t-검정의 비모수 버전이다.\n참고 [1] 연세대학교 심리통계 2019-1 수업자료 [2] https://blog.naver.com/istech7/50152096673\n[3] http://www.incodom.kr/R%ED%99%9C%EC%9A%A9/Wilcoxon_Signed-Rank_Test\n[4] https://dermabae.tistory.com/159\n","description":"","id":45,"section":"posts","tags":null,"title":"비모수통계학","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/nonparametric/"},{"content":"ANOVA ANOVA는 Analysis of Variance의 약자로, 한국어로는 분산 분석이라고 한다. 집단 간 분산과 집단 내 분산을 비교하여 처리효과가 있는지 살펴보는 통계방법이다.\n","description":"분산분석","id":46,"section":"posts","tags":null,"title":"ANOVA","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/anova/"},{"content":"t-검정 t-검정은 모집단의 분산이나 표준편차를 알지 못할 때, 모집단을 대표하는 표본으로부터 추정된 분산이나 표준편차를 가지고 검정하는 방법이다.\n1. 단일표본 t-검정 모집단의 평균이 특정검정값과 같은지 확인하는 통계빵법이다.\n2. 독립표본 t-검정 독립된 두 집단 간 비교하는 방법으로, 서로 다른 두 모집단으로부터 데이터가 추출되었을 때 시행한다.\n1 2 3 4 5 6 7 8  # independent t-test x1 = 3.6667 x2 = 6.0667 s1 = 2.60951 s2 = 2.37447 n1 = n2 = 15 sp = sqrt((s1^2*(n1-1) + s2^2*(n2-1)) / ((n1-1) + (n2-1))) t = (x1-x2) / (sp*sqrt(1/n1 + 1/n2))   3. 대응표본 t-검정 한 집단 내 비교하는 방법으로, 하나의 모집단으로부터 데이터를 반복 추출하였을 때 시행한다.\n1 2 3 4 5 6 7  # paired t-test xa \u0026lt;- c(1,10,2,6,5,2,3,4,2,1,2,2,3,4,8) xb \u0026lt;- c(2,10,5,7,6,5,6,9,6,7,8,5,4,2,9) n \u0026lt;- length(xa) d_bar = (sum(xa)-sum(xb))/n d_sd = sqrt(sum((xa-xb-d_bar)^2)/(n-1)) t = d_bar / (d_sd/sqrt(n))   참고 [1] 연세대학교 심리통계 2019-1 수업자료 [2] https://wikidocs.net/34009\n","description":"t-test","id":47,"section":"posts","tags":null,"title":"t 검정","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/t_test/"},{"content":"신뢰구간과 신용구간 간단하게 구분하자면, 신뢰구간은 빈도주의자가, 신용구간은 베이지안이 사용하는 것이다.\n일반적으로 신용구간을 신뢰구간으로 착각하는 경우가 많다.\n신뢰구간 (Confidence Interval) “If we repeat the experiment infinitely many times, 95% of the experiments will capture the population parameter in their confidence intervals.”\r 해석하자면, 무수히 많이 반복하여 데이터를 얻고 신뢰구간을 산출한다면, 그 수많은 신뢰구간 중 95%는 모수를 갖고 있을 것으로 신뢰한다는 의미이다. 그렇기 때문에 한번의 실험결과만으로 신뢰구간을 구하고 이를 활용하는 데에는 다소 무리가 있어보인다. 하지만 중심극한정리를 통해 정규성을 확보함으로써 어느 정도의 논리적 비약은 막는다고 빈도론자들은 생각한다.\n신용구간 (Credential Interval) “There is 95% probability/plausibility/likelihood that the population parameter lies in the interval.”\r 해당 구간에 모수가 있을 확률을 구한다. 95%가 되는 구간은 무수히 많이 잡을 수 있겠지만, 그중에서 HPD(Highest posterior Density) region을 구하여 활용한다. 이는 x축에 평행한 선을 위에서부터 내려오면서 적용하여 그 사이 영역의 넓이가 95%가 되는지 확인하는 방법으로 계산한다.\n신용구간을 사용함으로써 얻을 수 있는 장점은 크게 두 가지로 요약 된다.\n 사후분포가 정규분포가 아니더라도 확률계산을 할 수 있다. 이는 정의역에 대한 전제를 고려할 수 있다는 장점으로 이어진다. 사전확률을 고려함으로써 신뢰구간보다 빠르게 신용구간을 구할 수 있다.  참고사이트 [1] https://towardsdatascience.com/do-you-know-credible-interval-e5b833adf399\n[2] FCB figure 3.6\n","description":"","id":48,"section":"posts","tags":null,"title":"신뢰구간, 신용구간","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/interval/"},{"content":"정규성 검정 ","description":"","id":49,"section":"posts","tags":null,"title":"정규성 검정","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/normality/"},{"content":"삼각함수 공식(Trigonometric Functions) $\\sin^2{x} + \\cos^2{x} = 1 $\n$\\tan^2{x} + 1 = \\sec^2{x} $\n$\\cos{2x} = 1 - 2\\sin^2{x} = 2\\cos^2{x} -1 $\n$\\frac{d}{dx}\\tan{x} = \\sec^2{x} $\n$\\frac{d}{dx}\\sec{x} = \\sec{x}\\tan{x} $\n","description":"","id":50,"section":"posts","tags":null,"title":"삼각함수공식","uri":"https://jiwooblog.netlify.app/posts/statistics/calculus/3_trigonometry/"},{"content":"Pygame 기초 본 포스팅은 해당 사이트(https://kkamikoon.tistory.com/129)를 적극참고 하였습니다.\n코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  import pygame pygame.init() BLACK = (0,0,0) WHITE = (255,255,255) BLUE = (0,0,255) GREEN = (0,255,0) RED = (255,0,0) size = [400,300] screen = pygame.display.set_mode(size) pygame.display.set_caption(\u0026#39;Game Title\u0026#39;) done = False clock = pygame.time.Clock() while not done: clock.tick(10) for event in pygame.event.get(): if event.type == pygame.QUIT: done = True screen.fill(WHITE) pygame.draw.polygon(screen, GREEN, [[30,150], [125,100], [220,150]], 5) pygame.draw.polygon(screen, GREEN, [[30,150], [125,100], [220,150]],0) pygame.draw.lines(screen, RED,False, [[50,150], [50,250], [200,250], [200,150]],5) pygame.draw.rect(screen, BLACK, [75,175,75,50],5) pygame.draw.rect(screen, BLUE, [75,175,75,50],0) pygame.draw.line(screen, BLACK, [112,175], [112,225],5) pygame.draw.line(screen, BLACK, [75,200], [150,200],5) pygame.display.flip() #출처: https://kkamikoon.tistory.com/129   결과물 ","description":"pygame 기초","id":51,"section":"posts","tags":null,"title":"pygame[기초]","uri":"https://jiwooblog.netlify.app/posts/python/pygame_1/"},{"content":"오차 \u0026amp; 잔차  오차(error): 모집단 회귀식 예측값 - 실제 관측값 잔차(residual): 표본집단 회귀식 예측값 - 실제 관측값  ","description":"오차와 잔차의 차이","id":52,"section":"posts","tags":null,"title":"오차와 잔차","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/error_residual/"},{"content":"국내/해외 지수 및 환율 1월 넷째주    INDEX 1.18(월) 1.19(화)     코스피 3013.93 ▼71.97(2.33%) 3092.66 ▲78.73(2.61%)   코스닥 944.67 ▼19.77(2.05%) 957.75 ▲13.08(1.38%)   S\u0026amp;P500 3768.25 ▼27.29(0.72%) 3768.25 ▼27.27(0.72%)   나스닥 12998.50 ▼114.14(0.87%) 12998.50 ▼114.14(0.87%)   환율 1103.90 ▲4.50(0.41%) 1102.90 ▼1.00(0.09%)    1월 셋째주 1.15(금) 오후 10:36 기준    INDEX VALUE Change Rate     코스피 3085.90 ▼64.03(2.03%)   코스닥 964.44 ▼15..85(1.62%)   S\u0026amp;P500 3795.54 ▼14.30(0.38%)   나스닥 13112.65 ▼16.31(0.12%)   환율 1099.40 ▲1.40(0.13%)    ","description":"","id":53,"section":"updates","tags":null,"title":"주식","uri":"https://jiwooblog.netlify.app/updates/stock/"},{"content":"Part 3. API는 무엇인가 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. API 정의  Application Programming Interface 두 개의 시스템이 서로 상호작용하기 위한 인터페이스(데이터 주고 받기!) 일반적으로 API는 REST API를 지칭한다. ex) Web API: 웹을 통해 외부 서비스들로부터 정보를 불러오는 API  2. API 접근 권한  Authentication: Identity가 맞다는 증명 Authorization: API를 통한 어떠한 액션을 허용 둘은 다르다! Athentication을 하였다고 하더라도 Authorization을 허용하지 않을 수 있다! Security 이슈가 중요하다.  API Key?  보통 Request URL 혹은 Request Header에 포함되는 긴 스트링 ex) Google Maps Platform \u0026gt; Geocoding API  https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA\u0026amp;key=YOUR_API_KEY 여기서 YOUR_API-KEY 이부분을 채워주지 않으면, request denied가 뜨게 된다.    Baisc Auth  username:password와 같은 credential을 Base64로 인코딩한 값을 Request Header 안에 포함  OAuth 2.0  End User \u0026lt;=\u0026gt; My App \u0026lt;=\u0026gt; Server(ex. Spotify) (1) App에서 End User한테 생일, 전화번호, 플레이리스트와 같은 정보를 가져가는 것에 대해서 동의를 받는다. (2) 동의서를 받아왔으니 Server한테 API 요청을 하고 그에 맞는 데이터를 요구한다.  3. Spotify Web API  Spotify 직접 방문해보기  4. Endpoints \u0026amp; Methods  Resource: API를 통해 리턴된 정보 Endpoint: Resource 안에는 여러 개의 Endpoints가 존재 Method: 자원 접근에 허용된 행위(GET, POST, PUT, DELETE)     Method Action     GET 해당 리소스를 조회하고 정보를 가져온다.   HEAD 응답코드와 HEAD만 가져온다.   POST 요청된 리소스를 생성한다.   PUT 요청된 리소스를 업데이트 한다.   DELETE 요청된 리소스를 삭제한다.    5. Parameters  Parameters: Endpoint를 통해 Request할 때 같이 전달하는 옵션들     Type 내용     Header Request Header에 포함. 주로 Authorization에 관련   Path Query String(?) 이전에 Endpoint Path 안에 포함. ex) id   Query String Query STring(?) 이후에 포함. ex) ?utm_source=facebook\u0026amp;\u0026hellip;   Request Body Request Body 안에 포함. 주로 JSON 형태    \n","description":"API는 무엇인가","id":54,"section":"posts","tags":null,"title":"API","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part3_1/"},{"content":"Chapter 03. One-parameter Models 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\nBinomial Model Prior: $\\theta \\text{ ~ } Beta(a,b)$\nLikelihood: $Y|\\theta \\text{ ~ } Binomial(n, \\theta) $\nPosterior: $\\theta|y \\text{ ~ } Beta(a+y, b+n-y) $ a: prior 성공횟수, b: prior 실패횟수, $\\omega$=a+b: concentration $E[\\theta|y] = \\frac{a+y}{a+b+n} = \\frac{n}{a+b+n}\\times\\frac{y}{n} + \\frac{a+b}{a+b+n}\\times\\frac{a}{a+b}$ where $\\frac{y}{n}$ = sample mean, $\\frac{a}{a+b}$ = prior expectation Posterior Predictive\n$n^* = 1$일 때 : $\\tilde{Y}|y \\text{ ~ } Ber(\\frac{a+y}{a+b+n})$\n$n^* \\geq 2$일 때 : $p(\\tilde{Y}=y^*|y) = \\binom{n^*}{y^*}\\frac{B(a+y+y^*, b+n+n^*-y-y^*)}{B(a+y, b+n-y)}$ where $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} $\nPoisson Model Prior: $\\theta \\text{ ~ } Gamma(a,b) $\nLikelihood: $Y_1, ..., Y_n \\text{ ~ iid. } Poisson(\\theta)$\nPosterior: $\\theta|y_1, ..., y_n \\text{ ~ } Gamma(a+\\sum_{i=1}^{n}{y_i}, b+n) $ a: sum of counts from b prior observations, b: number of prior observations $E[\\theta|y_1, ..., y_n] = \\frac{a+\\sum y_i}{b+n} = \\frac{b}{b+n}\\frac{a}{b} + \\frac{n}{b+n}\\frac{\\sum y_i}{n}$ Posterior Predictive: $\\tilde{Y}=y^*|y_1, ..., y_n \\text{ ~ } NB(a+\\sum y_i+y^*, \\frac{b+n}{b+n+1}) $\n단, 여기서 $Negative Binomial$은 성공이 아닌 실패횟수를 세는 분포 형태이다. 자세한 내용은 확률분포 포스팅에서 확인하자.\nExponential Family exponential family(지수족)의 pdf 또는 pmf는 다음과 같은 형식으로 표현될 수 있어야 한다.\n$ p(y_i|\\phi) = h(y)c(\\phi)exp\\big[\\phi K(y)\\big]$\nexponential family 자체에 대해서 보다 자세한 것은 해당 포스팅을 참고하자.\nPrior\n$$\\begin{align}\rp(\\phi) \u0026amp;= k(n_0, t_0)c(\\phi)^n_0e^{n_0t_0\\phi} \\\\\r\u0026amp;\\propto c(\\phi)^n_0e^{n_0t_0\\phi}\r\\end{align}$$\nLikelihood\n$$L(\\phi|y_1,...,y_n) \\propto c(\\phi)^n exp(\\phi \\sum_{i=1}^{n}K(y_i))$$\nPosterior\n$$\\begin{align}\rp(\\phi|y) \u0026amp;\\propto p(\\phi)f(y|\\phi) \\\\\r\u0026amp;\\propto c(\\phi)^{n_0}e^{n_0t_0\\phi} \\cdot c(\\phi)^n exp(\\phi \\sum_{i=1}^{n}K(y_i)) \\\\\r\u0026amp;\\propto c(\\phi)^{n_0}exp\\big[n_0t_0\\phi + \\phi \\sum_{i=1}^{n}K(y_i) \\big] \\\\\r\u0026amp;\\propto c(\\phi)^{n_0}exp\\big[ \\phi \\big( n_0t_0 + n\\frac{\\sum_{i=1}^{n}K(y_i)}{n} \\big)\\big]\r\\end{align}$$\n여기서 $n_0$와 $t_0$은 각각 prior sample size와 prior guess of $K(Y)$를 뜻한다.\n  참고: FCB 책 표현  Prior: $p(\\theta) \\propto g(\\theta)^\\eta \\ exp(\\phi(\\theta)^T \\ \\nu)$\nLikelihood: $p(y|\\theta) = \\prod_{i=1}^{N} f(y_i) \\ g(\\theta)^N \\ exp(\\phi(\\theta)^T \\ \\sum_{i=1}^{N}s(y_i))$ where $\\sum_{i=1}^{N}s(y_i))$ is sufficient statistics $t(y)$\nPosterior: $p(\\theta|y) \\propto g(\\theta)^{\\eta+N} \\ exp(\\phi(\\theta)^T \\ (\\nu + t(y)) $   Conjugate Prior prior와 posterior의 확률분포형태가 같을 수 있도록 prior을 설정하면 이를 conjugate prior라고 한다.\n위의 예시 외에도 Normal model 등이 있는데, 이들에 대해서는 다음에 이어서 살펴보도록 하겠다.\n다양한 예시들은 위키백과에 자세히 나와있으니 궁금한 사람들은 추가적으로 살펴보아도 좋겠다.\n주의사항 사후확률분포가 차이가 많이 나는 것과 사후예측치가 차이가 많이 나는 것의 차이를 알아두어야 한다. 즉, {${\\theta_1 \u0026gt; \\theta_2}$}와 {$\\tilde{Y_1} \u0026gt; \\tilde{Y_2}$}는 다르다.\n Strong evidence of a difference between two populations does not mean that the difference itself is large.\n Conclusion Conjugacy를 잘 알아두자. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"One-parameter Models","id":55,"section":"posts","tags":null,"title":"Conjugacy","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb03/"},{"content":"2021.04.04  올해 4월 1일에 TexLive가 2021년 버전으로 업그레이드되면서 Rmarkdown으로 문서화 작업을 다시 구성하는데에 이틀 정도 애를 먹었다. 한번 까는 까는 데에만 한시간은 족히 걸렸는데, 정확히 무엇이 잘못된지를 몰라서 여러 번 반복했다. 결국 그거 자체는 설치를 잘 한 것이었다. 문제는 install.packages('tinytex'), library(tinytex)를 안 했다는 것이다\u0026hellip; 내년에 또 업데이트가 된다면 미리미리 알아두고 있어야겠다\u0026hellip;  2012.04.09  졸업요건 확인을 마쳤다. 오전까지 \u0026lsquo;제2외국어이수여부\u0026rsquo;가 빈칸이어서 전화를 했는데, 그사이에 P로 되어있었다.  ","description":"","id":56,"section":"updates","tags":null,"title":"April 2021","uri":"https://jiwooblog.netlify.app/updates/diary/2021_04/"},{"content":"base 함수 중 유용한 함수 훑어보기 1 2  library(tidyverse) library(palmerpenguins)   1. split   split 예시  1 2 3 4 5 6 7 8 9  #1col기준으로 분리 penguins %\u0026gt;% split(.$species) -\u0026gt; split1 #2col기준으로 분리 penguins %\u0026gt;% split(list(.$species, .$island)) -\u0026gt; split2 #10row기준으로 분리(row개수 안맞으면 error) splitrow \u0026lt;- rep(1:35, c(rep(10, 34), 4)) splitrow   ## [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3\r## [26] 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5\r## [51] 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8\r## [76] 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 10 10 10 10 10 10 10 10 10 10\r## [101] 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13\r## [126] 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15\r## [151] 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17 17 17 17 18 18 18 18 18\r## [176] 18 18 18 18 18 19 19 19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20\r## [201] 21 21 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 23 23 23 23 23\r## [226] 23 23 23 23 23 24 24 24 24 24 24 24 24 24 24 25 25 25 25 25 25 25 25 25 25\r## [251] 26 26 26 26 26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 28 28 28 28 28\r## [276] 28 28 28 28 28 29 29 29 29 29 29 29 29 29 29 30 30 30 30 30 30 30 30 30 30\r## [301] 31 31 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33\r## [326] 33 33 33 33 33 34 34 34 34 34 34 34 34 34 34 35 35 35 35\r1  nrow(penguins)   ## [1] 344\r1  penguins %\u0026gt;% split(splitrow) -\u0026gt; split3      ","description":"","id":57,"section":"posts","tags":null,"title":"base","uri":"https://jiwooblog.netlify.app/posts/r/base/"},{"content":"Pygame 응용 본 포스팅은 해당 영상을 적극참고 하였습니다.\n코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282  import os import pygame ############################################################################## # 기본 초기화 (반드시 해야 하는 것들) pygame.init() # 초기화 # 화면 크기 설정 screen_width = 640 # 가로 크기 screen_height = 480 # 세로 크기 screen = pygame.display.set_mode((screen_width, screen_height)) # 화면 타이틀 설정 pygame.display.set_caption(\u0026#39;Jiwoo Pang\u0026#39;) # FPS clock = pygame.time.Clock() ############################################################################## # 1. 사용자 게임 초기화 (배경화면, 게임 이미지, 좌표, 속도, 폰트 등) current_path = os.path.dirname(__file__) # 현재 파일의 위치 반환 image_path = os.path.join(current_path, \u0026#39;images\u0026#39;) # images 폴더 위치 반환 # 배경 만들기 background = pygame.image.load(os.path.join(image_path, \u0026#39;game_background.jpg\u0026#39;)) # 스테이지 만들기 stage = pygame.image.load(os.path.join(image_path, \u0026#39;stage.png\u0026#39;)) stage_size = stage.get_rect().size stage_height = stage_size[1] # 스테이지 높이 위에 캐릭터를 두기 위해 사용 # 캐릭터 만들기 character = pygame.image.load(os.path.join(image_path, \u0026#39;cat.png\u0026#39;)) character_size = character.get_rect().size character_width = character_size[0] character_height = character_size[1] character_x_pos = (screen_width/2) - (character_width/2) character_y_pos = screen_height - character_height - stage_height # 캐릭터 이동 방향 character_to_x = 0 # 캐릭터이동 속도 character_speed = 5 # 무기 만들기  weapon = pygame.image.load(os.path.join(image_path, \u0026#39;weapon.png\u0026#39;)) weapon_size = weapon.get_rect().size weapon_width = weapon_size[0] # 무기는 한 번에 여러발 발 사 가능 weapons = [] # 무기 이동 속도 weapon_speed = 10 # 공 만들기 (4개 크기에 대해 따로 처리) ball_images = [ pygame.image.load(os.path.join(image_path, \u0026#39;balloon1.png\u0026#39;)), pygame.image.load(os.path.join(image_path, \u0026#39;balloon2.png\u0026#39;)), pygame.image.load(os.path.join(image_path, \u0026#39;balloon3.png\u0026#39;)), pygame.image.load(os.path.join(image_path, \u0026#39;balloon4.png\u0026#39;))] # 공 크기에 따른 최초 스피드 ball_speed_y = [-18, -15, -12, -9] # 공들  balls = [] # 최초 발생하는 큰 공 추가 balls.append({ \u0026#39;pos_x\u0026#39; : 50, # 공의 x좌표 \u0026#39;pos_y\u0026#39; : 50, # 공의 y좌표 \u0026#39;img_idx\u0026#39; : 0, # 공의 이미지 인덱스 \u0026#39;to_x\u0026#39; : 3, # x축 이동방향  \u0026#39;to_y\u0026#39; : -6, # y축 이동방향 \u0026#39;init_spd_y\u0026#39; : ball_speed_y[0]}) # y 최초 속도 # 사라질 무기, 공 정보 저장 변수 weapon_to_remove = -1 ball_to_remove = -1 # 폰트 정의 game_font = pygame.font.Font(None, 40) total_time = 100 start_ticks = pygame.time.get_ticks() # 시작 시간 정의 # 게임 종료 메세지 game_result = \u0026#39;Game Over\u0026#39; running = True while running: dt = clock.tick(30) # 2. 이벤트 처리 (키보드. 마우스 등) for event in pygame.event.get(): if event.type == pygame.QUIT: running = False if event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: character_to_x -= character_speed elif event.key == pygame.K_RIGHT: character_to_x += character_speed elif event.key == pygame.K_SPACE: # 무기 발사 weapon_x_pos = character_x_pos + (character_width / 2) - (weapon_width / 2) weapon_y_pos = character_y_pos weapons.append([weapon_x_pos, weapon_y_pos]) if event.type == pygame.KEYUP: if event.key == pygame.K_LEFT or event.key == pygame.K_RIGHT: character_to_x = 0 # 3. 게임 캐릭터 위치 정의 character_x_pos += character_to_x if character_x_pos \u0026lt; 0: character_x_pos = 0 elif character_x_pos \u0026gt; screen_width - character_width: character_x_pos = screen_width - character_width # 무기 위치 조정 weapons = [ [w[0], w[1] - weapon_speed] for w in weapons] # 무기 위치를 위로 # 천장에 닿은 무기 없애기 weapons = [ [w[0], w[1]] for w in weapons if w[1] \u0026gt; 0] # 공 위치 정의 for ball_idx, ball_val in enumerate(balls): ball_pos_x = ball_val[\u0026#39;pos_x\u0026#39;] ball_pos_y = ball_val[\u0026#39;pos_y\u0026#39;] ball_img_idx = ball_val[\u0026#39;img_idx\u0026#39;] ball_size = ball_images[ball_img_idx].get_rect().size ball_width = ball_size[0] ball_height = ball_size[1] # 가로벽에 닿았을 때 공 이동 위치 변경 (튕겨나오는 효과) if ball_pos_x \u0026lt; 0 or ball_pos_x \u0026gt; screen_width - ball_width: ball_val[\u0026#39;to_x\u0026#39;] = ball_val[\u0026#39;to_x\u0026#39;] * (-1) # 세로 위치 # 스테이지에 튕겨서 올라가는 처리 if ball_pos_y \u0026gt;= screen_height - stage_height - ball_height: ball_val[\u0026#39;to_y\u0026#39;] = ball_val[\u0026#39;init_spd_y\u0026#39;] else: # 그외의 모든 경우에는 속도를 증가 ball_val[\u0026#39;to_y\u0026#39;] += 0.5 ball_val[\u0026#39;pos_x\u0026#39;] += ball_val[\u0026#39;to_x\u0026#39;] ball_val[\u0026#39;pos_y\u0026#39;] += ball_val[\u0026#39;to_y\u0026#39;] # 4. 충돌 처리 # 캐릭터 rect 정보 업데이트 character_rect = character.get_rect() character_rect.left = character_x_pos character_rect.top = character_y_pos for ball_idx, ball_val in enumerate(balls): ball_pos_x = ball_val[\u0026#39;pos_x\u0026#39;] ball_pos_y = ball_val[\u0026#39;pos_y\u0026#39;] ball_img_idx = ball_val[\u0026#39;img_idx\u0026#39;] # 공 rect 정보 업데이트 ball_rect = ball_images[ball_img_idx].get_rect() ball_rect.left = ball_pos_x ball_rect.top = ball_pos_y # 공과 캐릭터 충돌 처리 if character_rect.colliderect(ball_rect): running = False break # 공과 무기들 충돌 처리 for weapon_idx, weapon_val in enumerate(weapons): weapon_pos_x = weapon_val[0] weapon_pos_y = weapon_val[1] # 무기 rect 정보 업데이트 weapon_rect = weapon.get_rect() weapon_rect.left = weapon_pos_x weapon_rect.top = weapon_pos_y # 충돌 체크 if weapon_rect.colliderect(ball_rect): weapons_to_remove = weapon_idx # 해당 무기 없애기 위한 값 설정 ball_to_remove = ball_idx if ball_img_idx \u0026lt; 3: # 현재 공 크기 정보를 가지고 옴 ball_width = ball_rect.size[0] ball_height = ball_rect.size[1] # 나눠진 공 정보 small_ball_rect = ball_images[ball_img_idx + 1].get_rect() small_ball_width = small_ball_rect.size[0] small_ball_height = small_ball_rect.size[1] # 왼쪽으로 튕겨나가는 작은 공 balls.append({ \u0026#39;pos_x\u0026#39; : ball_pos_x + (ball_width / 2) - (small_ball_width / 2), # 공의 x좌표 \u0026#39;pos_y\u0026#39; : ball_pos_y + (ball_height / 2) - (small_ball_height / 2), # 공의 y좌표 \u0026#39;img_idx\u0026#39; : ball_img_idx + 1, # 공의 이미지 인덱스 \u0026#39;to_x\u0026#39; : -3, # x축 이동방향  \u0026#39;to_y\u0026#39; : -6, # y축 이동방향 \u0026#39;init_spd_y\u0026#39; : ball_speed_y[ball_img_idx + 1]}) # y 최초 속도 # 오른쪽으로 튕겨나가는 작은 공 balls.append({ \u0026#39;pos_x\u0026#39; : ball_pos_x + (ball_width / 2) - (small_ball_width / 2), # 공의 x좌표 \u0026#39;pos_y\u0026#39; : ball_pos_y + (ball_height / 2) - (small_ball_height / 2), # 공의 y좌표 \u0026#39;img_idx\u0026#39; : ball_img_idx + 1, # 공의 이미지 인덱스 \u0026#39;to_x\u0026#39; : 3, # x축 이동방향  \u0026#39;to_y\u0026#39; : -6, # y축 이동방향 \u0026#39;init_spd_y\u0026#39; : ball_speed_y[ball_img_idx + 1]}) # y 최초 속도  break else: # 계속 게임을 진행 continue # 안쪽 for문 조건이 맞지 않으면 continue. 바깥 for문 계속 수행 break # 안쪽 for문에서 break를 만나면 여기로 진입 가능. 2중 for문을 한번에 통과하는 트릭! # for 바깥 조건: # 바깥 동작 # for 안쪽 조건: # 안쪽 동작 # if 충돌하면: # break # else: # continue # break # 충돌된 공 or 무기 없애기 if ball_to_remove \u0026gt; -1: del balls[ball_to_remove] ball_to_remove = -1 if weapon_to_remove \u0026gt; -1: del weapons[weapon_to_remove] weapon_to_remove = -1 # 모든 공을 없앤 경우 게임 종료(성공) if len(balls) == 0: game_result = \u0026#39;Mission Complete\u0026#39; running = False # 5. 화면에 그리기 screen.blit(background, (0, 0)) for weapon_x_pos, weapon_y_pos in weapons: screen.blit(weapon, (weapon_x_pos, weapon_y_pos)) for idx, val in enumerate(balls): ball_pos_x = val[\u0026#39;pos_x\u0026#39;] ball_pos_y = val[\u0026#39;pos_y\u0026#39;] ball_img_dx = val[\u0026#39;img_idx\u0026#39;] screen.blit(ball_images[ball_img_idx], (ball_pos_x, ball_pos_y)) screen.blit(stage, (0, screen_height - stage_height)) screen.blit(character, (character_x_pos, character_y_pos)) # 경과 시간 계산 elapsed_time = (pygame.time.get_ticks() - start_ticks)/ 1000 timer = game_font.render(\u0026#39;Time: {}\u0026#39;.format(int(total_time - elapsed_time)), True, (255, 255, 255)) screen.blit(timer, (10, 10)) # 시간 초과했다면 if total_time - elapsed_time \u0026lt;= 0: game_result = \u0026#39;Time Over\u0026#39; running = False pygame.display.update() # 게임화면을 다시 그리기 # 게임 오버 메세지 msg = game_font.render(game_result, True, (255, 0, 0)) msg_rect = msg.get_rect(center=(int(screen_width/2), int(screen_height/2))) screen.blit(msg, msg_rect) pygame.display.update() # 잠시 대기 pygame.time.delay(2000) # 2초 정도 대기(ms 단위) # pygame 종료 pygame.quit()   결과물 ","description":"pygame 응용","id":58,"section":"posts","tags":null,"title":"pygame[응용]","uri":"https://jiwooblog.netlify.app/posts/python/pygame_2/"},{"content":"Chapter 04. Monte Carlo Approximation 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\nMonte Carlo Method Monte Carlo Method는 이름은 거창해보이지만 사실 그 방법은 매우 간단하다.\n우선, 사후분포($p(\\theta|y_1,...,y_n)$)로부터 S개의 random sample을 뽑는다.\n$$\\theta^{1}, \u0026hellip;, \\theta^{S} \\ \\stackrel{iid}{\\sim} \\ p(\\theta|y_1, \u0026hellip;, y_n) $$\n그러면 S가 커질수록 {$\\theta^{1}, ..., \\theta^{S}$}는 근사적으로 사후분포($p(\\theta|y_1,...,y_n)$)를 따른다.\n이를 통해 $E[\\theta|y_1, ..., y_n]$, $Var[\\theta|y_1, ..., y_n]$부터 중앙값, $\\alpha$ percentile 등의 통계량값들을 근사적으로 계산할 수 있다.\n이때 approximate Monte Carlo Standard error은 $\\sqrt{\\hat{\\sigma}^2/S}$이다. 그렇기 때문에, 가령 $E[\\theta|y_1, ..., y_n]$와 Monte Carlo 추정치의 차이가 0.01이하로 하고 싶다고 한다면, Monte Carlo Sample Size를 조정해주면 된다. 이때 예를 들어서 $\\hat{\\sigma}^2$가 0.024라고 한다면, Sample Size는 $2\\sqrt{0.024/S} \u0026lt; 0.01$로 계산해서 sample을 960개보다는 많이 뽑아야 함을 알 수 있다.\nMonte Carlo Method를 활용하면 다양한 것들을 할 수 있는데, 그 대표적인 예시로 아래 세 개를 이해해보자.\n1. Posterior Inference for Arbitrary Functions $\\theta$ 그 자체가 아니라 임의의 $f(\\theta)$의 posterior distribution이 궁금할 수 있다. 예를 들어서, log odds와 같은 것 말이다. 하지만 결국 $\\gamma = f(\\theta)$도 $\\theta$처럼 Monte Carlo Method로 사후분포을 추정할 수 있다.\n하나의 parameter만 있는 것이 아니라, 심지어 $Pr(\\theta_1 \u0026gt; \\theta_2 | Y_{1,1} = y_{1,1}, ..., Y_{n_2,2}=y_{n_2,2})$나 $Pr(\\theta_1/\\theta_2 | Y_{1,1} = y_{1,1}, ..., Y_{n_2,2}=y_{n_2,2})$처럼 parameter가 두 개인 경우도 구할 수 있다.\n2. Sampling from Predictive Distributions Step1. sample $\\theta^{(1)},...,\\theta^{(S)} \\text{ ~ i.i.d} \\ p(\\theta|y_1,...,y_n)$\nStep2. approximate $p(\\tilde{y}|y_1,...,.y_n)$ with $\\sum_{s=1}^{S}p(\\tilde{y}|\\theta^{(s)})/S$\n위 방법을 통해 $Pr(\\tilde{Y_1}\u0026gt;\\tilde{Y_2}|\\sum Y_{i,1}=217,\\sum Y_{i,2}=66)$를 근사하는 것도 가능하다. 왜냐하면 $Pr(\\tilde{Y_1})$와 $Pr(\\tilde{Y_2})$은 posterior independent하기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12  set.seed(1) a\u0026lt;-2 ; b\u0026lt;-1 sy1\u0026lt;-217 ; n1\u0026lt;-111 sy2\u0026lt;-66 ; n2\u0026lt;-44 theta1.mc\u0026lt;-rgamma(10000,a+sy1, b+n1) theta2.mc\u0026lt;-rgamma(10000,a+sy2, b+n2) y1.mc\u0026lt;-rpois(10000,theta1.mc) y2.mc\u0026lt;-rpois(10000,theta2.mc) mean(theta1.mc\u0026gt;theta2.mc)   ## [1] 0.9708\r1  mean(y1.mc\u0026gt;y2.mc)   ## [1] 0.4846\r위 결과를 통해서 주의깊게 살펴보아야 할 것은, 예측치의 차이와 모수의 차이가 같지 않다는 점이다.\n아래 세 개 구분하기  (1) sampling model: $Pr(\\tilde{Y}=\\tilde{y}|\\theta)$ (2) prior predictive model: $Pr(\\tilde{Y}=\\tilde{y})$  $\\theta$에 대한 사전확률분포가 관측가능한 데이터 $\\tilde{Y}$에 대해 합리적인 믿음을 나타낼 수 있는지 확인해보는 용도로 활용가능하다.   (3) posterior predictive model: $Pr(\\tilde{Y}=\\tilde{y}|Y_1=y_1, ..., Y_n=y_n)$  3. Posterior Predictive Model Checking  We should at least make sure that our model generates predictive datasets $\\tilde{Y}$ that resemble the observed dataset in terms of features that are of interest\n   Code  1 2 3  load(\u0026#34;gss.RData\u0026#34;) table(gss$DEG[gss$YEAR==1998])   ## ## 0 1 2 3 4 ## 430 1500 209 478 205\r1 2 3 4 5 6  y1\u0026lt;-gss$PRAYER[gss$YEAR==1998 \u0026amp; gss$RELIG==1 ] y1\u0026lt;-1*(y1==1) y1\u0026lt;-y1[!is.na(y1) ] sy1\u0026lt;-sum(y1) n1\u0026lt;-length(y1) sy1/n1   ## [1] 0.3616803\r1 2 3 4 5 6  y2\u0026lt;-gss$PRAYER[gss$YEAR==1998 \u0026amp; gss$RELIG!=1 ] y2\u0026lt;-1*(y2==1) y2\u0026lt;-y2[!is.na(y2) ] sy2\u0026lt;-sum(y2) n2\u0026lt;-length(y2) sy2/n2   ## [1] 0.5471464\r1  table(gss$FEMALE[gss$YEAR==1998])   ## ## 0 1 ## 1232 1600\r1 2 3 4 5 6  y\u0026lt;-gss$FEMALE[gss$YEAR==1998] y\u0026lt;-1*(y==1) y\u0026lt;-y[!is.na(y) ] sy\u0026lt;-sum(y) n\u0026lt;-length(y) sy/n   ## [1] 0.5649718\r1 2 3  sy\u0026lt;-sy2 n\u0026lt;-n2 sy/n   ## [1] 0.5471464\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #### MC approximations set.seed(1) a\u0026lt;-1 ; b\u0026lt;-1 theta.prior.sim\u0026lt;-rbeta(10000,a,b) gamma.prior.sim\u0026lt;- log( theta.prior.sim/(1-theta.prior.sim) ) n0\u0026lt;-860-441 ; n1\u0026lt;-441 theta.post.sim\u0026lt;-rbeta(10000,a+n1,b+n0) gamma.post.sim\u0026lt;- log( theta.post.sim/(1-theta.post.sim) ) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0)) par(mfrow=c(2,3)) par(cex=.8) par(mfrow=c(1,2),mar=c(3,3,1,1), mgp=c(1.75,.75,.0)) plot(density(gamma.prior.sim,adj=2),xlim=c(-5,5),main=\u0026#34;\u0026#34;, xlab=expression(gamma), ylab=expression(italic(p(gamma))),col=\u0026#34;gray\u0026#34;) plot(density(gamma.post.sim,adj=2),xlim=c(-5,5),main=\u0026#34;\u0026#34;,xlab=expression(gamma), ylab=expression(paste(italic(\u0026#34;p(\u0026#34;),gamma,\u0026#34;|\u0026#34;,y[1],\u0026#34;...\u0026#34;,y[n],\u0026#34;)\u0026#34;, sep=\u0026#34;\u0026#34;)) ) lines(density(gamma.prior.sim,adj=2),col=\u0026#34;gray\u0026#34;)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  set.seed(1) a\u0026lt;-2 ; b\u0026lt;-1 sy1\u0026lt;-217 ; n1\u0026lt;-111 sy2\u0026lt;-66 ; n2\u0026lt;-44 theta1.mc\u0026lt;-rgamma(10000,a+sy1, b+n1) theta2.mc\u0026lt;-rgamma(10000,a+sy2, b+n2) y1.mc\u0026lt;-rpois(10000,theta1.mc) y2.mc\u0026lt;-rpois(10000,theta2.mc) #### Posterior predictive check  y1\u0026lt;-gss$CHILDS[gss$FEMALE==1 \u0026amp; gss$YEAR\u0026gt;=1990 \u0026amp; gss$AGE==40 \u0026amp; gss$DEG\u0026lt;3 ] y1\u0026lt;-y1[!is.na(y1)] set.seed(1) a\u0026lt;-2 ; b\u0026lt;-1 t.mc\u0026lt;-NULL for(s in 1:10000) { theta1\u0026lt;-rgamma(1,a+sum(y1), b+length(y1)) y1.mc\u0026lt;-rpois(length(y1),theta1) t.mc\u0026lt;-c(t.mc,sum(y1.mc==2)/sum(y1.mc==1)) } t.obs\u0026lt;-sum(y1==2)/sum(y1==1) mean(t.mc\u0026gt;=t.obs)   ## [1] 0.0049\r   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  par(mar=c(3,3,1,1),mgp=c(1.75,.75,0)) par(mfrow=c(1,2)) ecdf\u0026lt;-(table(c(y1,0:9))-1 )/sum(table(y1)) #ecdf.mc\u0026lt;-(table(c(y1.mc,0:9))-1 )/sum(table(y1.mc)) ecdf.mc\u0026lt;- dnbinom(0:9,size=a+sum(y1),mu=(a+sum(y1))/(b+length(y1))) plot(0:9+.1,ecdf.mc,type=\u0026#34;h\u0026#34;,lwd=5,xlab=\u0026#34;number of children\u0026#34;, ylab=expression(paste(\u0026#34;Pr(\u0026#34;,italic(Y[i]==y[i]),\u0026#34;)\u0026#34;,sep=\u0026#34;\u0026#34;)),col=\u0026#34;gray\u0026#34;, ylim=c(0,.35)) points(0:9-.1, ecdf,lwd=5,col=\u0026#34;black\u0026#34;,type=\u0026#34;h\u0026#34;) legend(1.8,.35, legend=c(\u0026#34;empirical distribution\u0026#34;,\u0026#34;predictive distribution\u0026#34;), lwd=c(2,2),col= c(\u0026#34;black\u0026#34;,\u0026#34;gray\u0026#34;),bty=\u0026#34;n\u0026#34;,cex=.8) hist(t.mc,prob=T,main=\u0026#34;\u0026#34;,ylab=\u0026#34;\u0026#34;,xlab=expression(t(tilde(Y))) ) segments(t.obs,0,t.obs,.25,col=\u0026#34;black\u0026#34;,lwd=3)   실제(empirical) 분포와 예측 분포의 모습이 다소 차이가 남을 확인할 수 있다.\n참고 [1] FCB code\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":59,"section":"posts","tags":null,"title":"Monte Carlo Method","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb04/"},{"content":"Part 3. API는 무엇인가 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Spotify App 생성 및 토큰 발급 Client Credentials Flow 1 2 3 4 5  { \u0026#34;access_token\u0026#34;: \u0026#34;NgCXRKc...MzYjw\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, }    client id, client secret을 제공하면 우리는 3600초, 즉 1시간동안 사용할 수 있다.  2. Python 기본 1 2 3 4 5 6 7 8 9 10 11  import sys def main(): print(\u0026#39;fastcampus\u0026#39;) #python으로 실행했을 때, 해당 py파일 이름이 전달되면, main()을 실행하라 if __name__ == \u0026#39;__main__\u0026#39;: main() #직접 py파일이 실행 안되고, import spotify_api와 같이 모듈처럼 import되면, ~~를 print하라. else: print(\u0026#39;this script i being imported\u0026#39;)    Windows는 Windows Powershell을 통해서 진행하면 된다. 기본적으로 위처럼 코딩을 시작하게 된다.  3. Python Requests 패키지 requests python library \u0026gt; Developer Interface 참고하기\npowershell에서 pip install requests 실행하기\n4. API를 통한 데이터 요청 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  import sys import requests import base64 import json import logging client_id = \u0026#39;\u0026#39; # client_id 입력 client_secret = \u0026#39;\u0026#39; # client_secret 입력 def main(): headers = get_headers(client_id, client_secret) params = { \u0026#39;q\u0026#39;: \u0026#39;BTS\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;artist\u0026#39;, \u0026#39;limit\u0026#39;: 5 } r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) # print(r.status_code) # 200이면 이상 없는 것 # print(r.text) # sys.exit(0) def get_headers(client_id, client_secret): # 1시간만 있으면 expire되기 때문에 추가로 function 하나를 만들어두는 것이다. endpoint = \u0026#39;https://accounts.spotify.com/api/token\u0026#39; encoded = base64.b64encode(\u0026#34;{}:{}\u0026#34;.format(client_id, client_secret).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;ascii\u0026#39;) headers = { \u0026#39;Authorization\u0026#39;: \u0026#39;Basic {}\u0026#39;.format(encoded) } payload = { \u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39; } r = requests.post(endpoint, data=payload, headers=headers) # 중간에 잘 되는지 확인해보는 코드 # print(r.status_code) # print(r.text) # print(type(r.text)) #string으로 출력되므로 아래에서 json.loads를 통해 dictionary로 만들어줘야 한다. # sys.exit(0) access_token = json.loads(r.text)[\u0026#39;access_token\u0026#39;] headers = { \u0026#39;Authorization\u0026#39;: \u0026#34;Bearer {}\u0026#34;.format(access_token) } return headers if __name__ == \u0026#39;__main__\u0026#39;: main()   5. Status Code  Status Code를 알아야 하는 이유: 데이터 엔지니어의 잘못이 아닌, Spotify 서버의 오류 등으로 인한 문제인지 체크할 수 있다. Spotify Web API 기준이지만, RFC 2616와 RFC 6585에 의해 일반적으로 통용되는 기준이다.     STATUS CODE DESCRIPTION     200 OK - The request has succeeded. The client can read the result of the request in the body and the headers of the response.   201 Created - The request has been fulfilled and resulted in a new resource being created.   202 Accepted - The request has been accepted for processing, but the processing has not been completed.   204 No Content - The request has succeeded but returns no message body.   304 Not Modified. See Conditional requests.   400 Bad Request - The request could not be understood by the server due to malformed syntax. The message body will contain more information; see Response Schema.   401 Unauthorized - The request requires user authentication or, if the request included authorization credentials, authorization has been refused for those credentials.   403 Forbidden - The server understood the request, but is refusing to fulfill it.   404 Not Found - The requested resource could not be found. This error can be due to a temporary or permanent condition.   429 Too Many Requests - Rate limiting has been applied.   500 Internal Server Error. You should never receive this error because our clever coders catch them all … but if you are unlucky enough to get one, please report it to us through a comment at the bottom of this page.   502 Bad Gateway - The server was acting as a gateway or proxy and received an invalid response from the upstream server.   503 Service Unavailable - The server is currently unable to handle the request due to a temporary condition which will be alleviated after some delay. You can choose to resend the request again.    6. 에러 핸들링 sys.exit(0)과 sys.exit(1) 차이 1 2 3 4  # 프로그램을 정상적으로 종료시키고 싶을 때 sys.exit(0) # 프로그램을 강제적으로 종료시키고 싶을 때 sys.exit(1)   Status Code 401, 409 에러 핸들링 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) if r.status_code != 200: logging.error(r.text) ## Too many requests if r.status_code == 429: retry_after = json.loads(r.headers)[\u0026#39;Retry-After\u0026#39;] time.sleep(int(retry_after)) r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) ## access_token expireed elif r.status_code == 401: headers = get_headers(client_id, client_secret) r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) else: sys.exit(1) #강제종료   7. 페이지네이션 핸들링 참고사이트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  import sys import requests import base64 import json import logging client_id = \u0026#39;\u0026#39; # client_id 입력 client_secret = \u0026#39;\u0026#39; # client_secret 입력 def main(): headers = get_headers(client_id, client_secret) # Get BTS\u0026#39; Albums r = requests.get(\u0026#39;https://api.spotify.com/v1/artists/3Nrfpe0tUJi4K4DXYWgMUX/albums\u0026#39;, headers=headers) raw = json.loads(r.text) # total = raw[\u0026#39;total\u0026#39;] # 총 104개가 있음을 확인 # offset = raw[\u0026#39;offset\u0026#39;] # 시작은 0 # limit = raw[\u0026#39;limit\u0026#39;] # 20개씩 뽑겠다. next = raw[\u0026#39;next\u0026#39;] albums = [] albums.extend(raw[\u0026#39;items\u0026#39;]) ## 난 200개만 뽑아 오겠다. count = 0 while count \u0026lt; 200 and next: # while next: 라고만 하면 끝까지 가져오게 된다. r = requests.get(raw[\u0026#39;next\u0026#39;], headers=headers) raw = json.loads(r.text) next = raw[\u0026#39;next\u0026#39;] # print(next) # 맨 마지막에는 none이 나오게 된다. 이에 대해서는 Spotify 페이지를 참고하면 된다. albums.extend(raw[\u0026#39;items\u0026#39;]) count = len(albums) print(len(albums)) def get_headers(client_id, client_secret): endpoint = \u0026#39;https://accounts.spotify.com/api/token\u0026#39; encoded = base64.b64encode(\u0026#34;{}:{}\u0026#34;.format(client_id, client_secret).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;ascii\u0026#39;) headers = {\u0026#39;Authorization\u0026#39;: \u0026#39;Basic {}\u0026#39;.format(encoded)} payload = {\u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39;} r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)[\u0026#39;access_token\u0026#39;] headers = {\u0026#39;Authorization\u0026#39;: \u0026#34;Bearer {}\u0026#34;.format(access_token)} return headers if __name__ == \u0026#39;__main__\u0026#39;: main()   \n","description":"API는 무엇인가","id":60,"section":"posts","tags":null,"title":"Spotify API","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part3_2/"},{"content":"Chapter 05. Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods와 Bayesian Data Analysis를 참고하였다.\nWarm up!  Gamma Distribution Inverse Gamma Distribution Scaled Inverse Chi-squared Distribution  1. Single Parameter Conjugacy 평균이나 분산 중 하나만을 모르는 경우\n1-1. 평균을 모르는 경우 Prior: $\\mu \\text{ ~ } N(\\mu_0, \\tau_0^{2})$\nLikelihood: $y|\\mu \\text{ ~ } N(\\mu, \\sigma^2)$\nPosterior: $\\mu|y \\text{ ~ } N(\\mu_n, \\tau_n^{2})$\nwhere $\\frac{1}{\\tau_n^{2}} = \\frac{1}{\\tau_0^{2}} + \\frac{n}{\\sigma^2}$ and $\\mu_n = \\frac{\\frac{1}{\\tau_0^{2}}}{\\frac{1}{\\tau_0^{2}} + \\frac{n}{\\sigma^2}}\\mu_0 + \\frac{\\frac{n}{\\sigma^2}}{\\frac{1}{\\tau_0^{2}} + \\frac{n}{\\sigma^2}}\\bar{y} $\nPosterior Predictive: $\\tilde{y}|y \\text{ ~ } N(\\mu_n, \\sigma^2+\\tau_n^{2})$\n1-2. 분산을 모르는 경우 Prior: $\\sigma^2 \\text{ ~ } \\chi^{-2}(\\nu_0, \\sigma_0^2)$\nLikelihood: $y|\\sigma^2 \\text{ ~ } N(\\mu, \\sigma^2)$ Posterior: $\\sigma^2|y \\text{ ~ } \\chi^{-2}(\\nu_n, \\sigma_n^2)$\nwhere $\\nu_n = \\nu_0 + n$ and $\\sigma_n^2 = \\frac{\\nu_0\\sigma_0^2 + ns(y)}{\\nu_0 + n}$\nc.f. $s(y) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\mu)^2$, 이는 MLE이다(biased estimator). 참고로, 베이지안은 frequentist들의 기준인 unbiasedness를 중요하게 생각하지 않는다.\n2. Two Parameter marginal distribution 얻는 두 가지 방법  Integreation: joint posterior distribution을 구한 후, 관심 없는 모수(nuisance parameter)에 대해 적분 Simulation: joint posterior distribution에서 sample을 구한 후, 관심 있는 모수의 분포만 고려(나머지는 무시)  그렇다면 joint posterior distribution은 어떻게 구할까?  marginal and conditional simulation을 통해서 구할 수 있다.\n$\\theta_2 \\text{ ~ } \\theta_2|y$ and $\\theta_1 | \\theta_2, y$\n$\\rightarrow (\\theta_1, \\theta_2) \\text{ ~ } (\\theta_1, \\theta_2|y)$  2-1. noninformative prior Prior: $p(\\mu, \\sigma^2) = p(\\mu)p(\\sigma^2) \\propto (\\sigma^2)^{-1} $ (독립 가정, improper prior)\nLikelihood: $p(y|\\mu, \\sigma^2) \\propto \\sigma^{-n}exp(\\frac{-1}{2}\\sigma^2\\sum_{i=1}^{n}(y_i - \\mu)^2) $\nPosterior: $\\mu, \\sigma^2 |y \\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n}) \\times \\chi^{-2}(n-1, s^2)$\nPosterior Predictive: $\\tilde{y}|y \\text{ ~ } t_{n-1}(\\bar{y}, (1+\\frac{1}{n}s^2))$\n이는 posterior과 비교해서, data의 uncertainty($s^2$)이 추가된 형태라고 해석할 수 있다.\nPosterior Distribution 구하기 (Noninformative) 해당 Posterior Distribution을 구하는 과정은 다소 복잡하기 때문에 자세하게 서술해보도록 하겠다.\n우선 시작하기에 앞서, 한마디로 이 과정을 요악한다면 Conditional Posterior X Marginal일 것이다.\nSTEP1. $p(\\mu|\\sigma^2,y)$ $p(\\sigma^2|y)$의 형태를 파악한다.\n  $\\mu|\\sigma^2,y \\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n})$\n이부분은 위의 평균을 모르지만, 분산을 아는 경우에서 prior precision $\\frac{1}{\\tau^2}=0$으로 주면 위와 같이 나온다. prior precision을 0으로 주는 이유는, non-informative prior를 가정하고 있기 때문이다.\n  $\\sigma^2|y \\text{ ~ } \\chi^{-2}(n-1, s^2)$\n이는 아래의 수식을 계산해서 얻을 수 있다.\n  \\begin{align}\rp(\\mu, \\sigma^2|y) \u0026amp;\\propto p(\\mu, \\sigma^2) \\times p(y|\\mu, \\sigma^2) \\\\\r\u0026amp;\\propto \\sigma^{-n-2}exp\\bigg(\\frac{-1}{2\\sigma^2}\\big[(n-1)s^2 + n(\\bar{y}-\\mu)^2\\big]\\bigg) \\\\\r\\rightarrow p(\\sigma^2|y) \u0026amp;= \\int p(\\mu,\\sigma^2|y)d\\mu \\end{align}\nSTEP2. 베이즈룰을 이용하여 posterior distribution을 계산해준다.\n위의 과정을 거친다면, 그 결과는 아래와 같이 정리할 수 있다.\n\\begin{align}\r\\mu|\\sigma^2,y \u0026amp;\\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n}) \\\\\r\\sigma^2|y \u0026amp;\\text{ ~ } \\chi^{-2}(n-1, s^2) \\\\\r\\mu, \\sigma^2 |y \u0026amp;\\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n}) \\times \\chi^{-2}(n-1, s^2)\r\\end{align}\nPosterior Mean의 Marginal Distribution 구하기 번외로, $\\mu$의 marginal posterior distribution $p(\\mu|y)$은 $\\int p(\\mu,\\sigma^2)d\\sigma^2$를 통해서 구할 수 있다. 그리고 형태는 아래와 같다.\n$$p(\\mu|y) \\text{ ~ } t_{n-1}(\\bar{y}, \\frac{s^2}{n})$$\nPosterior Prediction 구하는 과정 \\begin{align}\rp(\\tilde{y}|y) \u0026amp;= \\int\\int p(\\tilde{y}|\\mu,\\sigma^2) p(\\mu, \\sigma^2|y)\\ d\\mu \\ d\\sigma^2 \\\\\r\u0026amp;= \\int\\int p(\\tilde{y}|\\mu,\\sigma^2) \\ p(\\mu|\\sigma^2,y)\\ d\\mu \\cdot p(\\sigma^2|y) \\ d\\sigma^2 \\\\ \u0026amp;= \\int p(\\tilde{y}|\\sigma^2) \\ p(\\sigma^2|y) \\ d\\sigma^2\r\\end{align}\nPosterior Predictive: $\\tilde{y}|y \\text{ ~ } t_{n-1}(\\bar{y}, (1+\\frac{1}{n}s^2))$\n이 결과를 바로 위의 Posterior Mean의 marginal 분포와 비교해보는 것이 중요하다.\n왜냐하면 prediction을 할 때에 $s^2$, 즉 uncertainty가 추가된다고 해석할 수 있기 때문이다.\nTwo parameter Normal model이 중요한 이유는 다음 3. Frequentist와 Bayesian의 차이을 보면 명확하다. Frequentist와 Bayesian의 기본적인 전제와 입장 차이를 이해한다면, 정보가 없는 prior가 결국 어떠한 결론으로 이어가는지 이해할 수 있다.\n2-2. conjugate prior Prior: $p(\\mu, \\sigma^2) = p(\\mu|\\sigma^2) \\times p(\\sigma_0^2) \\text{ ~ N-Inv-} \\chi^2(\\mu_0, \\frac{\\sigma^2}{k_0}; v_0, \\sigma_0^2)$\n\\begin{align}\r\\mu|\\sigma^2 \u0026amp;\\text{ ~ } N(\\mu_0, \\frac{\\sigma^2}{k_0}) \\\\\r\\sigma^2 \u0026amp;\\text{ ~ } \\chi^{-2}(v_0, \\sigma^2_0) \\\\\r\\rightarrow \\mu, \\sigma^2 \u0026amp;\\propto \\sigma^{-1}(\\sigma^2)^{-(\\frac{v_0}{2}+1)}exp\\bigg(\\frac{-1}{2\\sigma^2}\\big[v_0\\sigma_0^2 + k_0(\\mu_0 - \\mu)^2\\big]\\bigg)\r\\end{align}\nLikelihood: $p(y|\\mu, \\sigma^2) \\propto \\sigma^{-n}exp\\bigg(\\frac{-1}{2\\sigma^2}\\sum_{i=1}{n}(y_i-\\mu)^2\\bigg)$\nPosterior: $p(\\mu, \\sigma^2|y) \\text{ ~ N-Inv-}\\chi^2(\\mu_n, \\frac{\\sigma_n^2}{k_n}; v_n, \\sigma_n^2) $\n\\begin{align}\r\\mu_n \u0026amp;= \\frac{k_0}{k_0+n}\\mu_0 + \\frac{n}{k_0+n}\\bar{y} \\\\\rk_n \u0026amp;= k_0 +n \\\\\rv_n \u0026amp;= v_o + n \\\\\rv_n\\sigma_n^2 \u0026amp;= v_0\\sigma_0^2 + (n-1)s^2 + \\frac{k_0n}{k_0+n}(\\bar{y}-\\mu_0)^2 \\\\\r\\rightarrow \\text{posterior ss} \u0026amp;= \\text{prior ss} + \\text{sample ss} + \\text{additional uncertainty}(\\bar{y}-\\mu_0)\r\\end{align}\n3. Frequentist와 Bayesian의 차이 Frequentist: parameter를 알 때, 통계량의 분포에 대해 이야기한다.\nlet $y \\text{ ~ } N(\\mu, \\sigma^2)$\n $\\bar{y} \\text{ ~ } N(\\mu, \\frac{\\sigma^2}{n}) $ $\\frac{(n-1)s^2}{\\sigma^2} \\text{ ~ } \\chi^2(n-1)$ $\\frac{\\bar{y}-\\mu}{s/\\sqrt{n}}|\\mu,\\sigma^2 \\text{ ~ } t_{n-1}$  Bayesian: data를 알 때, parameter의 분포에 대해 이야기한다.\n $\\mu \\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n})$ $\\sigma^2 \\text{ ~ } \\chi^{-2}(n-1, s^2)$ $\\frac{\\mu-\\bar{y}}{s/\\sqrt{n}}|y \\text{ ~ } t_{n-1} $  만약 Bayesian이 noninformative prior를 가정한다면, 즉 prior가 거의 없다고 생각한다면 frequetist랑 결과가 비슷하게 나오는 것은 당연하다.\n4. Multinomial Model Likelihood: $y|\\theta \\text{ ~ Multinomial}(\\theta) \\propto \\prod_{j=1}^{k}\\theta_j^{y_j}$\nPrior: $\\theta \\text{ ~ } Dir(\\alpha) \\propto \\prod_{j=1}^{k}\\theta_j^{\\alpha_j-1}$\nPosterior: $\\theta|y \\text{ ~ } Dir(\\alpha +y) \\propto \\prod_{j=1}^{k}\\theta_j^{\\alpha_j-y_j-1}$\n참고로 Multinomial distribution은 이항분포의 확장이며, Dirichlet distribution은 베타분포의 확장이라고 생각하면 쉽다. 왜냐하면 Beta-Binomial 모델에 대해서는 Chapter3에서 이미 충분히 다루었기 때문이다.\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"Normal Model","id":61,"section":"posts","tags":null,"title":"Normal Model","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb05/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n0. Data Type  numeric data/time character/string unicode character/string binary miscellaneous  1. Relational Database(RDB)  모든 데이터를 2차원의 테이블로 표현 하나 이상의 테이블로 구성 Entity-Relationship 모델 Normalization (Reduce Redundacy)  2. AWS 클라우드 MySQL 데이터베이스 생성  aws.amazon.com \u0026gt; RDS \u0026gt; 데이터 생성 Templates \u0026gt; Free Tier로 설정 (과금 예방) Public Access 허용하기 VPC에서 인바운드 규칙에 MySql 추가하기  3. 터미널에서 데이터베이스 연결하기 (Windows 기준)\n mysql client workbench 다운로드 MySQL Workbench랑 AWS를 연결하고, 그것을 termianl(powershell)로 연결하는 법 termianl에서 아래의 커맨드를 작성하고, 이어서 비밀번호를 입력해주면 된다.  mysql -h {hostname} -P 3306 -D {Default Schema} -u {username} -p 4. MySQL 데이터베이스 안에서 테이블 생성 1 2  CREATE TABLE people (first_name VARCHAR(20), last_name VARCHAR(20), age INT); SHOW TABLES;   5. 엔터티 관계도(ERD)  Entity Relationship Diagram 데이터 모델링 설계 과정에서 사용하는 모델 약속된 기호를 이용하여 데이터베이스의 구조를 쉽게 이해하기 위함이다.  ERD의 기본요소  Entities: 개체 Attributes: 엔터티의 속성 Relationship: 엔터티 간의 관계  6. Primary Key \u0026amp; Unique Key Primary Key  테이블에 하나 밖에 없는 유니크한 구별 값 Null 값 안 됨  Foreign Key  한 개 이상 가능 NULL 값도 가능  Unique Key  Primary Key처럼 유니크하긴 하다. 하지만, Null 값은 하나는 가질 수 있다. 그리고 하나 이상의 유니크 키를 가질 수 있다. Primary Key보다는 index로서의 성능은 낮다. ex) Primary Key: 수험번호, Unique Key: 주민등록번호  ","description":"데이터의 이해와 데이터베이스","id":62,"section":"posts","tags":null,"title":"RDBMS","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_1/"},{"content":"Chapter 06. Posterior Approximation with the Gibbs sampler 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\n1. A Semi-conjugate prior distribution 2. Discrete approximations 3. Sampling from the conditional distributions 4. Gibbs Sampling 5. General properties of the Gibbs sampler 6. Introduction to MCMC diagnostics Conclusion semi-conjugate 분포를 모두 알면 full conditional probability를 아는 것과 거의 같다. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"Gibbs Sampling","id":63,"section":"posts","tags":null,"title":"Gibbs Sampling","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb06/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Spotify 데이터 이해 Spotify Web API \u0026gt; get an artist\nartist object 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  { \u0026#34;external_urls\u0026#34; : { \u0026#34;spotify\u0026#34; : \u0026#34;https://open.spotify.com/artist/0OdUWJ0sBjDrqHygGUXeCF\u0026#34; }, \u0026#34;followers\u0026#34; : { \u0026#34;href\u0026#34; : null, \u0026#34;total\u0026#34; : 306565 }, \u0026#34;genres\u0026#34; : [ \u0026#34;indie folk\u0026#34;, \u0026#34;indie pop\u0026#34; ], \u0026#34;href\u0026#34; : \u0026#34;https://api.spotify.com/v1/artists/0OdUWJ0sBjDrqHygGUXeCF\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;0OdUWJ0sBjDrqHygGUXeCF\u0026#34;, \u0026#34;images\u0026#34; : [ { \u0026#34;height\u0026#34; : 816, \u0026#34;url\u0026#34; : \u0026#34;https://i.scdn.co/image/eb266625dab075341e8c4378a177a27370f91903\u0026#34;, \u0026#34;width\u0026#34; : 1000 }, { \u0026#34;height\u0026#34; : 522, \u0026#34;url\u0026#34; : \u0026#34;https://i.scdn.co/image/2f91c3cace3c5a6a48f3d0e2fd21364d4911b332\u0026#34;, \u0026#34;width\u0026#34; : 640 }, { \u0026#34;height\u0026#34; : 163, \u0026#34;url\u0026#34; : \u0026#34;https://i.scdn.co/image/2efc93d7ee88435116093274980f04ebceb7b527\u0026#34;, \u0026#34;width\u0026#34; : 200 }, { \u0026#34;height\u0026#34; : 52, \u0026#34;url\u0026#34; : \u0026#34;https://i.scdn.co/image/4f25297750dfa4051195c36809a9049f6b841a23\u0026#34;, \u0026#34;width\u0026#34; : 64 } ], \u0026#34;name\u0026#34; : \u0026#34;Band of Horses\u0026#34;, \u0026#34;popularity\u0026#34; : 59, \u0026#34;type\u0026#34; : \u0026#34;artist\u0026#34;, \u0026#34;uri\u0026#34; : \u0026#34;spotify🧑‍🎨0OdUWJ0sBjDrqHygGUXeCF\u0026#34; }   2. Spotify 데이터 모델 예시 ","description":"데이터의 이해와 데이터베이스","id":64,"section":"posts","tags":null,"title":"Spotify Data","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_2/"},{"content":"2021.07.03  연남동에서 내츄럴 와인을 처음으로 마셔보았다. (위치: 로제 드 미네)   RUE DE LA SOIF Rose Pet Nat: 자몽맛 와인    L\u0026rsquo;indolent: 당도 있는 화이트와인    2021.07.06  첫번째. 신촌 소담식당 / 클로리스  2021.07.10  두번째. 연희동 시오 / 떼뮤즐렛(페델리에 로제)  ","description":"","id":65,"section":"updates","tags":null,"title":"July 2021","uri":"https://jiwooblog.netlify.app/updates/diary/2021_07/"},{"content":"Chapter 07. The Multivariate Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\n1. Multivariate Normal Desity univariate model에 대해서 앞선 챕터에서 이야기를 많이 했지만, 사실 현실세계에서는 multivariate인 경우가 훨씬 많다.\n1-1. Bivariate Normal   Bivariate Case  1 2 3 4 5  library(tidyverse) library(gridExtra) library(MASS) library(reshape2) library(ash)   1 2 3 4 5 6 7 8 9 10 11 12 13  #### 4-1. Draw yourself Figure 7.1 # 초기 설정 inv \u0026lt;- solve MU = matrix(c(50,50), ncol=1) SIGMA = matrix(c(64,0,0,144), ncol=2) # MVN pdf calc.dmvn = Vectorize(function(a,b, mu=MU, sigma=SIGMA){ y \u0026lt;- c(a,b) log.p \u0026lt;- (-nrow(mu)/2)*log(2*pi) - 0.5*log(det(sigma)) - 0.5*(t(y-mu) %*% inv(sigma) %*% (y-mu)) exp(log.p) })   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  # do it at once allInOne \u0026lt;- function(corr){ SIGMA = matrix(c(64,0,0,144), ncol=2) s1 \u0026lt;- sqrt(SIGMA[1,1]); s2 \u0026lt;- sqrt(SIGMA[2,2]) SIGMA[1,2] \u0026lt;- s1*s2*corr; SIGMA[2,1] \u0026lt;- s1*s2*corr # MVN density function calc.dmvn = Vectorize(function(a,b, mu=MU, sigma=SIGMA){ y \u0026lt;- c(a,b) log.p \u0026lt;- (-nrow(mu)/2)*log(2*pi) - 0.5*log(det(sigma)) - 0.5*(t(y-mu) %*% inv(sigma) %*% (y-mu)) exp(log.p) }) # sample sample = mvrnorm(n=30, mu=MU, Sigma=SIGMA) sample = data.frame(sample) colnames(sample) = c(\u0026#39;y1\u0026#39;,\u0026#39;y2\u0026#39;) # calculate density xLim = seq(20, 80, length=101) yLim = seq(20, 80, length=101) density.mvn \u0026lt;- outer(xLim, yLim, FUN=calc.dmvn) rownames(density.mvn) \u0026lt;- xLim colnames(density.mvn) \u0026lt;- yLim density.mvn \u0026lt;- melt(density.mvn) # graph density.mvn %\u0026gt;% ggplot(aes(x=Var1, y=Var2)) + geom_tile(aes(fill=value, alpha=value)) + geom_contour(aes(z=value), color=\u0026#39;white\u0026#39;, size=0.1) + geom_point(data=sample, mapping=aes(x=y1, y=y2, color=\u0026#39;red\u0026#39;), show.legend=FALSE) + scale_fill_gradient(low=\u0026#39;grey\u0026#39;, high=\u0026#39;steelblue\u0026#39;, guide=FALSE) + scale_alpha(guide=FALSE) + theme(legend.position=\u0026#39;None\u0026#39;) + theme_bw() + ggtitle(paste0(\u0026#39;corr=\u0026#39;,corr)) + xlab(\u0026#39;y1\u0026#39;) + ylab(\u0026#39;y2\u0026#39;) }   1 2 3 4  p1 \u0026lt;- allInOne(corr=-0.5) p2 \u0026lt;- allInOne(corr=0) p3 \u0026lt;- allInOne(corr=0.5) grid.arrange(p1,p2,p3, nrow=1)     1-2. Multivariate Normal Model $$p(\\boldsymbol{y}|\\boldsymbol{\\mu}, \\Sigma) = (2\\pi)^{-p/2}|\\Sigma|^{-1/2}exp\\Big(-\\frac{1}{2}(\\boldsymbol{y}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\boldsymbol{y}-\\boldsymbol{\\mu}) \\Big) $$\nwhere\n$$\\boldsymbol{y} = \\begin{pmatrix}\ry_1 \\\\\ry_2 \\\\\r\\vdots \\\\\ry_p\r\\end{pmatrix}$$\n$$\\boldsymbol{\\mu} = \\begin{pmatrix}\r\\mu_1 \\\\\r\\mu_2 \\\\\r\\vdots \\\\\r\\mu_p\r\\end{pmatrix}$$\n$$\\Sigma = \\begin{pmatrix}\r\\sigma^2_{1} \u0026amp; \\sigma_{1,2} \u0026amp; \\cdots \u0026amp; \\sigma_{1,p} \\\\\r\\sigma_{2,1} \u0026amp; \\sigma^2_{2} \u0026amp; \\cdots \u0026amp; \\sigma_{2,p} \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\\r\\sigma_{p,1} \u0026amp; \\cdots \u0026amp; \\cdots \u0026amp; \\sigma^2_{p}\r\\end{pmatrix}$$\n2. Semiconjugate prior distribution for the mean (known covariance matrix) Semiconjugate라고 하는 것은, 두 모수 중 하나가 주어졌을 경우에 conjugate한 경우를 뜻한다.\n여기서는 공분산 행렬이 주어졌을 때, 평균 벡터의 semiiconjugate prior를 구하는 것(조금 더 쉬움)을 먼저 보고 이어서 공분산 행렬의 semiconjugate prior를 구하는 것을 살펴볼 것이다.\nPrior: $\\boldsymbol{\\mu} \\text{ ~ } MVN(\\boldsymbol{\\mu_0}, \\Lambda_0)$\n\\begin{align}\rp(\\boldsymbol{\\mu}) \u0026amp;= (2\\pi)^{-p/2}|\\Lambda_0|^{-1/2}exp\\Big(-\\frac{1}{2}(\\boldsymbol{\\mu}-\\boldsymbol{\\mu_0})^T\\Lambda_0^{-1}(\\boldsymbol{\\mu}-\\boldsymbol{\\mu_0})\\Big) \\\\\r\u0026amp;\\propto exp(-\\frac{1}{2}\\boldsymbol{\\mu}^T\\Lambda^{-1}\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\Lambda^{-1}_0\\boldsymbol{\\mu_0}) \\\\\r\u0026amp;= exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_0\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_0})\r\\end{align}\nwhere $A_0 = \\Lambda^{-1}_0, \\boldsymbol{b_0} = \\Lambda^{-1}_0\\boldsymbol{\\mu_0}$\nLikelihood: $Y_1, ..., Y_n|\\boldsymbol{\\mu},\\Sigma \\text{ ~ iid } MVN(\\boldsymbol{\\mu}, \\Sigma)$\n\\begin{align}\rp(\\boldsymbol{y_1}, ...,\\boldsymbol{y_n}|\\boldsymbol{\\mu},\\Sigma) \u0026amp;= \\prod^{n}_{i=1} (2\\pi)^{-p/2}|\\Sigma|^{-1/2}exp\\Big(-\\frac{1}{2}(\\boldsymbol{y_i}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\boldsymbol{y_i}-\\boldsymbol{\\mu}) \\Big) \\\\\r\u0026amp;= (2\\pi)^{-np/2}|\\Sigma|^{-n/2}exp\\Big(-\\frac{1}{2}\\sum_{i=1}^{n}(\\boldsymbol{y_i}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\boldsymbol{y_i}-\\boldsymbol{\\mu}) \\Big) \\\\\r\u0026amp;\\propto exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_1\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_1})\r\\end{align}\nwhere $A_1 = n\\Sigma^{-1}, \\boldsymbol{b_1} = n\\Sigma^{-1}\\boldsymbol{\\bar{y}}$\nPosterior: $\\boldsymbol{\\mu}|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}, \\Sigma \\text{ ~ } MVN(\\boldsymbol{\\mu_n}, \\Lambda_n)$\n$$\n\\begin{align}\rp(\\boldsymbol{\\mu}|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}, \\Sigma) \u0026amp;\\propto exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_0\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_0}) \\times exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_1\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_1}) \\\\\r\u0026amp;= exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_n\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_n}) \\\\\r\\\\\r\\text{where } A_n \u0026amp;= A_0 + A_1 = \\Lambda_0^{-1}+n\\Sigma^{-1} \\\\\r\\boldsymbol{b_n} \u0026amp;= \\boldsymbol{b_0} + \\boldsymbol{b_1} = \\Lambda_0^{-1}\\boldsymbol{\\mu_0}+n\\Sigma^{-1}\\boldsymbol{\\bar{y}} \\\\\r\\\\\r\\rightarrow \\Lambda_n^{-1} \u0026amp;= \\Lambda_0^{-1}+n\\Sigma^{-1} \\\\\r\\boldsymbol{\\mu_n} \u0026amp;= (\\Lambda_0^{-1}+n\\Sigma^{-1})^{-1}(\\Lambda_0^{-1}\\boldsymbol{\\mu_0}+n\\Sigma^{-1}\\boldsymbol{\\bar{y}})\r\\end{align}\n$$\n3. Semiconjugate prior distribution for the covariance matrix (known mean) 이는 상대적으로 복잡한데, 그 이유는 이전과 달리 matrix 형태에다가 prior을 주어야하기 때문이다.\n그래서 구체적인 prior와 likelihood를 이야기하기 앞서서 필요한 두 가지 개념에 대해서 짚고 넘어가도록 하자.\n첫 번째는 inverse-Wishart 분포이며, 다음은 Positive Definite이라는 선형대수 개념이다.\n3-1. inverse-Wishart Distribution inverse-Wishart Distribution은 공분산 행렬의 semiconjugate prior을 주기 위해서 알아야 하는 함수이다. 낯선 확률분포처럼 보이기도 하지만, 자세히 살펴보면 이는 inverse-Gamma distribution의 다차원 확장 버전에 불과하긴 하다.\n3-2. Positive Definite Covariance Matrix는 Positive Definite Matrix이어야 한다. Positive Definite의 정의는 아래와 같다.\n$$\\boldsymbol{x'}\\Sigma\\boldsymbol{x} \u0026gt; 0 \\ \\text{ for all vectors} \\ \\boldsymbol{x}$$\nunivariate case에서 분산이 언제나 0 이상이어야 하는 것처럼, 이와 같은 맥락의 조건을 다차원에서 만족하려면 PD(Positive Definite)이어야 한다. 만약 공분산 행렬이 Positive Definite하다면, 이는 모든 분산이 0보다 크며 공분산이 -1과 1 사이에 있도록 한다.\n또한, PD는 대칭행렬(symmetric)에서 정의되는 개념이기 때문에, $\\sigma_{i,j} = \\sigma_{j,i}$라는 조건도 자연스럽게 성립한다.\n3-2-1. Positive Definite이 되기 위한 조건은? 다시 한번 말하자면, Positive Definite은 대칭행렬의 특수한 형태이며, 모든 eigenvalue들이 0보다 크다는 말과 같다.\n여기서 eigenvalue가 0보다 크다는 것은 정확히 무슨 의미일까?\n정방행렬을 Spectral Decomposition을 했을 때, $A = VDV^{-1}$\n$$A_{p\\text{ x }p} = \\begin{bmatrix}\r| \u0026amp; \u0026amp; |\\\\\ra_1 \u0026amp; \\cdots \u0026amp; a_p\\\\\r| \u0026amp; \u0026amp; |\\\\\r\\end{bmatrix} = \\begin{bmatrix}\r| \u0026amp; \u0026amp; |\\\\\rv_1 \u0026amp; \\cdots \u0026amp; v_p\\\\\r| \u0026amp; \u0026amp; | \\\\\r\\end{bmatrix} \\begin{bmatrix}\r\\lambda_1 \u0026amp; \u0026amp; \\\\\r\u0026amp; \\ddots \u0026amp; \\\\\r\u0026amp; \u0026amp; \\lambda_p\r\\end{bmatrix} \\begin{bmatrix}\r| \u0026amp; \u0026amp; |\\\\\rv_1 \u0026amp; \\cdots \u0026amp; v_p\\\\\r| \u0026amp; \u0026amp; |\\\\\r\\end{bmatrix}^{-1}$$\neigenvalue가 0보다 크다는 것은, 선형변환을 했을 때 그 기저의 방향이 반대로 바뀌지는 않는다는 것을 의미한다.\n3-3. random Covariance Matrix 만들기 이는 covariance matrix에 대해 uninformative prior를 주기 위함이다.\n$$\\frac{1}{n}\\sum_{i=1}^n\\boldsymbol{z_iz_i^T} = \\frac{1}{n}Z^TZ$$\n$$\\boldsymbol{z_iz_i^T} = \\begin{pmatrix}\rz_{i,1}^2 \u0026amp; z_{i,1}z_{i,2} \u0026amp; \\cdots \u0026amp; z_{i,1}z_{i,p} \\\\\rz_{i,2}z_{i,1} \u0026amp; z_{i,2}^2 \u0026amp; \\cdots \u0026amp; z_{i,2}z_{i,p} \\\\\r\\vdots \u0026amp; \u0026amp; \u0026amp; \\vdots \\\\\rz_{i,p}z_{i,1} \u0026amp; z_{i,p}z_{i,2} \u0026amp; \\cdots \u0026amp; z_{i,p}^2\r\\end{pmatrix}$$\n\\begin{align}\r\\frac{1}{n}\\big[Z^TZ\\big]_{j,j} \u0026amp;= \\frac{1}{n}\\sum_{i=1}^{n}z_{i,j}^2 = s_{j,j} = s_j^2\\\\\r\\frac{1}{n}\\big[Z^TZ\\big]_{j,k} \u0026amp;= \\frac{1}{n}\\sum_{i=1}^{n}z_{i,j}z_{i,k} = s_{j,k}\r\\end{align}\n여기서 n \u0026gt; p 이고, 모든 $\\boldsymbol{z_i}$들이 서로 선형독립이라면, $Z^TZ$는 항상 positive definite일 것이다.\n$$\\text{Proof) } \\boldsymbol{x}^{T}Z^{T}Z\\boldsymbol{x} = (Z\\boldsymbol{x})^{T}(Z\\boldsymbol{x}) = ||Z\\boldsymbol{x}||^2 \\ge 0$$\nSTEP1. Set $\\nu_0$(prior sample size), $\\Phi_0$(prior covariance matrix)\nSTEP2. Sample $\\boldsymbol{z_i} \\ \\stackrel{iid}{\\sim} \\ MVN(\\boldsymbol{0}, \\Phi_0)$\nSTEP3. Calculate $Z_TZ = \\sum_{i=1}^{\\nu_0}\\boldsymbol{z_iz_i^T}$\nSTEP4. repeat the procedure S times generating $Z_i^TZ_i$\n${Z_1^TZ_1, Z_2^TZ_2, ..., Z_S^TZ_S} \\text{ ~ } Wis(\\nu_0, \\Phi_0)$\nWishart분포와 inv-Wishart분포 특징 $$\\Sigma^{-1} \\sim Wis(\\nu_0, S_0^{-1}) \\rightarrow E[\\Sigma^{-1}]=\\nu_0S_0^{-1} \\\\\r\\Sigma \\sim Wis^{-1}(\\nu_0, S_0^{-1}) \\rightarrow E[\\Sigma]=\\frac{1}{\\nu_0-p-1}S_0$$\ncovariance matrix semiconjugate prior 모수 설정 방법 1-1. If belief that $\\Sigma = \\Sigma_0$ is strong, $\\nu_0$ \\uparrow 1-2. If belief that $\\Sigma = \\Sigma_0$ is weak, $\\nu_0 = p+2$\n2. Set $S_0=(\\nu_0-p-1)\\Sigma_0$\n3-4. Full conditional distribution of Covariance Matrix Prior: $\\Sigma \\sim \\text{inv-}Wis(\\nu_0, S_0^{-1})$\n$$p(\\Sigma) = \\bigg[2^{\\nu_0p/2}\\pi^{p/2}|S_0|^{\\nu_0/2}\\prod_{j=1}^{p}\\Gamma(\\frac{\\nu_0+1-j}{2})\\bigg]^{-1} \\times |\\Sigma|^{-(\\nu_0+p+1)/2} \\times exp\\Big(-\\frac{1}{2}tr(S_0\\Sigma^{-1})\\Big) $$\nLikelihood: $\\boldsymbol{Y}|\\boldsymbol{\\mu} \\stackrel{iid}\\sim MVN(\\boldsymbol{\\mu}, \\Sigma)$\n\\begin{align}\rp(\\boldsymbol{y_1, ..., y_n}|\\boldsymbol{\\mu}, \\Sigma) \u0026amp;= (2\\pi)^{-np/2}|\\Sigma|^{-n/2} exp\\bigg(-\\frac{1}{2}\\sum_{i=1}^{n} \\boldsymbol{(y_i-\\mu)}^T\\Sigma^{-1}\\boldsymbol{(y_i-\\mu)} \\bigg) \\\\\r\u0026amp;\\propto |\\Sigma|^{-n/2}exp\\bigg(-\\frac{1}{2}tr(S_\\mu\\Sigma^{-1})\\bigg)\r\\end{align}\nwhere $S_\\mu = \\sum_{i=1}^{n}\\boldsymbol{(y_i-\\mu)}\\boldsymbol{(y_i-\\mu)}^T$\nPosterior: $\\Sigma|\\boldsymbol{y} \\sim \\text{inv-}Wis(\\nu_0+n, [S_0+S_\\mu]^{-1})$\n\\begin{align}\rp(\\Sigma|\\boldsymbol{y_1, ..., y_n, \\mu}) \u0026amp;\\propto p(\\Sigma)p(\\boldsymbol{y_1, ..., y_n}|\\boldsymbol{\\mu},\\Sigma) \\\\\r\u0026amp;\\propto|\\Sigma|^{-(\\nu_0+p+1)/2} \\times exp\\Big(-\\frac{1}{2}tr(S_0\\Sigma^{-1})\\Big) \\times |\\Sigma|^{-n/2}exp\\bigg(-\\frac{1}{2}tr(S_\\mu\\Sigma^{-1})\\bigg) \\\\\r\u0026amp;\\propto |\\Sigma|^{-(\\nu_0+p+n+1)/2}exp\\bigg(-\\frac{1}{2}tr([S_0+S_\\mu]\\Sigma^{-1})\\bigg)\r\\end{align}\n\\begin{align}\rE[\\Sigma|\\boldsymbol{y_1, ..., y_n, \\mu}] \u0026amp;= \\frac{1}{\\nu_0+n-p-1}(S_0+S_\\mu) \\\\\r\u0026amp;= \\frac{\\nu_0-p-1}{\\nu_0+n-p-1}\\cdot\\frac{1}{\\nu_0-p-1}S_0 + \\frac{n}{\\nu_0+n-p-1}\\cdot\\frac{1}{n}S_\\mu \\\\\r\u0026amp;= \\frac{\\nu_0-p-1}{\\nu_0+n-p-1}\\cdot\\Sigma_0 + \\frac{n}{\\nu_0+n-p-1}\\cdot\\frac{1}{n}S_\\mu\r\\end{align}\nSummary  Semiconjugate prior for $\\mu$\nPrior: $\\boldsymbol{\\mu} \\text{ ~ } MVN(\\boldsymbol{\\mu_0}, \\Lambda_0)$\nLikelihood: $Y_1, ..., Y_n|\\boldsymbol{\\mu},\\Sigma \\text{ ~ iid } MVN(\\boldsymbol{\\mu}, \\Sigma)$\nPosterior: $\\boldsymbol{\\mu}|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}, \\Sigma \\text{ ~ } MVN(\\boldsymbol{\\mu_n}, \\Lambda_n)$  \\begin{align}\r\\Lambda_n^{-1} \u0026amp;= \\Lambda_0^{-1}+n\\Sigma^{-1} \\\\\r\\boldsymbol{\\mu_n} \u0026amp;= (\\Lambda_0^{-1}+n\\Sigma^{-1})^{-1}(\\Lambda_0^{-1}\\boldsymbol{\\mu_0}+n\\Sigma^{-1}\\boldsymbol{\\bar{y}})\r\\end{align}\nSemiconjugate prior for $\\Sigma$\nPrior: $\\Sigma \\sim \\text{inv-}Wis(\\nu_0, S_0^{-1}) \\text{ where } S_0 = (\\nu_0-p-1)\\Sigma_0$\nLikelihood: $\\boldsymbol{Y}|\\boldsymbol{\\mu} \\stackrel{iid}\\sim MVN(\\boldsymbol{\\mu}, \\Sigma)$\nPosterior: $\\Sigma|\\boldsymbol{y_1, ..., y_n} \\sim \\text{inv-}Wis(\\nu_0+n, [S_0+S_\\mu]^{-1})$  $$E[\\Sigma|\\boldsymbol{y_1, ..., y_n, \\mu}] = \\frac{\\nu_0-p-1}{\\nu_0+n-p-1}\\cdot\\Sigma_0 + \\frac{n}{\\nu_0+n-p-1}\\cdot\\frac{1}{n}S_\\mu$$\n  그래프 그리기  4-2. Draw yourself Figure 7.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  # Load Data test \u0026lt;- matrix(c(59, 43, 34, 32, 42, 38, 55, 67, 64, 45, 49, 72, 34, 70, 34, 50, 41, 52, 60, 34, 28, 35, 77, 39, 46, 26, 38, 43, 68, 86, 77, 60, 50, 59, 38, 48, 55, 58, 54, 60, 75, 47, 48, 33), ncol=2, byrow=FALSE) colnames(test) \u0026lt;- c(\u0026#39;pretest\u0026#39;,\u0026#39;posttest\u0026#39;) # Preparing n \u0026lt;- nrow(test) ybar \u0026lt;- colMeans(test) Sigma \u0026lt;- cov(test) THETA \u0026lt;- NULL SIGMA \u0026lt;- NULL inv \u0026lt;- solve sample.size = 5000 sample.new = NULL # prior mu0 \u0026lt;- c(50,50); nu0 \u0026lt;- 4 #(nu0 = p+2 = 4)  S0 \u0026lt;- L0 \u0026lt;- matrix(c(625,312.5,312.5,625), nrow=2, ncol=2) set.seed(2021) for(i in 1:sample.size){ # update theta Ln = inv(inv(L0) + n*inv(Sigma)) mun = Ln %*% (inv(L0)%*%mu0 + n*inv(Sigma)%*%ybar) theta = mvrnorm(1, mun, Ln) # update sigma Sn = S0 + (t(test)-theta)%*%t(t(test)-theta) Sigma = inv(rWishart(1, nu0+n, inv(Sn))[,,1]) # Save results THETA \u0026lt;- rbind(THETA, theta) SIGMA \u0026lt;- rbind(SIGMA, c(Sigma)) # sample new sample.new = rbind(sample.new, mvrnorm(n=1, mu=theta, Sigma=Sigma)) } rownames(THETA) \u0026lt;- 1:sample.size rownames(SIGMA) \u0026lt;- 1:sample.size   1 2 3 4 5 6 7 8 9 10  # graph(코드 따라하기) par(mfrow=c(1,2),mgp=c(1.75,.75,0),mar=c(3,3,1,1)) plot.hdr2d(THETA,xlab=expression(theta[1]),ylab=expression(theta[2]) ) abline(0,1) plot.hdr2d(sample.new,xlab=expression(italic(y[1])),ylab=expression(italic(y[2])), xlim=c(0,100),ylim=c(0,100) ) points(test[,1],test[,2],pch=16,cex=.7) abline(0,1)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # graph(ggplot 활용) p1 \u0026lt;- data.frame(THETA) %\u0026gt;% ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color=\u0026#39;orange\u0026#39;) + geom_abline(slope=1, intercept=0) + xlab(expression(theta[1])) + ylab(expression(theta[2])) + ggtitle(\u0026#39;Posterior draws of Mu\u0026#39;) p2 \u0026lt;- data.frame(sample.new) %\u0026gt;% ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color=\u0026#39;orange\u0026#39;) + geom_abline(slope=1, intercept=0) + xlab(expression(y[1])) + ylab(expression(y[2])) + ggtitle(\u0026#39;Posterior Predictive\u0026#39;) grid.arrange(p1, p2, nrow=1)     4. Gibbs Sampling of the mean and covariance Gibbs sampling은 full conditional distribution을 통해 차례대로 모수를 업데이트하면서 joint posterior distribution을 구하는 것이 목적이다.\nSTEP1. Full conditional distribution을 확보한다.\n $\\boldsymbol{\\mu}|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}, \\Sigma \\sim MVN(\\boldsymbol{\\mu_n}, \\Lambda_n)$ $\\Sigma|\\boldsymbol{y_1, ..., y_n}, \\boldsymbol{\\mu} \\sim \\text{inv-}Wis(\\nu_0+n, [S_0+S_\\mu]^{-1})$  STEP2. 차례대로 업데이트하면서 joint posterior distribution $\\boldsymbol{\\mu},\\Sigma|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}$을 구한다.\n4-1. NA imputation MAR(Missing at Random)인 경우에, missing data를 일종의 모수로 보고 gibbs sampling을 통해 na imputation을 해줄 수 있다.\nConclusion MVN도 잘 알아두자. inv-Wishart 분포도! 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":66,"section":"posts","tags":null,"title":"MVN","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb07/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Pymysql 패키지 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  import sys import requests import base64 import json import logging import pymysql #New library client_id = \u0026#39;\u0026#39; #직접 입력 client_secret = \u0026#39;\u0026#39; # 직접 입력 host = \u0026#39;\u0026#39; #host port = 3306 username = \u0026#39;\u0026#39; #user database = \u0026#39;\u0026#39; #db password = \u0026#39;\u0026#39; #passwd def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) cursor.execute(\u0026#39;SHOW TABLES\u0026#39;) print(cursor.fetchall()) print(\u0026#39;success\u0026#39;) sys.exit(0) if __name__ == \u0026#39;__main__\u0026#39;: main()   2. INSERT, UPDATE, REPLACE, INSERT IGNORE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  CREATE TABLE artists (id VARCHAR(255), name VARCHAR(255), followers INT, popularity INT, url VARCHAR(255), image_url VARCHAR(255), PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET=\u0026#39;utf8\u0026#39;; CREATE TABLE artist_genres (artist_id VARCHAR(255), genre VARCHAR(255)) ENGINE=InnoDB DEFAULT CHARSET=\u0026#39;utf8\u0026#39;; SHOW CREATE TABLE artists; -- INSERT INSERT INTO artist_genres (artist_id, genre) VALUES (\u0026#39;1234\u0026#39;, \u0026#39;pop\u0026#39;); DELETE FROM artist_genres; --제거 DROP TABLE artist_genres; --제거 CREATE TABLE artist_genres (artist_id VARCHAR(255), genre VARCHAR(255), UNIQUE KEY(artist_id, genre)) ENGINE=InnoDB DEFAULT CHARSET=\u0026#39;utf8\u0026#39;; --unique key 생성 INSERT INTO artist_genres (artist_id, genre) VALUES (\u0026#39;1234\u0026#39;, \u0026#39;pop\u0026#39;); INSERT INTO artist_genres (artist_id, genre) VALUES (\u0026#39;1234\u0026#39;, \u0026#39;pop\u0026#39;); --두 번 하면 ERROR  -- UPDATE UPDATE artist_genres SET genre=\u0026#39;pop\u0026#39; WHERE artist_id =\u0026#39;1234\u0026#39;; ALTER TABLE artist_genres ADD COLUMN country VARCHAR(255); ALTER TABLE artist_genres ADD COLUMN updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP; --업데이트되는 시각대 자동추가 INSERT INTO artist_genres (artist_id, genre, country) VALUES (\u0026#39;1234\u0026#39;,\u0026#39;pop\u0026#39;,\u0026#39;UK\u0026#39;); -- 오류 발생함  -- REPLACE REPLACE INTO artist_genres (artist_id, genre, country) VALUES (\u0026#39;1234\u0026#39;,\u0026#39;pop\u0026#39;,\u0026#39;UK\u0026#39;); /* 문제점(1) 지우고 업데이트하기 때문에 2번의 과정을 거쳐서 퍼포먼스적으로 문제 생길 수가 있다. 문제점(2) primary key가 auto_increment인 경우 새로운 숫자로 바뀌게 된다. */ -- INSERT IGNORE INSERT IGNORE INTO artist_genres (artist_id, genre, country) VALUES (\u0026#39;1234\u0026#39;,\u0026#39;rock\u0026#39;,\u0026#39;UK\u0026#39;); /* 문제점(1) 이미 값이 있으면 추가하지 않게 된다.*/ -- INSERT ... ON DUPLICATE KEY UPDATE INSERT INTO artist_genres (artist_id, genre, country) VALUES (\u0026#39;1234\u0026#39;,\u0026#39;rock\u0026#39;,\u0026#39;UK\u0026#39;) ON DUPLICATE KEY UPDATE artist_id=\u0026#39;1234\u0026#39;, genre=\u0026#39;rock\u0026#39;, country=\u0026#39;FR\u0026#39;; --UK를 FR로 바꾼다.  -- ETC ALTER TABLE artist_genres DROP COLUMN country; --불필요한 칼럼 지우기   *MySQL에서 진행하였다.\n특이사항  data type을 INTEGER로 하니까 안 되고, INT로 하니까 됐다.  3. _, .format() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) cursor.execute(\u0026#39;SHOW TABLES\u0026#39;) print(cursor.fetchall()) query = \u0026#34;INSERT INTO artist_genres (artist_id, genre) VALUES (\u0026#39;{0}\u0026#39;, \u0026#39;{1}\u0026#39;)\u0026#34;.format(\u0026#39;2345\u0026#39;,\u0026#39;hip-hop\u0026#39;) cursor.execute(query) conn.commit() sys.exit(0) if __name__ == \u0026#39;__main__\u0026#39;: main()   4. Dictionary와 JSON Package 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) headers = get_headers(client_id, client_secret) ## Spotify Search api params = { \u0026#39;q\u0026#39;: \u0026#39;BTS\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;artist\u0026#39;, \u0026#39;limit\u0026#39;: \u0026#39;5\u0026#39; } r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) raw = json.loads(r.text) print(raw[\u0026#39;artists\u0026#39;].keys) def get_headers(client_id, client_secret): endpoint = \u0026#39;https://accounts.spotify.com/api/token\u0026#39; encoded = base64.b64encode(\u0026#34;{}:{}\u0026#34;.format(client_id, client_secret).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;ascii\u0026#39;) headers = {\u0026#39;Authorization\u0026#39;: \u0026#39;Basic {}\u0026#39;.format(encoded)} payload = {\u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39;} r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)[\u0026#39;access_token\u0026#39;] headers = {\u0026#39;Authorization\u0026#39;: \u0026#34;Bearer {}\u0026#34;.format(access_token)} return headers if __name__ == \u0026#39;__main__\u0026#39;: main()   5. Duplicate Record 핸들링 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) headers = get_headers(client_id, client_secret) ## Spotify Search api params = { \u0026#39;q\u0026#39;: \u0026#39;BTS\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;artist\u0026#39;, \u0026#39;limit\u0026#39;: \u0026#39;1\u0026#39; } r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) raw = json.loads(r.text) artist_raw = raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0] if artist_raw[\u0026#39;name\u0026#39;] == params[\u0026#39;q\u0026#39;]: artist = { \u0026#39;id\u0026#39;: artist_raw[\u0026#39;id\u0026#39;], \u0026#39;name\u0026#39;: artist_raw[\u0026#39;name\u0026#39;], \u0026#39;followers\u0026#39;: artist_raw[\u0026#39;followers\u0026#39;][\u0026#39;total\u0026#39;], \u0026#39;popularity\u0026#39;: artist_raw[\u0026#39;popularity\u0026#39;], \u0026#39;url\u0026#39;: artist_raw[\u0026#39;external_urls\u0026#39;][\u0026#39;spotify\u0026#39;], \u0026#39;image_url\u0026#39;: artist_raw[\u0026#39;images\u0026#39;][0][\u0026#39;url\u0026#39;] } query = \u0026#34;\u0026#34;\u0026#34; INSERT INTO artists (id, name, followers, popularity, url, image_url) VALUES (\u0026#39;{}\u0026#39;, \u0026#39;{}\u0026#39;, {}, {}, \u0026#39;{}\u0026#39;, \u0026#39;{}\u0026#39;) ON DUPLICATE KEY UPDATE id=\u0026#39;{}\u0026#39;, name=\u0026#39;{}\u0026#39;, followers={}, popularity={}, url=\u0026#39;{}\u0026#39;, image_url=\u0026#39;{}\u0026#39; \u0026#34;\u0026#34;\u0026#34;.format( artist[\u0026#39;id\u0026#39;], artist[\u0026#39;name\u0026#39;], artist[\u0026#39;followers\u0026#39;], artist[\u0026#39;popularity\u0026#39;], artist[\u0026#39;url\u0026#39;], artist[\u0026#39;image_url\u0026#39;], artist[\u0026#39;id\u0026#39;], artist[\u0026#39;name\u0026#39;], artist[\u0026#39;followers\u0026#39;], artist[\u0026#39;popularity\u0026#39;], artist[\u0026#39;url\u0026#39;], artist[\u0026#39;image_url\u0026#39;] ) cursor.execute(query) conn.commit()   6. Duplicate Record 핸들링을 위한 파이썬 함수 5와 다른 점을 눈여겨 보기 (5를 보다 간단하게 한 코드) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  def main(): ## .... 여기까지는 위와 동일 r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) raw = json.loads(r.text) artist = {} artist_raw = raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0] if artist_raw[\u0026#39;name\u0026#39;] == params[\u0026#39;q\u0026#39;]: artist.update({ \u0026#39;id\u0026#39;: artist_raw[\u0026#39;id\u0026#39;], \u0026#39;name\u0026#39;: artist_raw[\u0026#39;name\u0026#39;], \u0026#39;followers\u0026#39;: artist_raw[\u0026#39;followers\u0026#39;][\u0026#39;total\u0026#39;], \u0026#39;popularity\u0026#39;: artist_raw[\u0026#39;popularity\u0026#39;], \u0026#39;url\u0026#39;: artist_raw[\u0026#39;external_urls\u0026#39;][\u0026#39;spotify\u0026#39;], \u0026#39;image_url\u0026#39;: artist_raw[\u0026#39;images\u0026#39;][0][\u0026#39;url\u0026#39;] }) insert_row(cursor, data=artist, table=\u0026#39;artists\u0026#39;) conn.commit() def insert_row(cursor, data, table): placeholders = \u0026#39;, \u0026#39;.join([\u0026#39;%s\u0026#39;] * len(data)) columns = \u0026#39;, \u0026#39;.join(data.keys()) key_placeholders = \u0026#39;, \u0026#39;.join([\u0026#39;{0}=%s\u0026#39;.format(k) for k in data.keys()]) sql = \u0026#39;INSERT INTO %s( %s) VALUES ( %s) ON DUPLICATE KEY UPDATE %s\u0026#39; % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)   7. Artist list 추출하기  패스트캠퍼스 강좌를 통해 제공된 artist_list.csv 파일을 활용하였다.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) headers = get_headers(client_id, client_secret) artists = [] with open(\u0026#39;../artist_list.csv\u0026#39;, encoding=\u0026#34;utf-8\u0026#34;) as f: raw = csv.reader(f) for row in raw: artists.append(row[0]) for a in artists: params = { \u0026#39;q\u0026#39;: a, \u0026#39;type\u0026#39;: \u0026#39;artist\u0026#39;, \u0026#39;limit\u0026#39;: \u0026#39;1\u0026#39; } r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) raw = json.loads(r.text) artist = {} try: if raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;name\u0026#39;] == params[\u0026#39;q\u0026#39;]: artist.update( { \u0026#39;id\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;id\u0026#39;], \u0026#39;name\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;name\u0026#39;], \u0026#39;followers\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;followers\u0026#39;][\u0026#39;total\u0026#39;], \u0026#39;popularity\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;popularity\u0026#39;], \u0026#39;url\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;external_urls\u0026#39;][\u0026#39;spotify\u0026#39;], \u0026#39;image_url\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;images\u0026#39;][0][\u0026#39;url\u0026#39;] } ) insert_row(cursor, artist, \u0026#39;artists\u0026#39;) except: logging.error(\u0026#39;NO ITEMS FROM SEARCH API\u0026#39;) continue conn.commit() # sys.exit(0)   ERROR:root:NO ITEMS FROM SEARCH API와 같은 에러가 여러 개가 나오게 된다.\n말그대로 SERACH API를 통해서 ITEMS를 찾지 못하게 된 경우에 해당한다.\n8. Batch 형식으로 데이터 요청  한번에 묶어서 API에 전달하는 방식이다. 모든 API가 제공하는 것은 아니긴 하다! Spotify는 \u0026lsquo;Get Several Artists\u0026rsquo;하는 법을 제공하고 있다.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) headers = get_headers(client_id, client_secret) cursor.execute(\u0026#34;SELECT id FROM artists\u0026#34;) artists = [] for (id, ) in cursor.fetchall(): artists.append(id) artist_batch = [artists[i: i+50] for i in range(0, len(artists), 50)] for i in artist_batch: ids = \u0026#39;,\u0026#39;.join(i) URL = \u0026#39;https://api.spotify.com/v1/artists/?ids={}\u0026#39;.format(ids) r= requests.get(URL, headers=headers) raw = json.loads(r.text) print(raw) print(len(raw[\u0026#39;artists\u0026#39;])) sys.exit(0)   1 2  -- 제대로 잘 들어갔는지 확인해보기 select * from artist_genres limit 10;   9. MySQL 테이블들로 데이터 저장 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  def main(): ## ... 여기까지는 위와 똑같음 artist_batch = [artists[i: i+50] for i in range(0, len(artists), 50)] artist_genres = [] for i in artist_batch: ids = \u0026#39;,\u0026#39;.join(i) URL = \u0026#39;https://api.spotify.com/v1/artists/?ids={}\u0026#39;.format(ids) r= requests.get(URL, headers=headers) raw = json.loads(r.text) for artist in raw[\u0026#39;artists\u0026#39;]: for genre in artist[\u0026#39;genres\u0026#39;]: artist_genres.append( { \u0026#39;artist_id\u0026#39;: artist[\u0026#39;id\u0026#39;], \u0026#39;genre\u0026#39;: genre } ) for data in artist_genres: insert_row(cursor, data, \u0026#39;artist_genres\u0026#39;) conn.commit() sys.exit(0) ## ... 아래도 다 똑같음   \n","description":"데이터의 이해와 데이터베이스","id":67,"section":"posts","tags":null,"title":"Python \u0026 MySQL","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_3/"},{"content":"Chapter 08. Group Comparisons and Hierarchical Modeling 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\nHierarchical Model은 그룹 간 그리고 그룹 내 variability를 설정하는 데에 유용하다.\n Hierarchical Model describes both with-in group and between-group variability.\n Hierarchical Model은 베이즈 통계가 다른 응용분야에 널리 퍼져 사용되는 결정적 계기가 되었다. 그리고 위의 그림을 통해서 알 수 있듯이, 층이 여러 개 이기 때문에 복잡한 상황에서도 Estimation을 할 수 있다는 장점이 있다.\n가장 큰 특징을 간단하게 요약해보자면, 여러 그룹끼리 서로 정보를 주고받는다는 점이다.\n1. Exchangeability \u0026amp; De Finetti\u0026rsquo;s Theorem 이부분에 대해서는 Chapter2에서 이미 다룬 적은 있다. 그래도 이번 챕터의 논리전개에 대해서 정당성을 부여해주기 때문에 한 번 더 복습해보도록 하겠다. 우선 Exchangeability는 다음과 같이 쓸 수 있다.\n$$p(y_1, ..., y_n) = p(y_{\\pi_{1}}, ..., y_{\\pi_{n}})$$\n그리고 De Finetti\u0026rsquo;s Theorem은 exchangeability가 만족되면, Conditional Independence를 따른다는 것이었다. (참고로, 그 역은 자명하다.) 이때 조건으로는 small sample from large population이라고 한다. (자세한 것은 FCB를 읽어보자.)\n2. Hierarchical Beta-Binomial 2-1. Posterior 종류 정리 Joint Posterior $$\\begin{align}\rp(\\theta,\\psi |y) \u0026amp;\\propto p(y|\\theta, \\psi) \\ p(\\theta, \\psi) \\\\\r\u0026amp;= p(y|\\theta) \\ p(\\theta, \\psi) \\\\ \u0026amp;= p(y|\\theta) \\ p(\\theta| \\psi) \\ p(\\psi)\r\\end{align}$$\nConditional Posterior $$\\begin{align}\rp(\\theta | \\psi, y) \u0026amp;\\propto p(y, \\psi|\\theta) \\ p(\\theta) \\\\\r\u0026amp;= p(y|\\theta,\\psi) \\ p(\\psi|\\theta) \\ p(\\theta) \\\\ \u0026amp;= p(y|\\theta) \\ p(\\theta, \\psi) \\\\\r\u0026amp;= p(y|\\theta) \\ p(\\theta| \\psi) \\ p(\\psi)\r\\end{align}$$\nJoint Posterior와 Conditional Posterior 각각의 마지막끼리 같다는 사실에 주목해보자.\nMarginal Posterior $$p(\\psi|y) = \\int_\\Theta p(\\theta, \\psi | y)d\\theta \\\\\rp(\\psi|y) = \\frac{p(\\theta,\\psi|y)}{p(\\theta|\\psi, y)}$$\n둘 중 편한 것으로 계산한다.\n2-2. 간단 정리 $$p(\\theta, \\alpha, \\beta|y) \\propto p(\\alpha,\\beta) \\ \\prod_{j=1}^{m}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta_j^{\\alpha-1}(1-\\theta_j)^{\\beta-1} \\ \\prod_{j=1}^{m}\\theta_j^{y_j}(1-\\theta_j)^{n_j-y_j}$$\n3층: hyperprior $\\psi$\n2층: $\\theta_j|\\psi \\sim Beta(\\alpha,\\beta)$\n1층: $y_j|\\theta_j \\sim Binom(n_j, \\theta_j)$\n2-3. hyperprior는 어떻게 주어야 할까? 결론부터 말하자면,\n$$p(\\alpha, \\beta) \\propto (\\alpha+\\beta)^{-\\frac{5}{2}}$$\n이렇게 준다고 한다. 해당 수식에 대한 그래프는 아래와 같은데, 시각적으로 uninformative에 가깝다는 것을 알 수 있다.\n이는 원래는 아래와 같은 형태였다.\n$$p\\Big(log\\frac{\\alpha}{\\beta}, (\\alpha+\\beta)^{-\\frac{1}{2}}\\Big) \\propto 1$$\n변수변환을 통해 유도하는 과정은 변수변환 포스팅에 자세하게 설명되어 있다. 그래서 결론적으로 아래와 같이 쓸 수 있다.\n$$\\rightarrow p(\\theta, \\alpha, \\beta|y) \\propto (\\alpha+\\beta)^{-\\frac{5}{2}} \\ \\prod_{j=1}^{m}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta_j^{\\alpha-1}(1-\\theta_j)^{\\beta-1} \\ \\prod_{j=1}^{m}\\theta_j^{y_j}(1-\\theta_j)^{n_j-y_j}$$\n2-4. 예시 FCB 책에 나온 종양 쥐(Rat Tumor) 예시를 살펴보자.\n1) 95% Posterior interval 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  ### Raw data num_diseased \u0026lt;- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2, 2,1,5,2,5,3,2,7,7,3,3,2,9,10,4,4,4,4,4,4,4,10,4,4,4,5,11,12, 5,5,6,5,6,6,6,6,16,15,15,9,4) num_of_obs \u0026lt;- c(20,20,20,20,20,20,20,19,19,19,19,18,18,17,20,20,20,20,19,19,18,18,25,24, 23,20,20,20,20,20,20,10,49,19,46,27,17,49,47,20,20,13,48,50,20,20,20,20, 20,20,20,48,19,19,19,22,46,49,20,20,23,19,22,20,20,20,52,46,47,24,14) ### Data frame DF \u0026lt;- data.frame(cbind(num_diseased,num_of_obs)) colnames(DF) \u0026lt;- c(\u0026#34;y\u0026#34;,\u0026#34;n\u0026#34;) DF$ybar = DF$y/DF$n ### theta\u0026#39;s uniform prior: beta(1,1) DF$postmean = (DF$y + 1) / (DF$n + 1) DF$lb = mapply(function(y, n) qbeta(0.025, y + 1, n - y + 1), DF$y, DF$n) DF$ub = mapply(function(y, n) qbeta(1 - 0.025, y + 1, n - y + 1), DF$y, DF$n) title1 = expression(paste(\u0026#34;95% Posterior interval, \u0026#34;, beta(1, 1), \u0026#34; prior\u0026#34;)) p1 = ggplot(DF, aes(x = ybar, ymin = lb, ymax = ub)) + geom_linerange() + geom_point(aes(y = postmean)) + geom_abline(slope = 1, intercept = 0) + labs(title = title1, x = \u0026#34;observed mean\u0026#34;, y = \u0026#34;posterior mean\u0026#34;) p1   2) Marginal Distribution of alpha \u0026amp; beta 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  ### Hierarchical model using theta\u0026#39;s prior: beta(alpha,beta) griddens = 1e2 A = seq(0.5, 6, length.out = griddens) B = seq(3, 33, length.out = griddens) cA = rep(A, each = length(B)) cB = rep(B, length(A)) lpfun = function(a, b, y, n){ # marginal posterior (-5/2)* log(a+b) + sum(lgamma(a+b) - lgamma(a) - lgamma(b) + lgamma(a+y) + lgamma(b+n-y) - lgamma(a+b+n))} lp = mapply(lpfun, cA, cB, MoreArgs = list(DF$y, DF$n)) df_marg = data.frame(alpha= cA, beta= cB, posterior = exp(lp)/sum(exp(lp))) # posterior prob.합이 1이 되도록 조정 title2 = TeX(\u0026#39;The marginal of $\\\\alpha$ and $\\\\beta$\u0026#39;) p2 = ggplot(data = df_marg, aes(x=alpha, y=beta))+ geom_raster(aes(fill = posterior, alpha= posterior), interpolate= T)+ geom_contour(aes(z= posterior), color = \u0026#39;black\u0026#39;, size= 0.2)+ coord_cartesian(xlim= c(1,5), ylim= c(4,26))+ labs(x = TeX(\u0026#39;$\\\\alpha$\u0026#39;), y= TeX(\u0026#39;$\\\\beta$\u0026#39;), title= title2)+ scale_fill_gradient(low= \u0026#34;cornflowerblue\u0026#34;, high= \u0026#34;navy\u0026#34;, guide= F)+ scale_alpha(range= c(0,1), guide=F) p2   1 2 3 4 5 6 7 8  title_alpha \u0026lt;- TeX(\u0026#39;Marginal Posterior of $\\\\alpha$|y\u0026#39;) title_beta \u0026lt;- TeX(\u0026#39;Marginal Posterior of $\\\\beta$|y\u0026#39;) marg_alpha \u0026lt;- df_marg %\u0026gt;% group_by(alpha) %\u0026gt;% summarise(marg = sum(posterior)) %\u0026gt;% ggplot(aes(x=alpha,y=marg)) + geom_line(size=2, color=\u0026#39;cornflowerblue\u0026#39;) + labs(x=TeX(\u0026#39;$\\\\alpha$\u0026#39;), y= TeX(\u0026#39;p($\\\\alpha$|y)\u0026#39;), title=title_alpha)   ## `summarise()` ungrouping output (override with `.groups` argument)\r1 2 3 4 5  marg_beta \u0026lt;- df_marg %\u0026gt;% group_by(beta) %\u0026gt;% summarise(marg = sum(posterior)) %\u0026gt;% ggplot(aes(x=beta,y=marg)) + geom_line(size=2, color=\u0026#39;darkgreen\u0026#39;) + labs(x=TeX(\u0026#39;$\\\\beta$\u0026#39;), y= TeX(\u0026#39;p($\\\\beta$|y)\u0026#39;), title=title_beta)   ## `summarise()` ungrouping output (override with `.groups` argument)\r1  grid.arrange(marg_alpha, marg_beta, ncol=2)   3) 95% posterior interval, uninformative prior($\\alpha, \\beta$) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # Grid sampling to generate posterior samples of alpha, beta # (re)settings DF \u0026lt;- data.frame(cbind(num_diseased,num_of_obs)) colnames(DF) \u0026lt;- c(\u0026#34;y\u0026#34;,\u0026#34;n\u0026#34;) DF$ybar = DF$y/DF$n DF$postmean = DF$ub = DF$lb = rep(NA, nrow(DF)) Nsamp = 1e3 set.seed(101) # sample alpha, beta samp_idx = sample(length(df_marg$posterior), size = Nsamp, replace=T, prob = df_marg$posterior) samp_A = cA[samp_idx] samp_B = cB[samp_idx] # sample theta_j for each j for(i in 1:nrow(DF)){ n = DF$n[i]; y = DF$y[i] theta_j = mapply(function(a, b, n, y) rbeta(1, y+a, n-y+b), samp_A, samp_B, MoreArgs = list(n=n, y=y)) DF$lb[i] = quantile(theta_j, 0.025) DF$ub[i] = quantile(theta_j, 1-0.025) DF$postmean[i] = mean(theta_j) } # 95% posterior interval title3 = TeX(\u0026#39;95% posterior interval, uninformative $\\\\alpha$, $\\\\beta$ prior\u0026#39;) p3 = ggplot(DF, aes(x=ybar, ymin=lb, ymax=ub))+ geom_linerange()+geom_point(aes(y=postmean))+geom_abline(slope = 1, intercept = 0)+ labs(title=title3, x=\u0026#34;observed mean\u0026#34;, y=\u0026#34;posterior mean\u0026#34;) p3   4) 1과 3 비교해보기 3. Hierarchical Normal Conclusion Prior의 Prior, HyperPrior를 활용하여 보다 설득력 있는 모델을 만들어보자! Reference [1] FCB\n[2] ESC 2021-1 Spring 세션\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":68,"section":"posts","tags":null,"title":"Hierarchical Model","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb08/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Artist Data 1  SELECT genre, COUNT(*) FROM artist_genres GROUP BY 1 ORDER BY 2 DESC LIMIT 20;   2. Artist Genre Analysis with SQL 1 2 3  SELECT popularity, name FROM artists ORDER BY 1 DESC LIMIT 20; SELECT genre, COUNT(*) FROM artists t1 JOIN artist_genres t2 ON t2.artist_id = t1.id WHERE t1.popularity \u0026gt; 80 GROUP BY 1 ORDER BY 2 DESC LIMIT 20;    join을 통해 ERD의 장점을 활용하여 기초분석을 할 수 있다.  ","description":"데이터의 이해와 데이터베이스","id":69,"section":"posts","tags":null,"title":"SQL 활용 (MySQL)","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_4/"},{"content":"\r\rChapter 09. Linear Regression\r본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\n1. Linear Regression Model\rLinear Regression Model: a particular type of smoothly changing model for $p(y|\\boldsymbol{x})$ that specifies that the conditional expectation $E[Y|\\boldsymbol{x}]$ has a form that is linear in a set of parameters\n\\[\\int yp(y|\\boldsymbol{x})dy = E[Y|\\boldsymbol{x}] = \\beta_1x_1 + \\cdots + \\beta_px_p = \\boldsymbol{\\beta}^T\\boldsymbol{x}\\]\nNormal Linear Regression Model: 기존 Linear Regression Model에 추가해서, 아래의 조건이 추가된 것\r\\[\\epsilon_1, ..., \\epsilon_n \\stackrel{i.i.d}\\sim N(0,\\sigma^2) \\\\\rY_i = \\boldsymbol{\\beta}^T\\boldsymbol{x}_i + \\epsilon_i \\\\\r\\boldsymbol{y|X,\\beta},\\sigma^2 \\sim MVN(\\boldsymbol{X\\beta}, \\sigma^2\\boldsymbol{I})\\]\n1-1. $\\beta$ 추정(OLS)\r$$\\[\\begin{align} \\hat{\\beta} \u0026amp;\\sim N(\\beta, \\sigma^2(X^TX)^{-1}) \\\\\r\\\\\r\\arg \\min_\\beta \\sum e_i^2 \u0026amp;= \\arg \\min_\\beta(Y-X\\beta)^T(Y-X\\beta)\\\\\r\u0026amp;= \\frac{d}{d\\beta}(Y^TY -\\beta^TX^TY-Y^TX\\beta+\\beta^TX^TX\\beta) \\\\\r\u0026amp;= -2Y^TX+2\\beta^TX^TX \\\\\r\u0026amp;\\stackrel{let}= 0 \\\\\r\\rightarrow X^TX\\beta \u0026amp;= X^TY\\\\\r\\rightarrow \\hat{\\beta} \u0026amp;= (X^TX)^{-1}X^TY \\\\\r\\\\\rE(\\hat{\\beta}) \u0026amp;= E((X^TX)^{-1}X^TY) \\\\\r\u0026amp;= (X^TX)^{-1}X^TE(Y) \\\\\r\u0026amp;= (X^TX)^{-1}X^TX\\beta \\\\\r\u0026amp;= \\beta \\\\\rCov(\\hat{\\beta}) \u0026amp;= Cov((X^TX)^{-1}X^TY) \\\\\r\u0026amp;= (X^TX)^{-1}X^TCov(Y)\\Big((X^TX)^{-1}X^T\\Big)^T \\\\\r\u0026amp;= \\sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1} \\\\\r\u0026amp;= \\sigma^2 (X^TX)^{-1} \\end{align}\\]$$\n1-2. $\\sigma^2$ 추정(OLS)\r여기서 $\\sigma^2$는 아래의 과정을 통해 추정된다.\n\\[\\hat{\\sigma}^2_{ols} = \\frac{SSR}{n-p} = \\frac{\\sum (y_i-X_i\\hat{\\beta}_{ols})^2}{n-p} \\\\\r\\text{proof) }\\frac{SSR}{\\sigma^2} \\sim \\chi^2(n-p) \\\\\rE(SSR) = (n-p)\\sigma^2 \\\\\r\\therefore \\hat{\\sigma}^2_{ols} = \\frac{SSR}{n-p}\\]\n1-1)과 1-2)를 종합해서 해석해보자면, $\\hat{\\beta}_{ols}$의 분산은 $\\sigma^2 (X^TX)^{-1}$에다가 $\\sigma^2$ 대신에 $\\hat{\\sigma}^2_{ols}$을 대입한 것의 대각원소로 추정하면 된다. 그리고 이것을 루트 취해주면, 각 베타원소들의 standard error를 알 수 있게 된다. 이를 통해서 각 계수의 유의성을 생각해볼 수 있다.\n1-3. Conjugacy 복습\n1-3-1. Matching Trick\r\\[\\begin{align} p(y|\\mu,\\Sigma) \u0026amp;\\propto exp(-\\frac{1}{2}(y-\\mu)^T\\Sigma^{-1}(y-\\mu)) \\\\\r\u0026amp;\\propto exp(-\\frac{1}{2}y^T\\Sigma^{-1}y + y^T\\Sigma^{-1}\\mu) \\end{align}\\]\n1-3-2. Scaled Inverse Gamma distribution\r\\[\\text{X~} \\chi^{-2}(\\nu, \\tau^2) = \\Gamma^{-1}(\\nu/2, \\nu\\tau^2/2) \\\\\r\\rightarrow f(x) = \\frac{(\\nu\\tau^2/2)^{\\nu/2}}{\\Gamma(\\nu/2)}\\left(\\frac{1}{x}\\right)^{\\nu/2+1}exp(-\\frac{\\nu\\tau^2}{2x}) \\]\r$\\sigma^2 \\sim \\chi^2(\\nu_0, \\sigma^2_0)$으로 prior를 주게 된다면, $\\nu_o$는 prior size와 같은 의미를 갖고 $\\sigma^2_0$는 prior variance로서의 의미를 갖는다.\n\r2. Bayesian estimation for a regression model\r2-1. Semi-conjugate prior (independent prior)\rsemi-conjugate: prior와 conditional posterior가 같은 분포\n- Slopes $\\beta$\nlikelihood: $\\boldsymbol{y|X,\\beta},\\sigma^2 \\sim MVN(\\boldsymbol{X\\beta}, \\sigma^2\\boldsymbol{I})$\nprior: $\\beta \\sim N(\\beta_0, \\Sigma_0)$\nposterior: $\\beta|y,X,\\sigma^2 \\sim N(\\beta_n, \\Sigma_0)$\nwhere $\\beta_n = \\Sigma_n(\\Sigma_0^{-1}\\beta_0 + X^Ty/\\sigma^2), \\Sigma_n = (\\Sigma_0^{-1} + X^TX/\\sigma^2)^{-1}$\n(posterior 유도 증명)\r\\[\\begin{align}\rp(\\beta|y,\\sigma^2) \u0026amp;\\propto p(y|\\beta,\\sigma^2)p(\\beta) \\\\\r\u0026amp;\\propto exp(-\\frac{1}{2\\sigma^2}(y^Ty - 2\\beta^TX^Ty + \\beta^TX^TX\\beta) - \\frac{1}{2}(\\beta^T\\Sigma_0^{-1}\\beta - 2\\beta^T\\Sigma_0^{-1}\\beta_0)) \\\\\r\u0026amp;\\propto exp\\Big(-\\frac{1}{2}\\beta^T(X^TX/\\sigma^2 + \\Sigma_0^{-1})\\beta + \\beta^T(X^Ty/\\sigma^2 + \\Sigma_0^{-1}\\beta_0)\\Big) \\\\\r\\therefore \\Sigma_n^{-1} \u0026amp;= X^TX/\\sigma^2 + \\Sigma_0^{-1} \\\\\r\\beta_n \u0026amp;= \\Sigma_n(X^Ty/\\sigma^2 + \\Sigma_0^{-1}\\beta_0)\r\\end{align}\\]\n\n- Error Variance $\\sigma^2$\nlikelihood: $\\boldsymbol{y|X,\\beta},\\sigma^2 \\sim MVN(\\boldsymbol{X\\beta}, \\sigma^2\\boldsymbol{I})$\nprior: $\\sigma^2 \\sim \\Gamma^{-1}(\\frac{\\nu_0}{2}, \\frac{\\nu_0\\sigma^2_0}{2})$\nposterior: $\\sigma^2|y,X,\\beta \\sim \\Gamma^{-1}(\\frac{\\nu_n}{2}, \\frac{\\nu_n\\sigma^2_n}{2})$\nwhere $\\nu_n = \\nu_0+n, \\sigma^2_n = \\frac{1}{\\nu_n}(\\nu_0\\sigma^2_0 + SSR(\\beta))$\n(posterior 유도 증명)\r\\[\\begin{align}\rp(\\sigma^2|y, \\beta) \u0026amp;\\propto p(\\sigma^2)p(y|\\beta,\\sigma^2)\\\\\r\u0026amp;\\propto (\\frac{1}{\\sigma^2})^{\\nu_0/2+1}exp(-\\frac{\\nu_0\\sigma^2_0}{2\\sigma^2})(\\frac{1}{\\sigma^2})^{n/2}exp(-\\frac{SSR(\\beta)}{2\\sigma^2}) \\\\\r\u0026amp;= (\\frac{1}{\\sigma^2})^{(\\nu_0+n)/2+1}exp(-\\frac{\\nu_0\\sigma^2_0+SSR(\\beta)}{2\\sigma^2}) \\\\\r\\therefore \\nu_n \u0026amp;= \\nu_0+n \\\\\r\\sigma^2_n \u0026amp;= (\\nu_0\\sigma^2_0+SSR(\\beta))/\\nu_n\r\\end{align}\\]\n\r2-2. Full-conjugate prior (dependent prior)\r\\[\\begin{align}\rp(\\beta,\\sigma^2|y) \u0026amp;= p(\\beta|\\sigma^2,y) \\ p(\\sigma^2|y) \\\\\r\u0026amp;\\propto p(\\beta|\\sigma^2,y)p(\\sigma^2)p(y|\\sigma^2) \\\\\r\\end{align}\\]\n위에서 각각에 대한 분포는 아래와 같다.\r\\[\\begin{align}\r\\beta|y,\\sigma^2 \u0026amp;\\sim N(\\frac{g}{g+1}\\hat{\\beta}_{mle},\\frac{g}{g+1}Var(\\hat{\\beta}_{mle})) \\\\\r\\sigma^2 \u0026amp;\\sim \\chi^{-2}(\\nu_0, \\sigma^2_0) \\\\\rp(y|\\sigma^2) \u0026amp;= \\int p(y|\\beta,\\sigma^2)p(\\beta|\\sigma^2)d\\beta \\end{align}\\]\n여기서 integral 안에 있는 것을 자세히 살펴보면 아래와 같다.\r\\[\\begin{align}\rp(y|\\beta,\\sigma^2)p(\\beta|\\sigma^2) \u0026amp;= \\Big(\\frac{1}{2\\pi\\sigma^2}\\Big)^{n/2}exp(-\\frac{1}{2}(y-X\\beta)^T(y-X\\beta)) \\times \\Big(\\frac{1}{2\\pi}\\Big)^{p/2}|g\\sigma^2(X^TX)^{-1}|^{-\\frac{1}{2}}exp(-\\frac{1}{2g\\sigma^2}\\beta^TX^TX\\beta) \\\\ \u0026amp;\\because \\beta \\sim N(\\beta_0=0, \\Sigma_0 = g\\sigma^2(X^TX)^{-1}) \\\\\r\\end{align}\\]\n그러므로 다시 돌아와서 작성해보자면\r\\[\\begin{align}\rp(y|\\sigma^2) \u0026amp;= \\int p(y|\\beta,\\sigma^2)p(\\beta|\\sigma^2)d\\beta \\\\\r\u0026amp;\\propto \\Big(\\frac{1}{\\sigma^2}\\Big)^{n/2}|g\\sigma^2(X^TX)^{-1}|^{-\\frac{1}{2}}exp(-\\frac{1}{2\\sigma^2}y^Ty)\\int exp(\\frac{1}{\\sigma^2}\\beta^TX^Ty - \\frac{1}{2\\sigma^2}(1+\\frac{1}{g})\\beta^TX^TX\\beta)d\\beta\r\\end{align}\\]\n이중에서 맨 마지막에 위치한 $(1+\\frac{1}{g})\\beta^TX^TX\\beta$를 자세히 보자.\n여기서 $m, V$는 각각 $\\beta|y,\\sigma^2$(posterior)의 평균과 분산이라고 하자. 이를 활용하여 해당 부분을 조작해보면 아래와 같이 나온다.\n\\[\\begin{align}\r(1+\\frac{1}{g})\\beta^TX^TX\\beta \u0026amp;= \\frac{g+1}{g} \\times \\Bigg((\\beta-m)^TX^TX(\\beta-m) + 2m^TX^TX\\beta -m^TX^TXm \\Bigg) \\\\\r(1) \\frac{g+1}{g}(\\beta-m)^TX^TX(\\beta-m) \u0026amp;= \\sigma^2(\\beta-m)^TV^{-1}(\\beta-m)\\\\\r(2) \\frac{g+1}{g}2m^TX^TX\\beta \u0026amp;= 2\\beta^TX^Ty \\\\\r(3) \\frac{g+1}{g}m^TX^TXm \u0026amp;= \\sigma^2m^TV^{-1}m \\\\\r\\therefore (1+\\frac{1}{g})\\beta^TX^TX\\beta \u0026amp;= \\sigma^2(\\beta-m)^TV^{-1}(\\beta-m) + 2\\beta^TX^Ty - \\sigma^2m^TV^{-1}m\r\\end{align}\\]\n\\[\\begin{align}\rp(y|\\sigma^2) \u0026amp;= \\int p(y|\\beta,\\sigma^2)p(\\beta|\\sigma^2)d\\beta \\\\\r\u0026amp;\\propto \\Big(\\frac{1}{\\sigma^2}\\Big)^{n/2}|g\\sigma^2(X^TX)^{-1}|^{-\\frac{1}{2}}exp(-\\frac{1}{2\\sigma^2}y^Ty + \\frac{1}{2}m^TV^{-1}m)\\int exp\\Big(-\\frac{1}{2}(\\beta-m)^TV^{-1}(\\beta-m)\\Big)d\\beta \\\\\r\u0026amp;= \\Big(\\frac{1}{\\sigma^2}\\Big)^{n/2}|g\\sigma^2(X^TX)^{-1}|^{-\\frac{1}{2}} exp\\big(-\\frac{1}{2\\sigma^2}(y^Ty - \\sigma^2m^TV^{-1}m)\\big) \\ (2\\pi)^{n/2}|\\frac{g}{g+1}\\sigma^2(X^TX)^{-1}|^{\\frac{1}{2}} \\\\\r\u0026amp;\\propto \\Big(\\frac{1}{\\sigma^2}\\Big)^{n/2}(1+g)^{-p/2}exp\\big(-\\frac{1}{2\\sigma^2}SSR(g)\\big) \\\\\r\u0026amp;\\text{where } SSR(g) = y^Ty - \\sigma^2m^TV^{-1}m = y^T\\Big(I - \\frac{g}{g+1}X(X^TX)^{-1}X^T\\Big)y\r\\end{align}\\]\n여기서 위와 마찬가지로, g가 클수록 약한 prior를 뜻한다. 그렇기 때문에 g를 무한대로 보내게 되면, $SSR(g) \\rightarrow SSR(\\hat{\\beta}_{mle})$\n\\[\\begin{align}\rp(\\sigma^2|y) \u0026amp;\\propto p(\\sigma^2)p(y|\\sigma^2) \\\\\r\u0026amp;\\propto \\Big(\\frac{1}{\\sigma^2}\\Big)^{(\\nu_0+n)/2+1}exp(-\\frac{\\nu_0\\sigma^2_0 + SSR(g)}{2\\sigma^2}) \\\\\r\\therefore \\sigma^2|y \u0026amp;\\sim \\chi^{-2}(\\nu_0+n, \\frac{\\nu_0\\sigma^2_0 + SSR(g)}{\\nu_0+n})\r\\end{align}\\]\nFull-conjugate prior를 통해 구한 posterior를 정리하자면 아래와 같다.\r\\[\r\\sigma^2|y \\sim \\chi^{-2}(\\nu_0+n, \\frac{\\nu_0\\sigma^2_0 + SSR(g)}{\\nu_0+n}) \\\\\r\\beta|y,\\sigma^2 \\sim N(\\frac{g}{g+1}\\hat{\\beta}_{mle},\\frac{g}{g+1}Var(\\hat{\\beta}_{mle})) \\\\\r\\text{where } Var(\\hat{\\beta}_{mle}) = \\sigma^2(X^TX)^{-1}\r\\]\rGibbs sampling이 아닌, Monte Carlo Approximation을 통해서 효율적으로 joint posterior distribution을 구할 수 있다. 이 부분에서 semi-conjugate prior와 큰 차이가 있다고 할 수 있다.\n\r\r3. Model Selection\r  Code Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  ## Data load  data = dget(\u0026#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/yX.o2uptake\u0026#39;) y = data[,1] X = data[,-1] inv = solve ## set prior g = length(y) nu0 = 1 s20 = summary(lm(y~-1+X))$sigma^2 n = length(y) p = ncol(X) ## MCMC setup S = 1000 set.seed(2021) BETA = matrix(NA, nrow=S, ncol=p) sigma2 = matrix(NA, nrow=S, ncol=1) BETA[1,] = inv(t(X) %*% X) %*% t(X) %*% y sigma2[1,] = s20 ## gibbs sampling nun = nu0 + n betan = (g/(g+1)) * inv(t(X) %*% X) %*% t(X) %*% y for(s in 2:S){ s2n = nu0*s20 + t(y-X%*%BETA[s-1,]) %*% (y-X%*%BETA[s-1,]) sigma2[s,] = 1/rgamma(1, shape=nun/2, rate=s2n/2) Sigman = (g/(g+1)) * sigma2[s,] * inv(t(X) %*% X) BETA[s,] = MASS::mvrnorm(n=1, betan, Sigman) } ## graph colnames(BETA) = colnames(X) gather(as.data.frame(BETA)) %\u0026gt;% ggplot(aes(y=value, fill=key)) + geom_histogram() + coord_flip() + facet_wrap(~key, scales=\u0026#39;free_x\u0026#39;) + ggtitle(\u0026#39;Posterior samples of Beta\u0026#39;) + theme(legend.position = \u0026#39;None\u0026#39;)   ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r \r--\r\r\r","description":"","id":70,"section":"posts","tags":null,"title":"Bayesian Linear Regression","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb09/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n이 포스팅은 NoSQL 중 DynamoDB를 위주로 서술되어 있습니다.\n1. NoSQL vs. RDB  Not Only SQL 차이점(1) 다이나믹 스키마  구조를 정의하지 않고도 Documents, Key Values 등을 생성 각각의 Document가 서로 다른 구조로 구성 가능 데이터베이스들마다 다른 syntax 필드 추가 가능   차이점(2) Scalabilty  SQL DB: vertically scalable - CPU, RAM, SSD로 용량 문제 해결결 NoSQL DB: horizontally scalable - Sharding, Partitioning로 용량 문제 해결    2. Partition  데이터 나누기(vertical \u0026amp; horizontal)  데이터 매니지먼트, 퍼포먼스 등 다양한 이유     Vertical Partition  테이블을 더 작은 테이블로 나누기(Normalization와는 다름) ex. 지속적으로 업데이트되는 칼럼과 아닌 칼럼들 나누기   Horizontal Partition  Schema / Structure 자체를 복사하여 데이터 자체를 Sharded Key로 분리 NosQL DB에서는 필수적이다.    3. DynamoDB  aws.amazon.com \u0026gt; DynamoDB Partition Key는 SQL에서 Primary Key와 유사하다.  4. AWS SDK - Boto3 Package (DynamoDB 연결) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import sys import os import boto3 import logging def main(): try: dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;, region_name=\u0026#39;ap-northeast-2\u0026#39;, endpoint_url=\u0026#39;http://dynamodb.ap-northeast-2.amazonaws.com\u0026#39;) except: logging.error(\u0026#34;could not connect to dynamodb\u0026#34;) sys.exit(1) print(\u0026#39;Success\u0026#39;) if __name__==\u0026#39;__main__\u0026#39;: main()   5. 테이블 생성 및 스펙  Provisioned(할당됨) vs. On-demand(온디맨드)  6. Global Index, Local Index 7. INSERT(Single, Batch items) boto3 Documentation 읽어보기\nCreating a New Item 1 2 3 4 5 6 7 8 9  table.put_item( Item={ \u0026#39;username\u0026#39;: \u0026#39;janedoe\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;Jane\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Doe\u0026#39;, \u0026#39;age\u0026#39;: 25, \u0026#39;account_type\u0026#39;: \u0026#39;standard_user\u0026#39;, } )   Batch Writing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  with table.batch_writer() as batch: batch.put_item( Item={ \u0026#39;account_type\u0026#39;: \u0026#39;standard_user\u0026#39;, \u0026#39;username\u0026#39;: \u0026#39;johndoe\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;John\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Doe\u0026#39;, \u0026#39;age\u0026#39;: 25, \u0026#39;address\u0026#39;: { \u0026#39;road\u0026#39;: \u0026#39;1 Jefferson Street\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Los Angeles\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;CA\u0026#39;, \u0026#39;zipcode\u0026#39;: 90001 } } ) batch.put_item( Item={ \u0026#39;account_type\u0026#39;: \u0026#39;super_user\u0026#39;, \u0026#39;username\u0026#39;: \u0026#39;janedoering\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;Jane\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Doering\u0026#39;, \u0026#39;age\u0026#39;: 40, \u0026#39;address\u0026#39;: { \u0026#39;road\u0026#39;: \u0026#39;2 Washington Avenue\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Seattle\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;WA\u0026#39;, \u0026#39;zipcode\u0026#39;: 98109 } } ) batch.put_item( Item={ \u0026#39;account_type\u0026#39;: \u0026#39;standard_user\u0026#39;, \u0026#39;username\u0026#39;: \u0026#39;bobsmith\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;Bob\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Smith\u0026#39;, \u0026#39;age\u0026#39;: 18, \u0026#39;address\u0026#39;: { \u0026#39;road\u0026#39;: \u0026#39;3 Madison Lane\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Louisville\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;KY\u0026#39;, \u0026#39;zipcode\u0026#39;: 40213 } } ) batch.put_item( Item={ \u0026#39;account_type\u0026#39;: \u0026#39;super_user\u0026#39;, \u0026#39;username\u0026#39;: \u0026#39;alicedoe\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;Alice\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Doe\u0026#39;, \u0026#39;age\u0026#39;: 27, \u0026#39;address\u0026#39;: { \u0026#39;road\u0026#39;: \u0026#39;1 Jefferson Street\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Los Angeles\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;CA\u0026#39;, \u0026#39;zipcode\u0026#39;: 90001 } } )   8. 데이터 요청 및 제한점 boto3 Documentation 읽어보기\nGetting an Item 1 2 3 4 5 6  response = table.get_item( Key = { \u0026#39;artist_id\u0026#39;: \u0026#39;00FQb4jTyendYWaN8pK0wa\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;0Oqc0kKFsQ6MhFOLBNZIGX\u0026#39; } )   ClientError: An error occurred (ValidationException) when calling the GetItem operation: The provided key element does not match the schema\r위와 같은 에러가 뜬다면, key값을 제대로 다 넣었는지 확인해본다.\nQuerying and Scanning  Querying: Primary Key 값을 알고 있을 때 활용 Scanning: Primary Key 값을 모르지만, 다른 attribute를 알 때 활용\n- Scan은 모든 행을 다 훑는 비효율적인 기능이므로 꼭 필요할 때만 쓰는 것이 권장된다.  1 2 3 4 5 6 7 8 9 10 11 12  # Querying response = table.query( KeyConditionExpression=Key(\u0026#39;artist_id\u0026#39;).eq(\u0026#39;00FQb4jTyendYWaN8pK0wa\u0026#39;), FilterExpression=Attr(\u0026#39;popularity\u0026#39;).gt(75) #query도 filterexpresson 쓸 수 있다! ) print(len(response[\u0026#39;Items\u0026#39;])) # Scanning response = table.scan( FilterExpression=Attr(\u0026#39;popularity\u0026#39;).gt(75) ) print(len(response[\u0026#39;Items\u0026#39;]))   \n","description":"데이터의 이해와 데이터베이스","id":71,"section":"posts","tags":null,"title":"NoSQL (DynamoDB)","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_5/"},{"content":"Part 5. 데이터 엔지니어링 구축 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. 데이터 레이크 vs. 데이터 웨어하우스    구분 데이터 레이크 데이터 웨어하우스     Data Structure Raw Processed   Purpose of Data Not yet Determined In Use   Users Data Scientists Business Professionals   Accessibility High / Quick to update Complicated / Costly     Schema의 차이가 가장 크다. 데이터레이크는 차세대 시스템으로서 더욱 주목 받게 될 것이다. ETL(Extract-Transform-Load)  2. 데이터 레이크 아키텍처 데이터 파이프라인의 관심사  어떻게 관리 스케쥴링 에러 핸들링 데이터 백필  3. AWS S3  AWS S3 버킷 생성 cf. AWS Glue: table schema 관리 (데이터 레이크는 바뀐다.)  4. JSON, Parquet 1 2 3 4 5  # JSON 형식으로 하는 법 with open(\u0026#39;top_tracks.json\u0026#39;, \u0026#39;w\u0026#39;) as f: for i in top_tracks: json.dump(i, f) f.write(os.linesep)   1 2 3 4 5 6 7 8 9 10 11  # Parquet 형식으로 하는 법 # 퍼포먼스적으로 우수해진다. top_tracks = pd.DataFrame(raw) top_tracks.to_parquet(\u0026#39;top-tracks.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;, compressions=\u0026#39;snappy\u0026#39;) dt = datetime.utcnow().strftime(\u0026#34;%Y-%m-%d\u0026#34;) s3 = boto3.resource(\u0026#39;s3\u0026#39;) object = s3.Object(\u0026#39;spotify-artists\u0026#39;, \u0026#39;dt={}/top-tracks.parquet\u0026#39;.format(dt)) data = open(\u0026#39;top-tracks.parquet\u0026#39;, \u0026#39;rb\u0026#39;) object.put(Body=data)   5. S3 Data Lake ImportError: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\rSpotify audio features\n","description":"데이터 엔지니어링 구축","id":72,"section":"posts","tags":null,"title":"데이터 레이크","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part5_1/"},{"content":"Part 5. 데이터 엔지니어링 구축 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Presto 2. Serverless 3. AWS Athena 4. 테이블 생성 5. 데이터 쿼리 ","description":"데이터 엔지니어링 구축","id":73,"section":"posts","tags":null,"title":"S3 Athena","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part5_2/"},{"content":"Part 5. 데이터 엔지니어링 구축 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Apache Spark 2. EC-2 제플린 인스턴스 생성 ","description":"데이터 엔지니어링 구축","id":74,"section":"posts","tags":null,"title":"Apache Spark","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part5_3/"},{"content":"Part 5. 데이터 엔지니어링 구축 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Spark RDD 2. Spark Dataframes 3. Select Subset Columns 4. Filter Rows 5. Create UDF 6. Join 7. SQL 8. 데이터분석 with Spark 9. 시각화 with Spark ","description":"데이터 엔지니어링 구축","id":75,"section":"posts","tags":null,"title":"Pyspark","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part5_4/"},{"content":"Bias-Variance Tradeoff $$\\text{let } y = f(x) + \\varepsilon, \\text{ where } \\varepsilon \\text{ ~ } N(0, \\sigma^2)$$\n\\begin{align}\rMSE(\\hat{y}) \u0026amp;= E[(y-\\hat{y})^2] \\\\\r\u0026amp;= E[(f(x) + \\varepsilon - \\hat{f}(x))^2] \\\\\r\u0026amp;= E[(f(x)-\\hat{f}(x))^2] + E[\\varepsilon^2] + 2E[\\varepsilon(f(x) - \\hat{f}(x))] \\\\\r\u0026amp;= E[(f(x) - E(\\hat{f}(x)) + E(\\hat{f}(x)) - \\hat{f}(x) )^2] + \\sigma^2 \\\\\r\u0026amp;= E[(f(x) - E(\\hat{f}(x)))^2] + E[(\\hat{f}(x)-E(\\hat{f}(x)))^2] + 2(f(x) - E(\\hat{f}(x)))E[\\hat{f}(x) - E(\\hat{f}(x))] + \\sigma^2 \\\\\r\u0026amp;= {bias[\\hat{f}(x)]}^2 + Var(\\hat{f}(x)) + \\sigma^2 \\\\\r\u0026amp;= bias^2(\\hat{y}) + Var(\\hat{y}) + \\sigma^2\r\\end{align}\n$bias^2(\\hat{y})$: 참값과 추정치 평균의 차이 (간단한 모형일수록 높음)\n$Var(\\hat{y})$: 추정치와 추정치 평균의 차이 (복잡한 모형일수록 높음)\n$\\sigma^2$: irreducible error\n편향(bias)는 간단한 모형일수록 높으며, 분산(variance)는 복잡한 모형일수록 높다. 이때 $\\sigma^2$는 줄일 수 없는 오차이다. 그렇다면 적절한 모형 선택과 실험설계를 통해서 과적합을 방지해야 하는데, 이러한 부분은 머신러닝을 하면서 특히 더 주의해야 하는 부분이다.\nbias와 variance의 차이가 헷갈린다면 아래의 그림을 보면 훨씬 잘 이해가 될 것이다.\n참고 [1] https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":76,"section":"posts","tags":null,"title":"편향-분산 Tradeoff","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/bias_variance/"},{"content":"Performance Measure 성능을 평가하는 지표에는 여러 가지가 있다. 그중에서 대표적인 몇 개를 알아보고자 한다.\n1. Accuracy $$Accuracy = \\frac{\\text{correctly predicted}}{\\text{all dataset}}$$\nAccuracy는 balanced data가 아니라면 좋은 지표로서의 역할을 하기 힘들다. 왜냐하면 A,B,C,D라는 그룹이 있을 때, B~D가 각각 10개의 케이스 밖에 없고 A가 혼자서 500개의 케이스가 있다고 가정한다면 Accuracy 지표는 A 그룹에 의해 좌지우지 될 것이기 때문이다.\n2. Precision  Given a class prediction from the classifier, how likely is to be correct?\n내가 \u0026lsquo;TRUE\u0026rsquo;라고 말한 것 중에서 몇 %가 진짜 \u0026lsquo;TRUE\u0026rsquo;인가?\n $$Precision = \\frac{TP}{TP + FP} $$\nTP: True Positive, FP: False Positive\n3. Recall  Given a class, will the classifier detect it?\n진짜 \u0026lsquo;TRUE\u0026rsquo; 중에서 내가 \u0026lsquo;TRUE\u0026rsquo;라고 말한 것은 몇 %인가?\n $$Recall = \\frac{TP}{TP + FN} $$\nTP: True Positive, FN: False Negative\n4. F1 Score Precision과 Recall의 조화평균\n$$F1 \\text{ score} = 2 \\times \\frac{\\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}} $$\n 출처: https://www.youtube.com/watch?v=HBi-P5j0Kec F1 Score는 상대적으로 imbalanced data에서도 효과가 좋다. 그 이유는 위 그림을 통해서 이해할 수 있다.\n조화평균은 더 극단치에 대해서 페널티를 주기 때문이다.\n참고 [1] 허민석 유튜브\n","description":"","id":77,"section":"posts","tags":null,"title":"F1 Score","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/performance_measure/"},{"content":"Fisher Information Fisher information은 score function의 분산이다. 이 점을 명심해서 의미를 생각해보자. 아래는 Fisher information을 다양한 꼴로 표현해본 것이다.\n\\begin{align}\rI(\\theta;x) \u0026amp;= Var(\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)) \\\\\r\u0026amp;= E\\bigg[\\Big(\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\Big)^2\\bigg] \\\\\r\u0026amp;= -E\\bigg[\\frac{\\partial^2}{\\partial\\theta^2}log(f(x;\\theta)\\bigg]\r\\end{align}\n계산 증명 위에 대한 자세한 계산 증명은 아래와 같다.\n우선, score function의 평균이 0임을 구하는 것부터 시작한다.\n$$\\int \\frac{\\partial}{\\partial\\theta}logf(x;\\theta)f(x;\\theta)=0 $$\n양변을 $\\theta$로 미분해준다.\n$$\\int\\frac{\\partial^2}{\\partial\\theta^2}logf(x;\\theta)f(x;\\theta)dx + \\int\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)f'(x;\\theta)dx = 0 $$\n오른쪽 부분을 우변으로 이항하여 정리해주면 아래와 같다.\n\\begin{align}\rE\\bigg[\\frac{\\partial^2}{\\partial\\theta^2}logf(x;\\theta)\\bigg] \u0026amp;= -\\int\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)f'(x;\\theta)dx \\\\ \u0026amp;= -\\int\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\frac{f'(x;\\theta)}{f(x;\\theta)}f(x;\\theta)dx \\\\\r\u0026amp;= -\\int\\bigg(\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\bigg)^2f(x;\\theta)dx \\\\\r\u0026amp;= -E\\bigg[\\Big(\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\Big)^2\\bigg]\r\\end{align}\n","description":"","id":78,"section":"posts","tags":null,"title":"Fisher Information","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/fisher_information/"},{"content":"Jeffrey\u0026rsquo;s Prior 1. uninformative prior의 제한점 uninformative prior를 임의로 주게 될 경우, 여러 문제점이 있을 수 있는데 그 중 하나는 변수변환에 취약해질 수 있다는 점이다.\n예를 들어, $p(\\theta) \\propto 1$라고 uninformative prior를 주자. 그리고 $ \\phi = exp(\\theta)$라고 가정해보자.\n\\begin{align}\rp(\\phi) \u0026amp;\\propto p(\\theta) \\bigg|\\frac{d\\theta}{d\\phi}\\bigg| \\\\\r\u0026amp;\\propto \\frac{1}{\\phi} \\neq 1\r\\end{align}\n변수변환 후에는 prior가 uninformative하지 않게 되어버림을 확인할 수 있다.\n2. Jeffrey\u0026rsquo;s prior 그렇다면 어떻게 해야 변수변환에 강건한 prior를 줄 수 있을까?\n$$\\pi(\\phi) \\propto \\sqrt{I(\\theta)} $$\n위와 같이 주면 된다. 여기서 $I(\\theta) $는 Fisher Information을 뜻하며, 아래와 같다.\n$$I(\\theta) = -E\\Big[ \\frac{\\partial^2}{\\partial{\\theta}^2}ln L(x|\\theta) \\Big]$$\n3. 증명 이를 한 번 증명해보자. 단, $\\phi = f(\\theta), \\ f:\\text{one-to-one}$이다.\n\\begin{align}\rp(\\theta) \\propto \\sqrt{I(\\theta)} \u0026amp;\\xrightarrow{?} \\ p(\\phi) \\propto \\sqrt{I(\\phi)} \\\\\r\\\\\rp(\\phi) \u0026amp;= p(\\theta) \\bigg| \\frac{\\partial\\theta}{\\partial\\phi} \\bigg| \\\\\r\\\\\rI(\\phi) \u0026amp;= -E\\bigg[\\frac{\\partial^2}{\\partial\\phi^2}lnL(y|\\phi) \\bigg] \\\\\r\u0026amp;= E\\bigg[\\big(\\frac{\\partial}{\\partial\\phi}lnL(y|\\phi) \\big)^2 \\bigg] \\\\\r\u0026amp;= E\\bigg[\\big(\\frac{\\partial}{\\partial\\theta}lnL(y|\\theta) \\big)^2 \\big(\\frac{\\partial\\theta}{\\partial\\phi} \\big)^2 \\bigg] \\\\ \u0026amp;= I(\\theta) \\bigg|\\frac{\\partial\\theta}{\\partial\\phi} \\bigg|^2\\\\\r\\\\\rp(\\phi) \u0026amp;= p(\\theta)\\bigg|\\frac{\\partial\\theta}{\\partial\\phi} \\bigg| \\\\\r\u0026amp;\\propto \\sqrt{I(\\theta)}\\bigg|\\frac{\\partial\\theta}{\\partial\\phi} \\bigg| \\\\\r\u0026amp;=\\sqrt{I(\\phi)} \\\\\r\\\\\r\\rightarrow p(\\phi) \u0026amp;\\propto \\sqrt{I(\\phi)}\r\\end{align}\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":79,"section":"posts","tags":null,"title":"Jeffrey's Prior","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/jeffrey_prior/"},{"content":"Score Function $X \\text{ ~ } f(x;\\theta)$일 때, score function $s(\\theta;x)$은 다음과 같이 정의한다.\n$$\\frac{\\partial}{\\partial\\theta}logf(x;\\theta) $$\n평균 계산 증명 \\begin{align}\rE\\big[s(\\theta;x) \\big] \u0026amp;= \\int\\bigg[\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\bigg]f(x;\\theta)dx \\\\\r\u0026amp;= \\int\\frac{f'(x;\\theta)}{f(x;\\theta)}f(x;\\theta)dx \\\\\r\u0026amp;= \\int f'(x;\\theta)dx \\\\\r\u0026amp;= \\frac{\\partial}{\\partial\\theta}\\int f(x;\\theta)dx \\\\\r\u0026amp;= \\frac{\\partial}{\\partial\\theta}1 = 0\r\\end{align}\n의미 score function은 log likelihood의 기울기를 나타낸다는 데에서 의미가 있다.\nscore function의 평균이 0이라는 것의 의미를 그래프를 likelihood 그래프를 상상하여 생각해보자.\n","description":"","id":80,"section":"posts","tags":null,"title":"Score Function","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/score_function/"},{"content":"Empirical Bayes 베이지안은 기본적으로 prior, 즉 사전확률을 설정한다.\n그런데 이때 data density(histogram)에 비슷한 모양을 갖도록 prior를 설정하고자 하는 경우도 있는데, 이를 Empirical Bayes라고 한다.\n사전확률인데 왜 데이터를 보고 설정하는지에 대한 의문이 생길 수도 있긴 한다\u0026hellip;\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":81,"section":"posts","tags":null,"title":"Empirical Bayes","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/empricialbayes/"},{"content":"Gibbs Sampler 기본 원리 (추후 업데이트)\n예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  ## Data load  data = dget(\u0026#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/yX.o2uptake\u0026#39;) y = data[,1] X = data[,-1] inv = solve ## set prior g = length(y) nu0 = 1 s20 = summary(lm(y~-1+X))$sigma^2 n = length(y) p = ncol(X) ## setup S = 1000 set.seed(2021) BETA = matrix(NA, nrow=S, ncol=p) sigma2 = matrix(NA, nrow=S, ncol=1) BETA[1,] = inv(t(X) %*% X) %*% t(X) %*% y sigma2[1,] = s20 ## gibbs sampling nun = nu0 + n betan = (g/(g+1)) * inv(t(X) %*% X) %*% t(X) %*% y for(s in 2:S){ s2n = nu0*s20 + t(y-X%*%BETA[s-1,]) %*% (y-X%*%BETA[s-1,]) sigma2[s,] = 1/rgamma(1, shape=nun/2, rate=s2n/2) Sigman = (g/(g+1)) * sigma2[s,] * inv(t(X) %*% X) BETA[s,] = MASS::mvrnorm(n=1, betan, Sigman) } ## graph colnames(BETA) = colnames(X) gather(as.data.frame(BETA)) %\u0026gt;% ggplot(aes(y=value, fill=key)) + geom_histogram() + coord_flip() + facet_wrap(~key, scales=\u0026#39;free_x\u0026#39;) + ggtitle(\u0026#39;Posterior samples of Beta\u0026#39;) + theme(legend.position = \u0026#39;None\u0026#39;)   ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r종류 1. Systematic Sweep Gibbs Sampler 모든 모수에 대해 샘플링을 반복하여 업데이트하는 방법\n2. Random Sweep Gibbs Sampler 무작위로 모수를 뽑아서 업데이트하는 방법\n3. Grouped Gibbs Sampler 여러 개의 모수를 한번에 뽑아서 업데이트하는 방법\n이때 모수 간의 연관성이 생긴다.\n4. Collapsed Gibbs Sampler 적분을 통해서 특정 모수와 독립적인 분포를 계산한 후, 샘플링하여 업데이트하는 방법\n참고사이트 [1] https://niceguy1575.tistory.com/entry/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%86%B5%EA%B3%84%ED%95%99-4-Gibbs-Sampler%EA%B9%81%EC%8A%A4-%EC%83%98%ED%94%8C%EB%9F%AC-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%EC%84%B1%EC%A7%88\n","description":"","id":82,"section":"posts","tags":null,"title":"Gibbs Sampler","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/gibbs_sampler/"},{"content":"fread 패키지로 대용량 데이터 빠르게 불러오기 약 24000행의 샘플 csv가 있다고 가정하자. 그렇다면 fread와 read.csv의 성능은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11  library(data.table) fread(\u0026#39;sample.csv\u0026#39;) #약 24000x3 system.time(fread(\u0026#39;sample.csv\u0026#39;)) # 사용자 시스템 elapsed  # 0.02 0.00 0.01  system.time(read.csv(\u0026#39;sample.csv\u0026#39;)) # 사용자 시스템 elapsed  # 0.74 0.03 0.77   read.csv는 0.77초가 걸리는 데에 반해 fread는 0.01초 만에 읽어왔다.\n이외에도 3백만 행의 csv으로 실험해본 결과, 각각 2초와 33초로 그 성능 차이가 더욱 도드라짐을 알 수 있었다.\n","description":"","id":83,"section":"posts","tags":null,"title":"fread","uri":"https://jiwooblog.netlify.app/posts/r/fread/"},{"content":"\r\ry \u0026lt;- c(93, 112, 122, 135, 122, 150, 118, 90, 124, 114)\rn \u0026lt;- length(y)\rs2 \u0026lt;- var(y)\rmy \u0026lt;- mean(y) \r# helper functions to sample from and evaluate\r# scaled inverse chi-squared distribution\rrsinvchisq \u0026lt;- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...)\rdsinvchisq \u0026lt;- function(x, nu, s2){\rexp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x)\r}\rns \u0026lt;- 1000\rsigma2 \u0026lt;- rsinvchisq(ns, n-1, s2)\rmu \u0026lt;- my + sqrt(sigma2/n)*rnorm(length(sigma2))\rsigma \u0026lt;- sqrt(sigma2)\rynew \u0026lt;- rnorm(ns, mu, sigma)\rt1l \u0026lt;- c(90, 150)\rt2l \u0026lt;- c(10, 60)\rnl \u0026lt;- c(50, 185)\rt1 \u0026lt;- seq(t1l[1], t1l[2], length.out = ns)\rt2 \u0026lt;- seq(t2l[1], t2l[2], length.out = ns)\rxynew \u0026lt;- seq(nl[1], nl[2], length.out = ns)\r# multiplication by 1./sqrt(s2/n) is due to the transformation of\r# variable z=(x-mean(y))/sqrt(s2/n), see BDA3 p. 21\rpm \u0026lt;- dt((t1-my) / sqrt(s2/n), n-1) / sqrt(s2/n)\rpmk \u0026lt;- density(mu, adjust = 2, n = ns, from = t1l[1], to = t1l[2])$y\r# the multiplication by 2*t2 is due to the transformation of\r# variable z=t2^2, see BDA3 p. 21\rps \u0026lt;- dsinvchisq(t2^2, n-1, s2) * 2*t2\rpsk \u0026lt;- density(sigma, n = ns, from = t2l[1], to = t2l[2])$y\r# multiplication by 1./sqrt(s2/n) is due to the transformation of variable\r# see BDA3 p. 21\rp_new \u0026lt;- dt((xynew-my) / sqrt(s2*(1+1/n)), n-1) / sqrt(s2*(1+1/n))\r# Combine grid points into another data frame\r# with all pairwise combinations\rdfj \u0026lt;- data.frame(t1 = rep(t1, each = length(t2)),\rt2 = rep(t2, length(t1)))\rdfj$z \u0026lt;- dsinvchisq(dfj$t2^2, n-1, s2) * 2*dfj$t2 * dnorm(dfj$t1, my, dfj$t2/sqrt(n))\r# breaks for plotting the contours\rcl \u0026lt;- seq(1e-5, max(dfj$z), length.out = 6)\rDemo 3.1\rdfm \u0026lt;- data.frame(t1, Exact = pm, Empirical = pmk) %\u0026gt;% gather(grp, p, -t1)\rmargmu \u0026lt;- ggplot(dfm) +\rgeom_line(aes(t1, p, color = grp)) +\rcoord_cartesian(xlim = t1l) +\rlabs(title = \u0026#39;Marginal of mu\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;\u0026#39;) +\rscale_y_continuous(breaks = NULL) +\rtheme(legend.background = element_blank(),\rlegend.position = c(0.75, 0.8),\rlegend.title = element_blank())\rdfs \u0026lt;- data.frame(t2, Exact = ps, Empirical = psk) %\u0026gt;% gather(grp, p, -t2)\rmargsig \u0026lt;- ggplot(dfs) +\rgeom_line(aes(t2, p, color = grp)) +\rcoord_cartesian(xlim = t2l) +\rcoord_flip() +\rlabs(title = \u0026#39;Marginal of sigma\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;\u0026#39;) +\rscale_y_continuous(breaks = NULL) +\rtheme(legend.background = element_blank(),\rlegend.position = c(0.75, 0.8),\rlegend.title = element_blank())\rjoint1labs \u0026lt;- c(\u0026#39;Samples\u0026#39;,\u0026#39;Exact contour\u0026#39;)\rjoint1 \u0026lt;- ggplot() +\rgeom_point(data = data.frame(mu,sigma), aes(mu, sigma, col = \u0026#39;1\u0026#39;), size = 0.1) +\rgeom_contour(data = dfj, aes(t1, t2, z = z, col = \u0026#39;2\u0026#39;), breaks = cl) +\rcoord_cartesian(xlim = t1l,ylim = t2l) +\rlabs(title = \u0026#39;Joint posterior\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;\u0026#39;) +\rscale_y_continuous(labels = NULL) +\rscale_x_continuous(labels = NULL) +\rscale_color_manual(values=c(\u0026#39;blue\u0026#39;, \u0026#39;black\u0026#39;), labels = joint1labs) +\rguides(color = guide_legend(nrow = 1, override.aes = list(\rshape = c(16, NA), linetype = c(0, 1), size = c(2, 1)))) +\rtheme(legend.background = element_blank(),\rlegend.position = c(0.5, 0.9),\rlegend.title = element_blank())\rgrid.arrange(joint1, margsig, margmu, nrow = 2)\r\r","description":"","id":84,"section":"posts","tags":null,"title":"BDA Example","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/bda_example/"},{"content":"\r\r\r\r\r\r.scroll-100 {\rmax-height: 100px;\roverflow-y: auto;\rbackground-color: 'white';\r}\r\rpre {\rmax-height: 300px;\roverflow-y: auto;\rbackground-color: 'white';\r}\rpre[class] {\rmax-height: 100px;\rbackground-color: 'green';\r}\r\r\r# plotly\r```r\rlibrary(ggplot2)\rlibrary(plotly)\rlibrary(gapminder)\r1 2 3 4 5 6 7 8  p \u0026lt;- gapminder %\u0026gt;% filter(year==1977) %\u0026gt;% ggplot( aes(gdpPercap, lifeExp, size = pop, color=continent)) + geom_point() + scale_x_log10() + theme_bw() ggplotly(p)   \r{\"x\":{\"data\":[{\"x\":[3.69111835304969,3.47837128685849,3.0124834262036,3.50716177034781,2.87121498358872,2.74515544499733,3.25125676748187,3.04507811557752,3.05460729073385,3.06915101836112,2.90078062155373,3.51310821042633,3.40101028408395,3.48879895792208,3.44490216214739,2.98162238872728,2.7039391611549,2.74570576546197,4.3373708614867,2.94682314869669,2.99704718642708,2.94185210799775,2.88350583493768,3.10298675433845,2.87237164152654,2.8063987207202,4.34145849934605,3.18871158763867,2.82166002181421,2.83657428165302,3.17536457562432,3.56948896052176,3.37486193965761,2.70098023955392,3.58843821514052,2.90789326381397,3.29709308978461,3.6354640490023,2.82612704524949,3.23994022438144,3.19361683040004,3.12978175413031,3.16166517152032,3.90464260358686,3.34301221488094,3.57765383934322,2.9833972607425,3.185478974435,3.49427662628827,2.92620510614229,3.20103869689067,2.83606300605042],\"y\":[58.014,39.483,49.19,59.319,46.137,45.91,49.355,46.775,47.383,50.939,47.804,55.625,52.374,46.519,53.319,42.024,44.535,44.51,52.79,41.842,51.756,40.762,37.465,56.155,52.208,43.764,57.442,46.881,43.767,41.714,50.852,64.93,55.73,42.495,56.437,41.291,44.514,67.064,45,58.55,48.879,36.788,41.974,55.527,47.8,52.537,49.919,52.887,59.837,50.35,51.386,57.674],\"text\":[\"gdpPercap: 4910.4168\nlifeExp: 58.01400\npop: 17152804\ncontinent: Africa\",\"gdpPercap: 3008.6474\nlifeExp: 39.48300\npop: 6162675\ncontinent: Africa\",\"gdpPercap: 1029.1613\nlifeExp: 49.19000\npop: 3168267\ncontinent: Africa\",\"gdpPercap: 3214.8578\nlifeExp: 59.31900\npop: 781472\ncontinent: Africa\",\"gdpPercap: 743.3870\nlifeExp: 46.13700\npop: 5889574\ncontinent: Africa\",\"gdpPercap: 556.1033\nlifeExp: 45.91000\npop: 3834415\ncontinent: Africa\",\"gdpPercap: 1783.4329\nlifeExp: 49.35500\npop: 7959865\ncontinent: Africa\",\"gdpPercap: 1109.3743\nlifeExp: 46.77500\npop: 2167533\ncontinent: Africa\",\"gdpPercap: 1133.9850\nlifeExp: 47.38300\npop: 4388260\ncontinent: Africa\",\"gdpPercap: 1172.6030\nlifeExp: 50.93900\npop: 304739\ncontinent: Africa\",\"gdpPercap: 795.7573\nlifeExp: 47.80400\npop: 26480870\ncontinent: Africa\",\"gdpPercap: 3259.1790\nlifeExp: 55.62500\npop: 1536769\ncontinent: Africa\",\"gdpPercap: 2517.7365\nlifeExp: 52.37400\npop: 7459574\ncontinent: Africa\",\"gdpPercap: 3081.7610\nlifeExp: 46.51900\npop: 228694\ncontinent: Africa\",\"gdpPercap: 2785.4936\nlifeExp: 53.31900\npop: 38783863\ncontinent: Africa\",\"gdpPercap: 958.5668\nlifeExp: 42.02400\npop: 192675\ncontinent: Africa\",\"gdpPercap: 505.7538\nlifeExp: 44.53500\npop: 2512642\ncontinent: Africa\",\"gdpPercap: 556.8084\nlifeExp: 44.51000\npop: 34617799\ncontinent: Africa\",\"gdpPercap: 21745.5733\nlifeExp: 52.79000\npop: 706367\ncontinent: Africa\",\"gdpPercap: 884.7553\nlifeExp: 41.84200\npop: 608274\ncontinent: Africa\",\"gdpPercap: 993.2240\nlifeExp: 51.75600\npop: 10538093\ncontinent: Africa\",\"gdpPercap: 874.6859\nlifeExp: 40.76200\npop: 4227026\ncontinent: Africa\",\"gdpPercap: 764.7260\nlifeExp: 37.46500\npop: 745228\ncontinent: Africa\",\"gdpPercap: 1267.6132\nlifeExp: 56.15500\npop: 14500404\ncontinent: Africa\",\"gdpPercap: 745.3695\nlifeExp: 52.20800\npop: 1251524\ncontinent: Africa\",\"gdpPercap: 640.3224\nlifeExp: 43.76400\npop: 1703617\ncontinent: Africa\",\"gdpPercap: 21951.2118\nlifeExp: 57.44200\npop: 2721783\ncontinent: Africa\",\"gdpPercap: 1544.2286\nlifeExp: 46.88100\npop: 8007166\ncontinent: Africa\",\"gdpPercap: 663.2237\nlifeExp: 43.76700\npop: 5637246\ncontinent: Africa\",\"gdpPercap: 686.3953\nlifeExp: 41.71400\npop: 6491649\ncontinent: Africa\",\"gdpPercap: 1497.4922\nlifeExp: 50.85200\npop: 1456688\ncontinent: Africa\",\"gdpPercap: 3710.9830\nlifeExp: 64.93000\npop: 913025\ncontinent: Africa\",\"gdpPercap: 2370.6200\nlifeExp: 55.73000\npop: 18396941\ncontinent: Africa\",\"gdpPercap: 502.3197\nlifeExp: 42.49500\npop: 11127868\ncontinent: Africa\",\"gdpPercap: 3876.4860\nlifeExp: 56.43700\npop: 977026\ncontinent: Africa\",\"gdpPercap: 808.8971\nlifeExp: 41.29100\npop: 5682086\ncontinent: Africa\",\"gdpPercap: 1981.9518\nlifeExp: 44.51400\npop: 62209173\ncontinent: Africa\",\"gdpPercap: 4319.8041\nlifeExp: 67.06400\npop: 492095\ncontinent: Africa\",\"gdpPercap: 670.0806\nlifeExp: 45.00000\npop: 4657072\ncontinent: Africa\",\"gdpPercap: 1737.5617\nlifeExp: 58.55000\npop: 86796\ncontinent: Africa\",\"gdpPercap: 1561.7691\nlifeExp: 48.87900\npop: 5260855\ncontinent: Africa\",\"gdpPercap: 1348.2852\nlifeExp: 36.78800\npop: 3140897\ncontinent: Africa\",\"gdpPercap: 1450.9925\nlifeExp: 41.97400\npop: 4353666\ncontinent: Africa\",\"gdpPercap: 8028.6514\nlifeExp: 55.52700\npop: 27129932\ncontinent: Africa\",\"gdpPercap: 2202.9884\nlifeExp: 47.80000\npop: 17104986\ncontinent: Africa\",\"gdpPercap: 3781.4106\nlifeExp: 52.53700\npop: 551425\ncontinent: Africa\",\"gdpPercap: 962.4923\nlifeExp: 49.91900\npop: 17129565\ncontinent: Africa\",\"gdpPercap: 1532.7770\nlifeExp: 52.88700\npop: 2308582\ncontinent: Africa\",\"gdpPercap: 3120.8768\nlifeExp: 59.83700\npop: 6005061\ncontinent: Africa\",\"gdpPercap: 843.7331\nlifeExp: 50.35000\npop: 11457758\ncontinent: Africa\",\"gdpPercap: 1588.6883\nlifeExp: 51.38600\npop: 5216550\ncontinent: Africa\",\"gdpPercap: 685.5877\nlifeExp: 57.67400\npop: 6642107\ncontinent: Africa\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":[6.32127781530702,5.29612899121632,4.85958274371316,4.29233940761101,5.26165272498491,4.97061767194295,5.5059176412168,4.66704214356912,5.05559917855771,4.06676322554192,6.94049569932146,4.52040583610645,5.45016604603988,4.01129639562885,7.60694585246374,3.97973112975184,4.73782100329548,7.39505365224446,4.26382530133105,4.22383634289638,5.76860640347983,5.03145493864573,4.27878249624843,6.11542223529683,4.44354450304809,4.56187177172394,4.77827606357594,5.51109590728372,5.22907021838905,5.3366454219231,4.49965617393791,4.33879188894232,6.41229679974379,5.82395887424691,4.36004866819238,5.23491359575959,8.62896025232351,4.17122854948148,5.09486783222961,3.77952755905512,5.17905878226218,4.854775454452,5.05045749460444,6.97912565512447,6.31771439639559,4.19891947998289,6.31954665895726,4.69663035294985,5.27632870617264,5.85427629024999,5.17305387000117,5.35482857766587],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"Africa\",\"legendgroup\":\"Africa\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4.00341859740172,3.54999558616307,3.82348196645535,4.34421307668719,3.67731159008544,3.58158649939174,3.77282591203612,3.80485437036137,3.42845697609622,3.82475196835622,3.71087205749874,3.68841917661215,3.27283885748582,3.50558514939778,3.82283441748743,3.88507437266432,3.7392851792172,3.72850897552038,3.51166593362054,3.7980489038637,3.98991789679654,3.89760258369119,4.38152357914239,3.81320321246222,4.11872592974958],\"y\":[68.481,50.023,61.489,74.21,67.052,63.837,70.75,72.649,61.788,61.31,56.696,56.029,49.923,57.402,70.11,65.032,57.47,68.681,66.353,58.447,73.44,68.3,73.38,69.481,67.456],\"text\":[\"gdpPercap: 10079.0267\nlifeExp: 68.48100\npop: 26983828\ncontinent: Americas\",\"gdpPercap: 3548.0978\nlifeExp: 50.02300\npop: 5079716\ncontinent: Americas\",\"gdpPercap: 6660.1187\nlifeExp: 61.48900\npop: 114313951\ncontinent: Americas\",\"gdpPercap: 22090.8831\nlifeExp: 74.21000\npop: 23796400\ncontinent: Americas\",\"gdpPercap: 4756.7638\nlifeExp: 67.05200\npop: 10599793\ncontinent: Americas\",\"gdpPercap: 3815.8079\nlifeExp: 63.83700\npop: 25094412\ncontinent: Americas\",\"gdpPercap: 5926.8770\nlifeExp: 70.75000\npop: 2108457\ncontinent: Americas\",\"gdpPercap: 6380.4950\nlifeExp: 72.64900\npop: 9537988\ncontinent: Americas\",\"gdpPercap: 2681.9889\nlifeExp: 61.78800\npop: 5302800\ncontinent: Americas\",\"gdpPercap: 6679.6233\nlifeExp: 61.31000\npop: 7278866\ncontinent: Americas\",\"gdpPercap: 5138.9224\nlifeExp: 56.69600\npop: 4282586\ncontinent: Americas\",\"gdpPercap: 4879.9927\nlifeExp: 56.02900\npop: 5703430\ncontinent: Americas\",\"gdpPercap: 1874.2989\nlifeExp: 49.92300\npop: 4908554\ncontinent: Americas\",\"gdpPercap: 3203.2081\nlifeExp: 57.40200\npop: 3055235\ncontinent: Americas\",\"gdpPercap: 6650.1956\nlifeExp: 70.11000\npop: 2156814\ncontinent: Americas\",\"gdpPercap: 7674.9291\nlifeExp: 65.03200\npop: 63759976\ncontinent: Americas\",\"gdpPercap: 5486.3711\nlifeExp: 57.47000\npop: 2554598\ncontinent: Americas\",\"gdpPercap: 5351.9121\nlifeExp: 68.68100\npop: 1839782\ncontinent: Americas\",\"gdpPercap: 3248.3733\nlifeExp: 66.35300\npop: 2984494\ncontinent: Americas\",\"gdpPercap: 6281.2909\nlifeExp: 58.44700\npop: 15990099\ncontinent: Americas\",\"gdpPercap: 9770.5249\nlifeExp: 73.44000\npop: 3080828\ncontinent: Americas\",\"gdpPercap: 7899.5542\nlifeExp: 68.30000\npop: 1039009\ncontinent: Americas\",\"gdpPercap: 24072.6321\nlifeExp: 73.38000\npop: 220239000\ncontinent: Americas\",\"gdpPercap: 6504.3397\nlifeExp: 69.48100\npop: 2873520\ncontinent: Americas\",\"gdpPercap: 13143.9510\nlifeExp: 67.45600\npop: 13503563\ncontinent: Americas\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(163,165,0,1)\",\"opacity\":1,\"size\":[6.97047083068522,5.15434238640086,10.3553727984455,6.77543956769576,5.77446910007444,6.85635435789675,4.65435232683653,5.6710443310913,5.18472018302092,5.42956521861593,5.0398270921093,5.23768683822733,5.13057191783969,4.83958883002819,4.66475315834704,8.68911697083458,4.74607251669638,4.59414954550014,4.82688148286866,6.23316601826461,4.84414878929001,4.37991825947829,12.9086371873162,4.80663028657321,6.03320025163606],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(163,165,0,1)\"}},\"hoveron\":\"points\",\"name\":\"Americas\",\"legendgroup\":\"Americas\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2.89548517717506,4.28645875933068,2.81946314412135,2.72013629197205,2.86995736499049,4.048680298964,2.91027070197067,3.14072860861607,4.07513053543838,4.16696961434647,4.12406772903255,4.22037948985684,3.45520305365965,3.61345080718246,3.66812684894598,4.7728017852095,3.93750268823479,3.58296303108212,3.21682849824462,2.56937390961505,2.84142982784579,4.07365765189317,3.07037821749517,3.37533512418369,4.53361654092087,4.04960907918865,3.12993971717703,3.50453672644772,3.74791804661293,3.29252733979409,2.85341657088882,3.56618184839601,3.26239535810376],\"y\":[38.438,65.593,46.923,31.22,63.96736,73.6,54.208,52.702,57.702,60.413,73.06,75.38,61.134,67.159,64.766,69.343,66.099,65.256,55.491,56.059,46.748,57.367,54.043,60.06,58.69,70.795,65.949,61.195,70.59,62.494,55.764,60.765,44.175],\"text\":[\"gdpPercap: 786.1134\nlifeExp: 38.43800\npop: 14880372\ncontinent: Asia\",\"gdpPercap: 19340.1020\nlifeExp: 65.59300\npop: 297410\ncontinent: Asia\",\"gdpPercap: 659.8772\nlifeExp: 46.92300\npop: 80428306\ncontinent: Asia\",\"gdpPercap: 524.9722\nlifeExp: 31.22000\npop: 6978607\ncontinent: Asia\",\"gdpPercap: 741.2375\nlifeExp: 63.96736\npop: 943455000\ncontinent: Asia\",\"gdpPercap: 11186.1413\nlifeExp: 73.60000\npop: 4583700\ncontinent: Asia\",\"gdpPercap: 813.3373\nlifeExp: 54.20800\npop: 634000000\ncontinent: Asia\",\"gdpPercap: 1382.7021\nlifeExp: 52.70200\npop: 136725000\ncontinent: Asia\",\"gdpPercap: 11888.5951\nlifeExp: 57.70200\npop: 35480679\ncontinent: Asia\",\"gdpPercap: 14688.2351\nlifeExp: 60.41300\npop: 11882916\ncontinent: Asia\",\"gdpPercap: 13306.6192\nlifeExp: 73.06000\npop: 3495918\ncontinent: Asia\",\"gdpPercap: 16610.3770\nlifeExp: 75.38000\npop: 113872473\ncontinent: Asia\",\"gdpPercap: 2852.3516\nlifeExp: 61.13400\npop: 1937652\ncontinent: Asia\",\"gdpPercap: 4106.3012\nlifeExp: 67.15900\npop: 16325320\ncontinent: Asia\",\"gdpPercap: 4657.2210\nlifeExp: 64.76600\npop: 36436000\ncontinent: Asia\",\"gdpPercap: 59265.4771\nlifeExp: 69.34300\npop: 1140357\ncontinent: Asia\",\"gdpPercap: 8659.6968\nlifeExp: 66.09900\npop: 3115787\ncontinent: Asia\",\"gdpPercap: 3827.9216\nlifeExp: 65.25600\npop: 12845381\ncontinent: Asia\",\"gdpPercap: 1647.5117\nlifeExp: 55.49100\npop: 1528000\ncontinent: Asia\",\"gdpPercap: 371.0000\nlifeExp: 56.05900\npop: 31528087\ncontinent: Asia\",\"gdpPercap: 694.1124\nlifeExp: 46.74800\npop: 13933198\ncontinent: Asia\",\"gdpPercap: 11848.3439\nlifeExp: 57.36700\npop: 1004533\ncontinent: Asia\",\"gdpPercap: 1175.9212\nlifeExp: 54.04300\npop: 78152686\ncontinent: Asia\",\"gdpPercap: 2373.2043\nlifeExp: 60.06000\npop: 46850962\ncontinent: Asia\",\"gdpPercap: 34167.7626\nlifeExp: 58.69000\npop: 8128505\ncontinent: Asia\",\"gdpPercap: 11210.0895\nlifeExp: 70.79500\npop: 2325300\ncontinent: Asia\",\"gdpPercap: 1348.7757\nlifeExp: 65.94900\npop: 14116836\ncontinent: Asia\",\"gdpPercap: 3195.4846\nlifeExp: 61.19500\npop: 7932503\ncontinent: Asia\",\"gdpPercap: 5596.5198\nlifeExp: 70.59000\npop: 16785196\ncontinent: Asia\",\"gdpPercap: 1961.2246\nlifeExp: 62.49400\npop: 44148285\ncontinent: Asia\",\"gdpPercap: 713.5371\nlifeExp: 55.76400\npop: 50533506\ncontinent: Asia\",\"gdpPercap: 3682.8315\nlifeExp: 60.76500\npop: 1261091\ncontinent: Asia\",\"gdpPercap: 1829.7652\nlifeExp: 44.17500\npop: 8403990\ncontinent: Asia\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,191,125,1)\",\"opacity\":1,\"size\":[6.14601109401926,4.06189233680308,9.29441811927379,5.39475462377927,22.6771653543307,5.08426676230058,19.2706113323364,10.9715821633898,7.43994831281003,5.89270757284543,4.9155533082844,10.342652961876,4.61658106384004,6.25889092142746,7.4890188516279,4.41106157328659,4.85034613145665,5.97722629927717,4.51816213431315,7.2295116202132,6.06899974449858,4.36894912815136,9.21575428359059,7.98702151037774,5.52430918996644,4.70007429187427,6.08413180764231,5.50291508915639,6.29375367739212,7.86362869673143,8.14954658274176,4.44626602186696,5.5539430462659],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,125,1)\"}},\"hoveron\":\"points\",\"name\":\"Asia\",\"legendgroup\":\"Asia\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[3.54814411807184,4.29555439638822,4.28144187754483,3.54758782074795,3.88151249733011,4.05328536317461,4.17026642863828,4.31011744274222,4.19327554039949,4.26227627214274,4.31202751232983,4.15215143685527,4.06725083957618,4.29347221889702,4.04731308089082,4.15399722211687,3.98208706693409,4.32652140433408,4.36756741361793,3.97809563416623,4.00742708886894,3.97110865215781,4.11329709452241,4.03832857580155,4.18403893629866,4.12178698250771,4.27544324046184,4.43107881405245,3.63033859911073,4.24126620200209],\"y\":[68.93,72.17,72.8,69.86,70.81,70.64,70.71,74.69,72.52,73.83,72.5,73.68,69.95,76.11,72.03,73.48,73.066,75.24,75.37,70.67,70.41,69.46,70.3,70.45,70.97,74.39,75.44,75.39,59.507,72.76],\"text\":[\"gdpPercap: 3533.0039\nlifeExp: 68.93000\npop: 2509048\ncontinent: Europe\",\"gdpPercap: 19749.4223\nlifeExp: 72.17000\npop: 7568430\ncontinent: Europe\",\"gdpPercap: 19117.9745\nlifeExp: 72.80000\npop: 9821800\ncontinent: Europe\",\"gdpPercap: 3528.4813\nlifeExp: 69.86000\npop: 4086000\ncontinent: Europe\",\"gdpPercap: 7612.2404\nlifeExp: 70.81000\npop: 8797022\ncontinent: Europe\",\"gdpPercap: 11305.3852\nlifeExp: 70.64000\npop: 4318673\ncontinent: Europe\",\"gdpPercap: 14800.1606\nlifeExp: 70.71000\npop: 10161915\ncontinent: Europe\",\"gdpPercap: 20422.9015\nlifeExp: 74.69000\npop: 5088419\ncontinent: Europe\",\"gdpPercap: 15605.4228\nlifeExp: 72.52000\npop: 4738902\ncontinent: Europe\",\"gdpPercap: 18292.6351\nlifeExp: 73.83000\npop: 53165019\ncontinent: Europe\",\"gdpPercap: 20512.9212\nlifeExp: 72.50000\npop: 78160773\ncontinent: Europe\",\"gdpPercap: 14195.5243\nlifeExp: 73.68000\npop: 9308479\ncontinent: Europe\",\"gdpPercap: 11674.8374\nlifeExp: 69.95000\npop: 10637171\ncontinent: Europe\",\"gdpPercap: 19654.9625\nlifeExp: 76.11000\npop: 221823\ncontinent: Europe\",\"gdpPercap: 11150.9811\nlifeExp: 72.03000\npop: 3271900\ncontinent: Europe\",\"gdpPercap: 14255.9847\nlifeExp: 73.48000\npop: 56059245\ncontinent: Europe\",\"gdpPercap: 9595.9299\nlifeExp: 73.06600\npop: 560073\ncontinent: Europe\",\"gdpPercap: 21209.0592\nlifeExp: 75.24000\npop: 13852989\ncontinent: Europe\",\"gdpPercap: 23311.3494\nlifeExp: 75.37000\npop: 4043205\ncontinent: Europe\",\"gdpPercap: 9508.1415\nlifeExp: 70.67000\npop: 34621254\ncontinent: Europe\",\"gdpPercap: 10172.4857\nlifeExp: 70.41000\npop: 9662600\ncontinent: Europe\",\"gdpPercap: 9356.3972\nlifeExp: 69.46000\npop: 21658597\ncontinent: Europe\",\"gdpPercap: 12980.6696\nlifeExp: 70.30000\npop: 8686367\ncontinent: Europe\",\"gdpPercap: 10922.6640\nlifeExp: 70.45000\npop: 4827803\ncontinent: Europe\",\"gdpPercap: 15277.0302\nlifeExp: 70.97000\npop: 1746919\ncontinent: Europe\",\"gdpPercap: 13236.9212\nlifeExp: 74.39000\npop: 36439000\ncontinent: Europe\",\"gdpPercap: 18855.7252\nlifeExp: 75.44000\npop: 8251648\ncontinent: Europe\",\"gdpPercap: 26982.2905\nlifeExp: 75.39000\npop: 6316424\ncontinent: Europe\",\"gdpPercap: 4269.1223\nlifeExp: 59.50700\npop: 42404033\ncontinent: Europe\",\"gdpPercap: 17428.7485\nlifeExp: 72.76000\npop: 56179000\ncontinent: Europe\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,176,246,1)\",\"opacity\":1,\"size\":[4.73711086274366,5.46245399673978,5.69923465730081,5.00994841135964,5.59538443706805,5.04523525790558,5.73248147708556,5.15554006271282,5.10659105931128,8.26207702648979,9.21603585103979,5.64793676344929,5.77801236958701,4.00561540333621,4.87759423629259,8.38266622872152,4.20280449359801,6.06235892227666,5.00334741211677,7.39523452347783,5.68347312501541,6.63718460047671,5.58381321630558,5.11921103857739,4.57227899402677,7.48917192574021,5.53761739047554,5.31519777193933,7.78197424302696,8.38758788519898],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,176,246,1)\"}},\"hoveron\":\"points\",\"name\":\"Europe\",\"legendgroup\":\"Europe\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4.26326190559163,4.21041798943211],\"y\":[73.49,72.22],\"text\":[\"gdpPercap: 18334.1975\nlifeExp: 73.49000\npop: 14074100\ncontinent: Oceania\",\"gdpPercap: 16233.7177\nlifeExp: 72.22000\npop: 3164900\ncontinent: Oceania\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(231,107,243,1)\",\"opacity\":1,\"size\":[6.08061917751821,4.85899251590851],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(231,107,243,1)\"}},\"hoveron\":\"points\",\"name\":\"Oceania\",\"legendgroup\":\"Oceania\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":26.2283105022831,\"r\":7.30593607305936,\"b\":40.1826484018265,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[2.45920251583532,4.88297317898922],\"tickmode\":\"array\",\"ticktext\":[\"300\",\"1000\",\"3000\",\"10000\",\"30000\"],\"tickvals\":[2.47712125471966,3,3.47712125471966,4,4.47712125471966],\"categoryorder\":\"array\",\"categoryarray\":[\"300\",\"1000\",\"3000\",\"10000\",\"30000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"gdpPercap\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[28.9755,78.3545],\"tickmode\":\"array\",\"ticktext\":[\"30\",\"40\",\"50\",\"60\",\"70\"],\"tickvals\":[30,40,50,60,70],\"categoryorder\":\"array\",\"categoryarray\":[\"30\",\"40\",\"50\",\"60\",\"70\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"lifeExp\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.93503937007874},\"annotations\":[{\"text\":\"continent\npop\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"11b040177098\":{\"x\":{},\"y\":{},\"size\":{},\"colour\":{},\"type\":\"scatter\"}},\"cur_data\":\"11b040177098\",\"visdat\":{\"11b040177098\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r1 2  tmp \u0026lt;- c(1,2,3,4) print(tmp)   ## [1] 1 2 3 4\r","description":"test","id":85,"section":"posts","tags":null,"title":"test","uri":"https://jiwooblog.netlify.app/posts/r/test/"},{"content":"Yang, Y., Zha, K., Chen, Y. C., Wang, H., \u0026amp; Katabi, D. (2021). Delving into Deep Imbalanced Regression. arXiv preprint arXiv:2102.09554.\nIn Short Imbalanced Regression(not classification) with LDS and FDS using kernel function\n1. Introduction 불균형데이터에 대해서 학습할 때, 많은 경우에 회귀 문제보다는 분류 문제에 초점이 맞춰져있다. 그러나 현실에서는 연속형 데이터가 불균형인 경우도 충분히 있다. 예를 들어, 연령 분포 데이터의 경우에는 각 나라에 따라서 나이대별 분포가 다르다. 이외에도 혈압이나 맥박수와 같은 환자 활력 징후 데이터나 응급실 체류시간과 같은 데이터들도 그 예시가 될 수 있겠다.\n2. Related Work 2-1. Imbalanced Classification  Data-level   ROS (Random Oversampling) RUS (Random Undersampling) SMOTE GAN (CGAN, FSC-GAN, MFC-GAN)  Algorithm-level   Inverse frequency weight Square root weight Focal Loss Two Stage Training  2-2. Imbalanced Regression 불균형 연속형 데이터 특징\n 클래스 구분이 없다. 주변값의 분포에 따라 불균형 수준이 다르다. 특정 타겟값에 대한 데이터가 없을 수 있다.  위와 같은 특징 때문에 불균형 연속형 데이터의 경우에는 imbalanced classification와 다르다. 그래서\u0026hellip;!\n resampling 또는 reweighting 방법을 적용하기 어렵다. 불균형/균형 경계가 뚜렷하지 않다. 주변 데이터를 통해 interpolation 또는 extrapolation을 해야 한다.   CIFAR-100: 100개 클래스 IMDB-WIKI: 0~99세  연속형 데이터의 학습 결과는 범주형 데이터의 학습 결과와 다소 다른 양상을 보인다.\n 범주형 데이터는 불균형의 정도가 오분류율 분포와 밀접한 관계가 있다. (상관계수 -0.76) 한편, 연속형 데이터는 불균형 정도가 상대적으로 덜 정확하게 오분류율 분포에 반영된다. (상관계수 -0.47)  3. Methods Problem Setting\n 인접 데이터 간 유사성 활용 커널 함수를 활용하여 불균형 문제 해소 커널밀도추정(KDE)  3-1. Label Distribution Smoothing (LDS) 레이블 공간 관점\nFigure2에서 보이는 바와 같이, 연속형 데이터와 범주형 데이터가 차이가 나는 이유는 Empirical label distribution과 (unseen data가 포함된) Real label density distribution이 다르기 때문이다. 실제 연속형 데이터는 위에서 언급된 바와 같이 주변 레이블간 연관성을 가진다.\n그래서 LDS의 커널 밀도 추정 과정을 통해 주변 데이터의 연속형이 반영된 Effective Label Density를 추출한다. 이렇게 되면, 예측 태스크에 영향을 미치는 실제 불균형 정도를 잘 반영하게 됨을 알 수 있다. 이는 상관계수가 -0.47에서 -0.83로, 그 절댓값이 상승했다는 점에서도 수치적으로 확인 가능하다. 이로 인해 $\\tilde{p}(y')$을 아래와 같이 정의한다면, 이의 역수를 손실함수의 가중치로 활용할 수 있게 된다.\n$$\n\\tilde{p}(y') = \\int_{Y}k(y, y')p(y)dy\n$$\n참고로 여기서 커널 함수란, 원점을 중심으로 대칭이며 적분값이 1인 non-negative 함수를 뜻한다. 대표적으로는 Gaussian 커널 또는 Laplacian 커널이 있다.\n3-2. Feature Distribution Smoothing (FDS) 특징 공간 관점\n타겟 공간에서의 연속성은 잘 학습된 모델의 특징공간에도 반영된다.\nbin = 타겟 공간을 b개로 나누는 동일한 간격 (ex. 연령: 1살)\n잘 학습된 encoder를 통해서 특징 공간을 얻을 수 있게 된다. 여기서 인물 image의 특징이 학습된 특징 공간 z에 요약하기 위해서 기초통계량을 구하게 되면, 모든 b에 대해서 평균과 분산을 다음과 같이 구할 수 있다. 이를 기준으로 특정 값 $b_0$를 고정하여두고 다른 $b$의 평균과 분산의 코사인 유사도를 계산한다.\n위 그림에서는 일단 30살을 기준으로 코사인 유사도를 계산한 것이다. 상식과 비슷하게, 30살 주변의 값들과는 높은 유사를 나타냈다. 하지만, 특이하게도 0~6살에 해당하는 값들과 유사도가 꽤 높게 나타나는 이상한 현상을 확인할 수 있다. 이는 해당 데이터가 상대적으로 적어서(few-shot region), 즉 데이터 불균형 문제로 인해 발생한 현상이라고 볼 수 있다. 이러한 문제 역시 LDS처럼 커널 함수를 통해 해결한다.\n$$\\mu_b = \\frac{1}{N_b}\\sum_{i=1}^{N_b}z_i \\rightarrow \\tilde{\\mu_b} = \\sum_{b' \\in B}k(y_b,y_{b'})\\mu_{b'} \\\\\r\\Sigma_b = \\frac{1}{N_b-1}\\sum_{i=1}^{N_b}(z_i-\\mu_b)(z_i-\\mu_b)^T \\rightarrow \\tilde{\\Sigma_b} = \\sum_{b' \\in B}k(y_b,y_{b'})\\Sigma_{b'}$$\n한 에폭에서 학습된 z의 통계량에 커널함수를 적용해서 calibration시키고, regression layer를 통과시켜서 손실함수를 계산한다.\n여기서 LDS와 달리 추가된 부분이 있는데, 이는 바로 업데이트 방식이다. 학습과정에서 안정적이고 정확한 추정치를 얻기 위해서, 매 epoch마다 EMA를 진행한다. 구체적으로 말하자면, 현재 에폭 내에 있는 샘플에 대해서 진행이 되면, 통계량을 업데이트 하기 위해서 모멘텀 업데이트 방식(EMA, exponential moving average)을 활용한다.\n그리고나서 마지막으로, 현재 통계량에 커널함수를 적용함으로써 다음 epoch으로 전달해준다.\nCalibration\n$$\\tilde{z} = \\tilde{\\Sigma}_{b}^{\\frac{1}{2}}\\Sigma_{b}^{-\\frac{1}{2}}(z-\\mu_b)+\\tilde{\\mu_b}$$\nFDS의 결과는 위와 같다. 왼쪽은 FDS를 적용하지 않은 것이고, 오른쪽은 FDS를 적용한 것이다. FDS를 적용한 오른쪽 그래프가 기본적인 상식을 반영하는 것처럼 보인다.\n이외에 FDS의 장점을 정리해보자면, 일종의 calibration layer로서 어떤 모델에도 직접적으로 적용될 수 있다는 점이다.\n4. Performace Comparison 4-1. Dataset 5개의 Dataset이 사용되었다. 직접 만든 것으로 보인다.\n IMDB-WIKI-DIR (age) AgeDB-DIR (age) STS-B-DIR (text similarity score) NYUD2-DIR (depth) SHHS-DIR (health condition score)  각 데이터들은 모두 불균형함을 확인할 수 있다.\n4-2. Baseline imbalanced classfication에서 활용되는 방법들을 차용함.\n Synthetic samples: (1) SmoteR (2) SMOGN Error-aware loss: (3) Focal-R Two-stage training: (4) regressor re-training(RRT) Cost-sensitive re-weighting: (5) naive inverse(INV) (6) square-root inverse(SQINV)  이를 (1) LDS (2) FDS (3) LDS+FDS가 추가된 버전과 함께 비교함. 그리고 이 모든 것들 중에서 가장 성능이 좋은 것을 VANILLA와 마지막으로 비교한다.\n4-3. Main Results 비교 metrics은 MAE(Mean Average Eror)와 GM(Geometric Mean Error)가 있다.\nIMDB-WIKI-DIR에서는 위와 같이 Medium-Shot과 Few-Shot에서 특히 유의미한 성능 상승이 있었다는 점이 특히 주목해볼 만하다. 이외의 데이터에서 성능은 아래와 같다.\n4-4. Further Analysis Extraopolation \u0026amp; Interpolation\nTraining Dataset에는 없고, Test Dataset에는 있는 부분에 대해서의 성능을 이야기하는 것 같다.\n5. Conclusion New task: Deep Imbalanced Regression(DIR)\nNew techniques: LDS \u0026amp; FDS\nNew benchmarks: IMDB-WIKI-DIR / AgeDB-DIR / STS-B-DIR / NYUD2-DIR / SHHS-DIR\n\u0026mdash; Critical Point (MY OWN OPINION)  bin을 몇 개의 b로 나눌지에 따라서 성능이 달라질 수 있겠다. 만약 엄청 세분화하게 된다면 성능이 저하될 것으로 예상되는데, 이렇게 본다면 완벽한 연속형 데이터라고는 보기 힘들지 않을까?  \u0026mdash; Reference [1] Youtube 연구자 발표 영상\n[2] Youtube\n","description":"","id":86,"section":"blog","tags":null,"title":"Delving into Deep Imbalanced Regression","uri":"https://jiwooblog.netlify.app/blog/211122_imbalanced_regression/"},{"content":"SAS Workshop 211118 SAS Workshop 대우관 415호(국제회의실)\n","description":"","id":87,"section":"blog","tags":["SAS"],"title":"SAS Workshop","uri":"https://jiwooblog.netlify.app/blog/211118_sasworkshop/"},{"content":"Ali-Gombe, A., \u0026amp; Elyan, E. (2019). MFC-GAN: class-imbalanced dataset classification using multiple fake class generative adversarial network. Neurocomputing, 361, 212-221.\nIn Short Data Geneation(Augmentation) with Multiclass Fakes\n1. Introduction Since both minority and majority classes come from the same distribution, these classes share some common features.\n=\u0026gt; Features learned from majority classes should aid in learning the minority classes.\n=\u0026gt; Class conditioned generation will focus the model into sampling minority classes.\n2. Related Works 2-1. Mutliclass에 대한 관심 부족 binary classification에 대해서는 연구가 많이 이루어졌지만, 이에 비해 multiclass classification에는 상대적으로 관심이 적었다. 그럼에도 이와 관련된 선행연구들이 있긴 한데, 예를 들어서 multi-class decomposition, Class Rectification Loss(CRL), mean squared false error 등이 있다.\n CRL: it performs hard mining of the minority class is each each batch forcing the model to create a boundary for each minority class with a hard positive and negative threshold. LMLE(Large Margin Local Embedding): it employs clustering among classes to maintain the structure of the minroity data.\n=\u0026gt; 하지만 위의 두 방법론은 데이터가 클 경우 계산량이 너무 많아진다는 단점들이 있다.  2-2. SMOTE SMOTE와 같이 기존에 유명한 방식들은 극단적으로 불균형인 데이터에 대해서는 비효율적이라고 알려져있다.\n2-3. GAN의 발전 minority class data를 생성하기 위해서 C-GAN(Conditional GAN)이 잠재적 해결방안이 될 수는 있지만, 완벽하지는 않다. 이처럼 vanilla GAN이나 AC-GAN들도 데이터가 불균형한 상태에서는 minority sample을 생성하는 데에 있어서 효과적이지 못했다. DCGAN이나 MelanoGAN은 효과적이기는 했으나, multiclass가 아닌 binary case에서였다.\n결론적으로, 기존에 제시된 방법들은 크게 불균형한 상황에서는 데이터 생성에 효과적이지 못했다. 즉, 충분히 다양한 데이터를 만들어내지 못했다(not enough data variance). 이에 반해 MFC-GAN은 보다 간단한 방법으로 특정 클래스에 해당하는 데이터 샘플을 생성할 수 있다. 3. Method GAN은 기본적으로 두 가지 종류의 학습데이터를 사용한다. 하나는 기존의 학습데이터에 있는 원본데이터이고, 나머지 하나는 Generator가 생성한 샘플들(fake)이다.\n10개의 클래스가 있다고 했을 때, 원래는 one-hot encoding 방식으로 1000000000로 표시했다. 하지만 여기서는 fake image에 대한 label도 따로 신경써준다. 그렇기 때문에 real image는 10000000000000000000로 표시하고, 이에 해당하는 fake image의 label은 00000000001000000000로 표시한다.\n3-1. Objective Function $L_s$: to estimate the sampling loss, which represents the probability of the sample being real or fake\n$L_{cd}$: to estimate the classification loss over the discriminator\n$L_{cg}$: to estimate the classification loss over the generator\n$L_{cd}$ means that the discriminator classifies samples as real or fake with associated class\n$L_{cg}$ means that the generator classifies fake samples as real classes\ngenerator: to maximize the difference of $L_s$ and $L_{cg}$\ndiscriminator: to maximize the sum of $L_s$ and $L_{cd}$\nMFC-GAN generator is sampled using a noise vector conditioned on real class labels\nlabel이 없는 경우에는 위와 같이 Vanilla GAN처럼 작동하게 된다.\n[핵심 POINT]\n본 논문의 핵심은 discriminator이다. 가령, AC-GAN의 discriminator는 $L_{cd}$가 아니라 $L_{cg}$와 $L_s$의 합을 최대화하는 것이 목표이다. 3-2. MFC-GAN vs. FSC-GAN MFC-GAN: generator is penalized according to how far the generated sample is from the real class label\nFSC-GAN: generator is penalized according to how far the generated sample is from the fake class label\n=\u0026gt; this promoted early convergence of MFC-GAN\n4. Experiments 4-1. Experimental set-up 라이브러리: tensorflow 1.0, Keras 2.0 비교모델: SMOTE, AC-GAN, FSC-GAN, 원데이터 공통 분류분석기: CNN 결과비교기준:   주관적: plausibility of sample(i.e., visual inspection) 객관적: 다양한 지표들을 통해 분류결과 성능 비교  4-2. Dataset 데이터: MNIST, E-MNIST, SVHN, CIFAR-10\n 대부분 강제로 imbalanced data로 만들어주고나서 활용함(특정 클래스 임의로 선택 후 undersampling) MNIST: 2개의 클래스를 임의로 골라 원데이터의 1%(50개)씩만 활용 (run마다 다른 클래스 선택) E-MNIST: 총 81만 여개 샘플에서, 62개 클래스 중 21개의 클래스에 해당하는 샘플들이 각각 3000개 이하이다. 즉, 이미 imbalanced이다. 여기서 가장 적은 10개의 클래스를 활용하였다. (G, K, Q, f, j, k, m, p, s, y) SVHN: MNIST처럼 2개의 클래스에서 50개씩만 사용하여 강제로 imbalanced를 만들어주었다. (1,2) CIFAR-10: 10개 클래스 중 두 개의 클래스에서 1000개 중에서 50개씩만 활용하였다. 즉, 강제로 imbalanced를 만들어주었다. (Aeroplane, Automobile)  MNIST와 E-MNIST의 경우에는 FSC-GAN의 구조를 차용하였고, SVHN과 CIFAR-10에서는 AC-GAN의 구조를 사용하였다. 그리고, AC-GAN, FSC-GAN, MFC-GAN 모두의 경우에 공정한 비교를 위해 spectral weight normalization을 생성기와 분류기 모두에 추가해주었다.\n4-3. Image Generation  network switcher의 의미\nDepending on the availability of labels, the network switcher feature enables both models to alternate between two training modes. The switcher is a piece-wise function that oscillates between supervised and unsupervised training.  4-4. Image Classification CNN을 공통 분류분석기로 활용하였다.\n MNIST, E-MNIST, SVHN: 3 layers with softmax activation layer (2 convolution layers, 3x3 kernels with 2x2 max-pooling, two filter maps, fully connected layer), ReLuactivated, 0.5 dropout ratio, Adadelta optimizer CIFAR-10: 3 convolution layers, 0.2 dropout ratio, SGD optimizer  5. Results   생성된 사진의 퀄리티가 우수함\n딱 보더라도 MFC-GAN이 훨씬 우수한 성능으로 이미지를 만들어냄을 눈으로 확인할 수 있다. 특히 (c)를 보면, unlabeled가 5만개였는데도 성능이 괜찮게 나왔다.\n  컴퓨팅 효율성\nFSC-GAN은 500 epoch가 필요한 데에 비해, MFC-GAN은 50 epoch만을 필요로 했다. 즉, data augmentation에는 MFC-GAN이 보다 우수함을 알 수 있었다.\n  객관적 성능지표에서도 우수함\nMFC-GAN에서 민감도, balanced accuracy, G-mean 모두 높게 나타났다. 이에 반해, FSC-GAN은 모든 경우에서 성능 향상으로 이어지지 않았다. 특정 상황에서는 SMOTE가 더 우수한 듯 보이기는 했지만, 이는 단순히 소수 집단에 대해 샘플이 더 많기 때문일 거라고 추측된다. 왜냐하면 샘플 수가 적은 클래스에 대해서는 성능이 안 좋은 것이 확인되었기 때문이다.\n  6. Discussion The fidelity and diversity of MFC-GAN minority samples made classification easier for the CNN. The diversity of generated samples indicates no sign of mode collapse in the model.\n한계점 1. CIFAR-10에서의 부족한 성능\nTable3에서 알 수 있듯이, CIFAR-10에서는 모든 모델들이 성능이 좋지 않게 나타났다. 이는 32x32라는 작은 이미지 사이즈에 비해 많은 정보량이 들어가있는 분류작업이기 때문이라고 보인다. (그래도 그중에서 그나마 MFC-GAN이 낫기는 했다.)\n한계점2. 특정 클래스에서의 부족한 성능\n특정 클래스에 대해서는 모든 모델들이 모두 성능이 안 좋게 나타났다. E-MNIST의 경우에는 m과 s가 이에 해당한다. 이는 s가 5, S, 2, z와 비슷하기 때문이라고 저자들은 보고 있다.\n\u0026mdash; Critical Point (MY OWN OPINION)   E-MINST, SVHN, CIFAR-10에서 다른 클래스가 minority class로서 선정이 되었다면 어떻게 결과가 달라졌을지 알 수가 없다. 선택된 클래스의 특성에 따른 결과가 아니라고 결론 지을 수가 없다는 한계점이 있다.\n  기본 CNN 모형 말고 ResNet나 YOLO와 같이 다른 architecture를 사용했으면 어땠을까라는 생각이 있다.\n  \u0026mdash; 참고사이트 [1] Spectral Normalization(1)\n[2] Spectral Normalization(2)\nFurther Study Needed  [1] Few-Shot Classifier GAN (FSC-GAN)\nAli-Gombe, A., Elyan, E., Savoye, Y., \u0026amp; Jayne, C. (2018, July). Few-shot classifier GAN. In 2018 International Joint Conference on Neural Networks (IJCNN) (pp. 1-8). IEEE. [2] Advanced GANs\n사이트1\n사이트2 ","description":"","id":88,"section":"blog","tags":null,"title":"MFC_GAN / Class-imbalanced dataset classification using Multiple Fake Generative Adversarial Network","uri":"https://jiwooblog.netlify.app/blog/211025_mfc_gan/"},{"content":"O’Brien, R., \u0026amp; Ishwaran, H. (2019). A random forests quantile classifier for class imbalanced data. Pattern recognition, 90, 232-249.\nIn Short 불균형데이터 처리를 위해, quantile classifier을 사용한 Random Forest\n1. Introduction 1-1. 불균형데이터의 정의 일반적으로 두 개의 클래스가 있는 상황에서, 한 클래스에 속한 원소가 나머지 클래스에 속한 원소에 비해 월등하게 많은 경우를 데이터가 불균형한 상황이라고 정의한다. (여기서는 $Y=1$이 Minoritiy, $Y=0$이 Majority라고 생각하자.)\n5개의 근접원소들에 대해서 Majority 클래스에 속하는 원소가 0~1개인 원소를 Safe, 2~3개는 Borderline, 4~5개는 Rare라고 부른다.\n1-2. IR (Imbalance Ratio) $$IR = \\frac{\\text{# of Majority class}}{\\text{# of Minority class}}$$\n1-3. Marginally imbalanced 정의: $p(x) \\ll \\frac{1}{2} \\text{ for all } x \\in X \\text{ where } p(x) = P(Y=1|X=x)$\n1-4. Conditionally imbalanced 정의: $\\text{there exists a set } A \\subset X \\text{ with nonzero probability, } P(X \\in A) \u0026gt;0, \\text{ such that } P(Y=1|X \\in A) \\approx 1 \\text{ and } p(x) \\ll \\frac{1}{2} \\text{ for } x \\notin A$\n1-5. Notation 정리 아래는 본 논문의 Table 1이다.\n2. Related Work 2-1. Data Level Methods 데이터 자체를 건드려서 해결하는 방식을 Data Level Method라고 칭한다. 본 논문에서 이야기하는 대표적인 예시로는 Balanced Random Forest(BRF)가 있다. 이는 다수 클래스에 속하는 것들을 적게 뽑는(undersampling) 방식이다. 이외에 SMOTE와 같은 oversampling 기법들도 있고, undersampling과 oversampling이 결합된 하이브리드 방식도 있다. 아래는 해당 논문에서 추가적으로 언급된 방법론들이다.\n One-sided Sampling: Tomek Links Neighborhood Balanced Bagging SMOTEBoost, RUSBoost, EUSBoost: combine boosting with sampling data at each boosting iteration  2-2. Algorithmic Level Methods 위처럼 데이터의 균형을 직접적으로 조절하는 방식이 있는가하면, 알고리즘적으로 분류 성능을 높이고자 하는 노력들도 있었다. 아래는 다양한 방법론들 예시이다.\n SHRINK Helling Distance Decision Trees(HDDT) Near-Bayesian Support Vector Machines(NBSVM) Class Switching according to NEarest Enemy Distance  2-3. Bayes Decision Rule $$\\delta_B(x) = I\\big( p(x) \\geq 1/2 \\big)$$\n참고로 여기서 $p(x) = P(Y=1 | X=x)$이다. 이는 IR이 커지면 문제가 된다. $p(x)$가 0에 가까우면 해당 classifier는 Majority 클래스로 예측하게 되는데, 일반적으로 다수의 원소가 속해있는 클래스로 예측하도록 $p(x)$가 0에 가깝게 되는 경우가 많기 때문이다. 그리고 이때 Bayes error는 아래와 같이 0에 가깝게 나오므로 마치 완벽한 분류기처럼 착각될 수 있다.\n$$r(\\delta_B) = E[\\min\\{p(X), 1-p(X)\\}] = E[p(X)] \\approx 0$$\n2-4. Balanced Random Forests (BRF) random forests with undersampling the majority class\n3. Q*-Classifier 3-1. Quantile classifier $$\\delta_q(x) = I\\big( p(x) \\geq q \\big), \\ 0\u0026lt;q\u0026lt;1$$\nquantile classifer가 무엇인지 이해하면, 해당 논문의 핵심 포인트인 q*-classifier을 이해할 수 있다.\n해당 방법론은 크게 두 가지 장점이 있다. 첫번째는 TPR과 TNR을 최대화한다는 점이다. 두번째는 cost-weighted Bayes classifier과 같이 작동함으로써 weighted risk를 최소화해준다.\n$$r(\\hat{\\delta}, \\ell_0, \\ell_1) = E\\Big[\\ell_{0}1_{(\\hat{\\delta}(X)=1, Y=0)} + \\ell_{1}1_{(\\hat{\\delta}(X)=0, Y=1)}\\Big]$$\n여기서 $\\ell_0$와 $\\ell_1$은 각각 Majority 원소 또는 Minority 원소를 잘못 분류할 때의 cost이며, 모두 양수이다.\ncost-weighted risk의 관점에서 보면, 최적의 classifier는 cost-weighted Bayes rule을 활용하는 것인데, 이는 아래와 같이 나타낼 수 있다.\n$$\\delta_{WB}(x) = 1_{\\big(p(x) \\geq \\frac{\\ell_0}{\\ell_0 + \\ell_1}\\big)}$$\n이것이 최적인 이유는 모든 분류기에 대해서 $r(\\delta_{WB}, \\ell_0, \\ell_1) \\leq r(\\hat{\\delta}, \\ell_0, \\ell_1)$를 만족하며, 그 리스크가 아래를 만족하기 때문이다.\n$$r(\\delta_{WB}, \\ell_0, \\ell_1) = E\\Big[min\\Big(\\ell_1p(X), \\ell_0(1-p(X))\\Big)\\Big]$$\r 위에 대한 증명은 논문 Appendix1에 정리되어있으며, 추후 추가 서술하도록 하겠다.\n3-2. TNR+TPR optimal TNR(True Negative Rate)와 TPR(True Positive Rate)의 합을 최대화시켜주는 분류기를 TNR+TPR optimal이라고 부른다.\n$$TPR = \\frac{TP}{TP+FN}, \\ TNR = \\frac{TN}{TN+FP}$$\n참고로 기본 Bayes Rule을 활용한 분류기는 TNR은 1에 가깝지만, TPR은 0에 가깝게 나온다는 한계가 있다.\n3-3. q*-classifier $$\\delta_D(x) = 1_{\\big(\\Delta_D(x) \\geq 1\\big)}\r\\text{, where } \\Delta_D(x) = \\frac{f_{X|Y}(x|1)}{f_{X|Y}(x|0)} = \\frac{p(x)(1-\\pi)}{(1-p(x))\\pi}$$\n여기서 $\\delta_{q*}(x) = I\\big(p(x) \\geq \\pi \\big) = \\delta_D(x)$를 q*-classifier라고 부른다. 이는 알고리즘적으로 데이터 불균형 문제를 해결하고자 하는 방법에 속하며, Density-based approach라고 할 수 있다. 왜냐하면 data density를 활용하여 클래스를 분류하기 때문이다.\ncf. Density-based approach\n$$\\delta_D(x) = 1_{\\big(f_{X|Y}(x|1) \\geq f_{X|Y}(x|0)\\big)}$$\n여기서 주목해야 할 점은 conditional density of the response ($p(x)$)가 아니라 conditional density of the features($f_{X|Y}$)를 활용했다는 점이다. 이로 인해 소수 클래스의 prevalance 효과를 제거할 수 있다.\nq*-classifier는 TNR+TPR optimal이다. (이에 대한 자세한 내용은 아래에 나와있다.) 뿐만 아니라, cost-weighted Bayes rule을 사용하는데, $\\ell_0 = \\pi$이고, $\\ell_1=(1-\\pi)$이다. 그렇게 하면 marginal 그리고 conditional imbalanced 상태에서 weighted risk가 모두 0에 가깝게 나온다. 이를 수식으로 표현하면 아래와 같다. 우변에 있는 ($\\pi$)는 marginally imbalanced한 상황에서도, conditionally imbalanced한 상황에서도 모두 0에 가까워야 한다는 사실을 알아두자.\n\\begin{align}\rr(\\delta_{q*}, \\pi, 1-\\pi) \u0026amp;= E[\\min\\{(1-\\pi)p(X), \\pi(1-p(X))\\}] \\\\\r\u0026amp;\\leq E[\\pi(1-p(X))] \\\\ \u0026amp;\\leq \\pi\r\\end{align}\n Theorem2 중에서 TNR+TPR optimal에 대한 증명은 아래와 같다. 참고로, FPR = 1-TNR, FNR = 1-TPR이므로, TNR와 TPR을 최대화하는 것은 FPR과 FNR을 최소화하는 것과 같다.\n\\begin{align}\rFPR(\\hat{\\delta}) \u0026amp;+ FNR(\\hat{\\delta})\\\\\r\u0026amp;= P\\{\\hat{\\delta}(X)=1|Y=0\\} + P\\{\\hat{\\delta}(X)=0|Y=1\\} \\\\\r\u0026amp;= \\frac{P\\{\\hat{\\delta}(X)=1, Y=0\\}}{P(Y=0)} + \\frac{P\\{\\hat{\\delta}(X)=0, Y=1\\}}{P(Y=1)} \\\\\r\u0026amp;= E\\Big[\\frac{1\\{\\hat{\\delta}(X)=1, Y=0\\}}{\\ell_1} + \\frac{1\\{\\hat{\\delta}(X)=0, Y=1\\}}{\\ell_0}\\Big]\r\\end{align}\n위에 $\\ell_0\\ell_1$을 곱해주면 아래의 식을 최소화해주는 것과 같다.\n$$E\\Big[\\ell_{0}1_{(\\hat{\\delta}(X)=1, Y=0)} + \\ell_{1}1_{(\\hat{\\delta}(X)=0, Y=1)}\\Big]$$\n그리고 이는 3-1.에서 보았듯이 weighted risk와 완전하게 같은 형태이다. 즉, weighted risk를 최소화한다면 TNR+TPR optimal 조건도 자연스럽게 만족이 될 것임을 알 수 있다.\n3-3-1. 특징(1) q*-classifier is invariant to response-based sampling cf) Response-based sampling: where data values are selected with probability that depend only on the value of Y and not X.\nUnder balanced subsampling, the subsampled Bayes rule $\\delta^{S}_{B}$ is TNR+TPR optimal.\r $$\\begin{equation}\rP(S=1 |Y) = \\begin{cases}\r\\pi_S(1), \u0026amp;\\mbox{if } Y=1 \\\\\r\\pi_S(0), \u0026amp;\\mbox{otherwise}\r\\end{cases} \\end{equation} \\quad (5)$$\n$$\\pi^S = P(Y=1|S=1) = \\frac{P(S=1|Y=1)P(Y=1)}{P(S=1)} = \\frac{\\pi_S(1)\\pi}{P(S=1)}. \\quad (6)\\\\\r1-\\pi^S = P(Y=0|S=1) = \\frac{\\pi_S(0)(1-\\pi)}{P(S=1)} \\\\\r\\therefore \\frac{\\pi_S(1)}{\\pi_S(0)} = \\frac{1-\\pi}{\\pi}. \\quad (7)$$\nsubsampled된 데이터들로 분류기를 학습시킨 것을 $\\delta_{B}^{S}$라고 하자. 이를 이후에는 subsampled Bayes rule이라고 부르겠다.\n$$\\delta_{B}^{S}(x) = 1 \\mbox{, if } \\frac{p^S(x)}{1-p^S(x) }\\geq 1 \\\\\r\\mbox{where } p^S(x) = \\frac{f^S_{X,Y}(x,1)}{f^S_X(x)}, \\ 1-p^S(x) = \\frac{f^S_{X,Y}(x,0)}{f^S_X(x)} \\\\\r\\therefore \\delta_{B}^{S}(x) = 1 \\mbox{, if } \\frac{f^S_{X,Y}(x,1)}{f^S_{X,Y}(x,0)}\\\\$$\n$$\\begin{align}\r\\mbox{where } f^S_{X,Y}(x,1) \u0026amp;= P(X=x, Y=1 |S=1) \\\\\r\u0026amp;= \\frac{P(X=x, Y=1, S=1)}{P(S=1)} \\\\\r\u0026amp;= \\frac{P(S=1|X=x, Y=1)P(X=x,Y=1)}{P(S=1)} \\\\\r\u0026amp;= \\frac{P(S=1|Y=1)f_{X,Y}(x,1)}{P(S=1)} \\\\\r\u0026amp;= \\frac{\\pi_S(1)p(x)f_X(x)}{P(S=1)}\r\\end{align}\\\\$$\n$$\\therefore \\frac{p^S(x)}{1-p^S(x)} = \\frac{p(x)\\pi_s(1)}{(1-p(x))\\pi_S(0)}\\\\\r\\therefore \\delta_{B}^{S}(x) = 1 \\mbox{, if } \\frac{p(x)}{1-p(x)} \\geq \\frac{\\pi_S(0)}{\\pi_S(1)} = \\frac{\\pi}{1-\\pi} \\quad \\mbox{by (7)} \\\\\r\\therefore \\delta_B^S(x) = \\delta_D(x)$$\nresponse-based sampling 형태인 (5)에 의해 $\\delta^{S}_{q*} = \\delta_{q*}$이므로 $\\delta^{S}_{q*}$는 TNR+TPR optimal이다.\n그리고 balanced sampling (7)에 의해 $\\delta^S_B = \\delta^{S}_{q*} = \\delta_{q*}$이며, 세 방법론은 모두 TNR+TPR optimal이다.\r 4. Application to Random Forests 기본 RF과 BRF의 차이점은, 부트스트랩 과정에서 샘플사이즈를 $N$이 아니라 $2N_1$만큼을 사용하고, 샘플링 확률을 $\\pi_S(1) = \\frac{N_0}{N_1}\\pi_S(0)$로 설정한다는 점에서 다르다.\n한편 기본 RF와 본 논문에서 제안하는 RFQ는, $\\delta_{FQ}(x) = 1_{\\hat{p}RF(x) \\geq \\frac{1}{2}}$ 대신에 $\\delta_{RFQ}(x) = 1_{\\hat{p}RF(x) \\geq \\pi}$를 쓴다는 차이점이 있다.\n5. Comparison to BRF 5-1. Why RFQ is better 우선 기본적으로 BRF와 RFQ 모두 TNR+TPR property를 갖고 있기는 하다. BRF의 경우는 Theorem 3에서 balancing condition (7)에 의해, RFQ의 경우는 Theorem 2에서 q*-classification을 사용한다는 점에서 확인할 수 있다.\n그런데 실제 확률 함수인 $p(x)$가 예측시 활용이 되는데, 실전에서는 이를 estimate하여 활용하여야 한다는 문제가 발생한다. BRF에 비해서 RFQ가 훨씬 많은 숫자의 샘플을 활용하기 때문에, 일반적으로 BRF에 비해 RFQ가 $p(x)$을 estimate하는 데에 유리하다고 할 수 있다. 특히 IR이 커지면 커질수록 $2N_1$은 $N$에 비해서 훨씬 작아지기 때문에, IR이 커지면 커질수록 BRF보다 RFQ가 더욱 유리하다. 뿐만 아니라 차원이 커질수록 estimation이 어려워지기 때문에, 이러한 상황에서도 RFQ가 유리하다고 볼 수 있다.\nRFQ의 q*-classifier에서 $q* = \\pi$로 사용하는데, empirical relative frequency로써 $\\hat{\\pi} = \\frac{N_1}{N_0 + N_1}$을 사용한다.\n5-2. G-mean $$\\mbox{G-mean} = (TNR \\times TPR)^{1/2}$$\nq가 근사적으로 $\\hat{\\pi}$에 가까워졌을 때, RFQ에 의한 G-mean이 최대치에 가깝다는 것을 143개의 벤치마크 데이터셋을 통해서 확인했다.(10-fold CV를 250번씩 시행하였다.) 이는 분류기에 있어서 TNR+TPR optimality가 중요한 특징이라는 것을 시사한다.\n(splitting criterion으로서 Gini index 대신 Hellinger distance를 사용해보긴 하였으나 크게 유의미하지는 않았다.)\n[생각해볼 점 1]\nG-mean을 performance metrics로써 활용할 때, 가중평균을 사용하면 조금 더 좋은 결과를 얻게 되지 않을까? 예를 들어, TPR에 조금 더 가중치를 두어서 $\\mbox{weighted G-mean} = TNR^{0.2} \\times TPR^{0.8}$처럼? 5-3. ex1) Simulated data epoch: 250, trees: 5000, nodesize=1, mtry=d/3\n위 Table은 complex imbalanced data in high dimensional settings에서 RFQ가 효과적임을 보여주고 있다.\n5-4. ex2) Cognitive impairment data Alzheimers Disease CSF Data from AppliedPredictiveModeling (N=333, d=130 where $N_0=242, N_1=91$ with IR=2.66)\nepoch: 250, trees: 5000, nodesize=1, mtry=d/3\nBRF의 경우에는 high dimensional이 될수록 성능이 낮아짐을 알 수 있다.\n5-5. ex3) Customer churn data N=3333 with $N_1=483$ and IR=5.90\nepoch: 250, trees: 5000, nodesize=1, mtry=d/3\n5-4와 같이, BRF는 high dimension일 때 성능이 좋지 않아짐을 알 수 있다.\n6. Multiclass Imbalanced Data Binary가 아니라 Multiclass의 경우에도 RFQ가 잘 작동하는지 확인해보았다.\n6-1. ex1) Waveform simulations $$\\mbox{weighted G-mean} = \\Big(TPR1^{\\beta_1} + TPR2^{\\beta_2} + TPR3^{\\beta_3}\\Big)^{1/(\\beta_1+\\beta_2+\\beta_3)}$$\n2개 아니라, 3개의 클래스로 나누어져있는 경우에 G-mean을 통해 세 모델을 분류하였다. $\\binom(3,2) = 3$이므로, 총 세 경우의 수에 있어서 TPR과 TNR을 계산한 후 weighted G-mean을 계싼하였다. 아래의 두 테이블의 차이는 각 그룹별 TPR의 가중치를 어떻게 두고 G-mean을 계산했는지에 따라 다르다. 참고로 unweighted G-mean은 multiclass 상황에서 적절하지는 않다. 특히 심각한 불균형이 존재할 경우 더욱 그러하다. 아래의 경우에는 $\\Beta_1, \\Beta_2, \\Beta_3$를 각각 러프하게 1/2, 1, 1로 넣었지만, 이는 저자가 의도하는 바를 담기에는 충분한 차이를 보이긴 했다. 아래의 표를 통해서 구체적인 수치를 확인해보도록 하자.\n6-2. ex2) Cassini simulations 위의 예시와 시사하는 바는 동일하다.\n7. Variable Importance   Breiman-Culter importance(tree-based) : not fit\n대부분의 노트들이 0을 갖고 있을 것이기 때문에 불균형데이터에서는 해당 기준으로 VIMP을 나타내는 데에는 적절하지 못하다.\n  G-mean with Ishwaran-Kogalur importance(ensemble) : do fit\nblocked ensemble의 prediction error를 통해서 계산한다.\n  1  library(randomForestSRC)   ## ## randomForestSRC 2.12.1 ## ## Type rfsrc.news() to see new features, changes, and bug fixes. ## 1 2 3 4  data(breast) breast \u0026lt;- na.omit(breast) o.rfq \u0026lt;- imbalanced(status ~ ., breast, importance = TRUE) print(o.rfq)   ## Sample size: 194\r## Frequency of class labels: 148, 46\r## Number of trees: 3000\r## Forest terminal node size: 1\r## Average no. of terminal nodes: 27.38267\r## No. of variables tried at each split: 6\r## Total no. of variables: 32\r## Resampling used to grow trees: swor\r## Resample size used to grow trees: 123\r## Analysis: RFQ\r## Family: class\r## Splitting rule: gini *random*\r## Number of random split points: 10\r## Imbalanced ratio: 3.217391\r## (OOB) G-mean: 0.5280054\r## (OOB) Normalized Brier score: 73.33872 ## (OOB) AUC: 56.26469 ## (OOB) Error rate: 0.4719946\r## ## Confusion matrix:\r## ## predicted\r## observed N R class.error\r## N 73 75 0.5068\r## R 20 26 0.4348\r## ## Overall (OOB) error rate: 47.199464%\r1  plot(o.rfq, plots.one.page = FALSE)   ## ## Importance Relative Imp\r## worst_concavepoints 0.0420 1.0000\r## worst_fractaldim 0.0207 0.4931\r## mean_texture 0.0207 0.4931\r## pnodes 0.0184 0.4381\r## tsize 0.0147 0.3492\r## mean_compactness 0.0110 0.2610\r## worst_symmetry 0.0103 0.2441\r## worst_radius 0.0049 0.1164\r## mean_radius 0.0049 0.1164\r## worst_concavity 0.0036 0.0864\r## SE_texture 0.0036 0.0864\r## mean_area 0.0036 0.0864\r## worst_perimeter 0.0011 0.0265\r## SE_area 0.0011 0.0265\r## SE_perimeter 0.0011 0.0265\r## mean_symmetry 0.0000 0.0000\r## SE_symmetry -0.0036 -0.0858\r## SE_concavity -0.0036 -0.0858\r## SE_radius -0.0047 -0.1121\r## SE_concavepoints -0.0064 -0.1514\r## SE_compactness -0.0064 -0.1514\r## mean_smoothness -0.0064 -0.1514\r## mean_concavity -0.0072 -0.1710\r## worst_area -0.0086 -0.2036\r## worst_compactness -0.0101 -0.2394\r## mean_fractaldim -0.0101 -0.2394\r## worst_texture -0.0137 -0.3269\r## SE_fractaldim -0.0137 -0.3269\r## worst_smoothness -0.0162 -0.3848\r## mean_concavepoints -0.0174 -0.4137\r## mean_perimeter -0.0237 -0.5635\r## SE_smoothness -0.0311 -0.7398\r1  get.imbalanced.performance(o.rfq)   ## n.majority n.minority iratio threshold sens spec ## 148.0000000 46.0000000 3.2173913 0.2371134 0.5652174 0.4932432 ## prec brier auc F1 balanced pr.auc.rand ## 0.2574257 0.7333872 0.5626469 0.3537415 0.5292303 0.2371134 ## pr.auc gmean ## 0.3299499 0.5280054\r8. Discussion high complexity, high imbalancedness, high dimensionality에서 RFQ가 효과적이었다.\nBRF가 아직 계산이 더 빠르기는 하지만 큰 차이는 아니다. 심지어 Theorem 4에 의해 subsampling을 한다면 computational load도 줄이면서 TNR+TPR optimal은 놓치지 않을 수 있다.\n9. Further Reference 불균형데이터에 대해서 알고 싶다면 아래의 세 논문을 추가 참고해보면 좋을 것 같다.\n Krawczyk, B. (2016). Learning from imbalanced data: open challenges and future directions. Progress in Artificial Intelligence, 5(4), 221-232. Haixiang, G., Yijing, L., Shang, J., Mingyun, G., Yuanyue, H., \u0026amp; Bing, G. (2017). Learning from class-imbalanced data: Review of methods and applications. Expert Systems with Applications, 73, 220-239. Das, S., Datta, S., \u0026amp; Chaudhuri, B. B. (2018). Handling data irregularities in classification: Foundations, trends, and future challenges. Pattern Recognition, 81, 674-693.  이 논문에 대해서 영어로 정리된 깃헙 페이지가 있다.\n","description":"","id":89,"section":"blog","tags":null,"title":"A Random Forests Quantile Classifier for Class Imbalanced Data","uri":"https://jiwooblog.netlify.app/blog/211003_rf_quantile_imblance/"},{"content":"[독후감] 차를 맛보는 여인\n1. 분포에 대한 관점 차이  Fisher: 자료는 실제분포의 모수를 추정하기 위해서만 사용 Pearson: 실제자료 기술  2. 개념 창시자 또는 방법론 원작자  MLE, 자유도, p-value: Fisher iterative method: Robert Recorde EM algorithm: Nan Laird, James Ware Probit Analysis: Bliss CLT: Lindeberg-Levy U-statistics: Hoeffding Goodness of fit test, 가설검정: Pearson  3. 베이즈 통계  \u0026lsquo;이후\u0026rsquo;의 조건으로 \u0026lsquo;이전\u0026rsquo;의 확률을 계산한다?! 베이즈 정리가 처음에 받아들여지지 않았던 이유 중 하나는 Pearson의 영향이 있다. Pearson에 의해 당시에는 표본들의 개별값보다는 그것들의 확률분포를 아는 것이 중요하다고 여겨졌다. 그리고 그 분포들의 모수값을 추정하는 것이 당시 통계학의 목표였는데, 베이즈 정리에 따르면 모수들은 무작위라고 가정하게 되므로 기존 노력들의 의미를 모두 뒤엎어버리는 일이 된다.  4. 확률이란?  확률이라는 단어는 개인의 불확실성에 대한 감각을 다루는 것으로 만들어졌다. 그런데 개인적 확률에 대한 관점에도 크게 두 가지 종류가 있다.   (Savage-de Finetti 방식) 각각의 사람은 개별적인 확률의 집합을 가진다. (Keynes 방식) 확률은 그 사람이 속한 문화 안에서 교육을 받은 사람이 가질 것으로 기대할 수 있는 믿음의 정도이다.  5. Kolmogorov의 업적  확률에 대한 실제 수학적 기초는?! =\u0026gt; 확률이론에 대한 공리 시간에 걸쳐서 수집된 데이터로 무엇을 할 수 있는가?! =\u0026gt; 확률과정(stochastic process)   1987년 Kolmogorov가 세상을 떠나고, 그의 넘처나는 아이디어들이 남았지만 아직 누구도 계승하고 있지 못하고 있다.  6. 비모수 방법의 의문점  데이터가 모수분포를 갖는다면, 비모수방법을 사용하는 건 얼마나 잘못된 것인가 데이터가 모수분포를 갖지 않는다면, 얼마나 벗어나야 비모수방법을 사용하는 것이 더 적절할까 ","description":"","id":90,"section":"blog","tags":["book"],"title":"차를 맛보는 여인","uri":"https://jiwooblog.netlify.app/blog/210711_the_lady_tasting_tea/"},{"content":"Natural Scene Categories Revealed in Distributed Patterns of Activity in the Human Brain(Walther, 2009)\nReference Walther, D. B., Caddigan, E., Fei-Fei, L., \u0026amp; Beck, D. M. (2009). Natural scene categories revealed in distributed patterns of activity in the human brain. Journal of neuroscience, 29(34), 10573-10581.\nSummary 자연 장면을 보고 카테고리를 분류할 때, 뇌에서 어떠한 패턴이 일어나는지 분석하는 것이 본 논문의 핵심이다. fMRI와 MVPA (multivoxel pattern analysis)를 활용하여 뇌의 어떤 영역에서 그리고 어떠한 패턴으로 자연 장면의 카테고리를 구분하는지 확인하였다. 다섯 명의 참가자가 있었으며 총 여섯 개의 카테고리(카테고리별 120개 사진)를 활용하였다. 연구 결과, V1, PPA, RSC, LOC 영역 모두에서 자연장면의 카테고리를 구분할 때 활용되는 정보를 갖고 있음을 확인하였다. 그리고 직접 버튼을 눌러서 카테고리를 분류하는 인간행동연구와 비교해본 결과, V1을 제외한 PPA, RSC, LOC가 해석한 정보를 인간이 활용하여 카테고리 분류를 한다고 판단하였다. 특히, fMRI 신호를 기반으로 카테고리를 예측분류한 것의 오답 패턴이 참가자들의 행동 오답패턴과 상관관계가 높은 것으로 앞서 언급한 세 뇌 영역에서 나타났다. 또한 눈여겨보아야 할 결과로는, 그림을 상하 반전하여 제시하였을 때 PPA 신호를 통한 예측과 행동연구 모두에서 정확도가 떨어졌다는 점이다. 종합적인 결론으로, PPA, RSC, LOC를 포함한 뇌영역들이 일종의 네트워크를 형성하여 정보를 모아 자연장면의 카테고리를 분류하는 작업을 시행한다고 볼 수 있다.\n해당 논문은 자연장면을 해석하는 데에 있어서 MVPA를 활용하여 여러 뇌 영역 간의 연관성 있는 패턴을 밝혔다는 데에 차별적인 의의가 있다고 할 수 있다. 또한, fMRI 신호를 분석하여 예측한 것과 실제 행동분석을 비교하면서 단순히 정보가 뇌 영역에 있다는 것과 그것을 활용하는 것의 차이를 명확하게 구분하였다는 점에서 한층 더 철저한 연구설계가 이루어졌음을 알 수 있다. 나아가, 뇌가 이미지 그 자체를 해석하는(decode) 것과 해당 이미지의 카테고리를 해석하는(decode)하는 것의 차이를 역시 확실히 하고 있다.\n그럼에도 불구하고 완벽하게 이해가 되지 않는 점들이 있긴 하다. 본 연구의 한계점은, 많은 연구에서 직면할 수밖에 없는 한계이기는 하지만, 판단기준에 대한 통계적 근거가 다소 설득력이 부족하다는 점이다. 행동연구와 fMRI 분석을 통한 예측의 실패를 비교할 때, frequent confusion과 아닌 것을 구분하는 기준이 다소 모호하다. 또한, 각 영역별로 행동분석과 상관관계를 분석하였을 때, 이상치처럼 보이는 것들에 의해 상관계수가 왜곡되었을 가능성도 충분히 존재한다. 그리고 피어슨 상관계수는 오로지 선형적 연관성만을 이야기할 수 있기 때문에 뇌 영역끼리 복잡한 관계로 관련이 있다면 이를 분석하는 데에 있어서는 상당히 제한적인 통계방법을 사용했다고 볼 수 있다.\n궁금증   decoding accuracy에 대해 t검정을 할 때, 기준치로서 1/6이 적절한가?! 만약 연구자가 20개의 카테고리를 활용하였다면 1/20이 적절하다고 할 수 있을까? 그리고 단순히 p-value만 볼 것이 아니라 effect size를 고려하지 않아도 괜찮은 걸까? 왜냐하면 이미지 개수가 많아지면 많아질수록 p-value은 점점 작게 나올 가능성이 높아지기 때문이다.\n  Figure 2에서 오답 비율이 높은 것(노란색)이 대각선을 기준으로 대칭이 되지 않아도 문제가 없는가? industry를 보고 building이라고 생각했으면, building을 보고 industry라고 생각했을 비율도 같이 높아야 하는 것 아닐까? 이부분에서 놓친 논리는 없을까?\n  연구자는 모든 뇌 영역을 decoder라고 보았다. 하지만, 뇌 영역의 입장에서 보면, 특정 영역은 다른 영역의 encoder로서 역할을 할 수도 있지 않을까? 만약 그렇다면 어떠한 방식으로 연구를 설계할 수 있을까?\n ","description":"","id":91,"section":"blog","tags":["Psychology"],"title":"Natural Scene Categories Revealed in Distributed Patterns of Activity in the Human Brain","uri":"https://jiwooblog.netlify.app/blog/210312_natural_scene_categorization/"},{"content":"빅콘테스트 챔피언리그 데이터분석 분야\n배운 점\n 자연어 전처리   정규표현식 기존에 제시된 칼럼의 수가 굉장히 적었다. 그중에서 그나마 정보를 담고 있는 것은 \u0026lsquo;상품명\u0026rsquo; 칼럼이었다. 이 데이터를 예를 들어, \u0026lsquo;NIKE 스트라이프 셔츠\u0026rsquo;와 같은 형식으로 브랜드가 일반적으로 앞에 나오고 뒤에 디테일한 상품 분류가 나왔다. 하지만 식료품과 같은 경우에는 브랜드가 없는 것들이 많았고, 띄어쓰기가 제대로 되어있지 않은 경우도 많았다. (아마도 현실 데이터는 이것보다도 더 정돈되지 않은 경우가 많을 터이다\u0026hellip;) 또한 브랜드의 표기 자체가 통일되지 않은 경우가 있었다.(ex. 카사미아=까사미아) NS Shop+ 공식홈페이지에서 나눠놓은 대분류, 중분류, 소분류 체계를 참고해서 나눴다. 너무 많은 상품명이 있었기 때문에, 대분류 3개씩 묶어서 역할분담을 해서 나머지 분류를 채웠는데, 그러다보니 사소하게 통일되지 않은 분류기준이 있어서 추후 약간의 어려움을 겪었다. 기준을 명확하게 정해두거나, 만약 그러지 못할 경우 최소한의 사람이 해당 업무를 했으면 어땠을까라는 생각이 든다. ","description":"","id":92,"section":"blog","tags":["project"],"title":"빅콘테스트 NS Shop+ 홈쇼핑 실적 예측","uri":"https://jiwooblog.netlify.app/blog/bigcontest2020/"},{"content":"서울시 수소차 충전소 입지 추천\n연세대학교 데이터사이언스입문 수업에서 진행한 프로젝트\n목표 최근 심각해진 미세먼지 등의 환경 문제에 대한 대책으로 수소차의 생산 및 보급 이슈가 주목받고 있다.\n이와 관련하여 서울시는 현대차가 수소전기차 보급 활성화를 위한 MOU를 체결하는 등의 노력을 기울이고 있다.\n다만, 수소차 충전소 인프라 형성이 아직 미흡한 탓에 보급에 다소 어려움을 겪고 있는 실정이다.\n환경부와 국토교통부 등의 관련 부처에서는 2040년까지 총 1200기의 충전소를 설치할 예정이다.\n이에 대해 지역, 사회별 변수들을 고려하여 서울 시내의 적합한 수소차 충전소 입지를 선정하는 것을 목표로 한다.\n결과물 블로그\n대쉬보드\n역할 마침 지난 2019년 겨울방학 때 머신러닝을 공부해보기 시작해서, 주로 모델링과 관련된 역할을 많이 맡았다.\n다양한 방법론을 시도하였는데, 의미 해석을 도출하기 위해 Random Forest를 채택하게 되었다. 하지만 데이터캠프를 통해서 shiny dashboard를 포함한 다양한 것들을 공부할 수 있었다.\n배운 점  데이터 수집 및 전처리   기존에 데이터가 주어지는 것이 아니기 때문에, 필요한 데이터를 직접 일일이 찾아나서는 노력을 했다. 그 과정에서 거리 변수를 직접 계산하여 파생변수를 만들었다. 지역 단위를 최대한 동 단위로 나눠서 계산을 했다.  tidyverse   이전에는 tidyverse 문법을 잘 모르고 기본 base 문법만 썼다. tidyverse의 편리함을 잘 모르고 python만 겨울방학동안 썼었는데, R의 %\u0026gt;% pipe line과 ggplot의 편리함을 알 수 있었다.  다양한 R 활용법   blogdown: R로 블로그 만들기 shiny: R로 대쉬보드 만들기 xaringan: R로 프렌젠테이션 자료 만들기  개선할 점  수소차 보유 대수 자체가 애초에 충분하지 못했기 때문에, 포아송회귀분석을 한번 해봤으면 어떨까 하는 아쉬움이 있다. xaringan 패키지를 너무 늦게 알게 되어서, 숙련도가 높지 못했다. (2021.03.14에 알게 된 정보로는, ) ","description":"","id":93,"section":"blog","tags":["project"],"title":"수소차 충전소 입지 추천","uri":"https://jiwooblog.netlify.app/blog/rhino/"},{"content":"마크다운 가이드라인\nLorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\n\rHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae.\nNote that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rCode block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item  First Sub-item Second Sub-item    Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ","description":"","id":94,"section":"blog","tags":["markdown"],"title":"Markdown Syntax Guide","uri":"https://jiwooblog.netlify.app/blog/markdown-syntax/"},{"content":"Youtube 링크 거는 법\nHugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\n\rYoutube 링크는 이렇게 하기!   ","description":"","id":95,"section":"blog","tags":["Youtube"],"title":"How to Link","uri":"https://jiwooblog.netlify.app/blog/rich-content/"},{"content":"이건 정확히 무엇인지 모르겠다..\nVagus elidunt \nThe Van de Graaf Canon\n","description":"","id":96,"section":"blog","tags":null,"title":"Placeholder Text","uri":"https://jiwooblog.netlify.app/blog/placeholder-text/"},{"content":"이모지 관련\nLorem est tota propiore conpellat pectoribus de pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nEmoji can be enabled in a Hugo project in a number of ways.\n\rThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 🙈 🙉 🙉 🙊 🙊\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3  .emoji { font-family: Apple Color Emoji,Segoe UI Emoji,NotoColorEmoji,Segoe UI Symbol,Android Emoji,EmojiSymbols; }  ","description":"","id":97,"section":"blog","tags":["emoji"],"title":"Emoji Support","uri":"https://jiwooblog.netlify.app/blog/emoji-support/"}]